# Sparse Group Factor Analysis (SGFA) for qMAP-PD

Python implementation of Sparse Group Factor Analysis (SGFA) designed to identify latent disease factors that capture associations between various neuroimaging data modalities within Parkinson's disease population subgroups. This project applies and adapts this model to the qMAP-PD (<https://qmaplab.com/qmap-pd>) study dataset.

## Features

- **Multi-view Factor Analysis**: Handles multiple neuroimaging modalities simultaneously
- **Sparsity & Grouping**: Implements both sparse and group penalties for interpretable results
- **Computational Optimization**: Memory-efficient processing with automatic system optimization and performance mixins
- **Comprehensive Testing**: Extensive experimental validation framework
- **Clinical Validation**: Built-in tools for clinical subtype analysis and biomarker discovery

## Project Structure

### Core Modules

- **[core/get_data.py](core/get_data.py)**: High-level interface to load datasets or generate synthetic data
- **[core/run_analysis.py](core/run_analysis.py)**: Main script containing the SGFA model and experiment runner
- **[core/utils.py](core/utils.py)**: Utility functions supporting the analysis pipeline

### Data Management

- **[data/](data/)**: Data loading and generation modules
  - `qmap_pd.py`: qMAP-PD dataset loader with preprocessing
  - `synthetic.py`: Synthetic multi-view data generator for testing

### Analysis Pipeline

- **[analysis/](analysis/)**: Core analysis components
  - `data_manager.py`: Data loading and preprocessing management ‚úÖ **IMPLEMENTED**
  - `config_manager.py`: Configuration and hyperparameter management ‚úÖ **IMPLEMENTED**
  - `model_runner.py`: Model training orchestration ‚úÖ **IMPLEMENTED**
  - `cross_validation.py`: Cross-validation framework ‚úÖ **IMPLEMENTED**
  - `cross_validation_library.py`: Advanced neuroimaging CV üîß **FRAMEWORK ONLY - NEEDS DEVELOPMENT**
  - `cv_fallbacks.py`: Reusable fallback utilities for advanced CV features ‚úÖ **IMPLEMENTED**
  - `experiment_utils.py`: Bridge utilities for experiments to use analysis pipeline ‚úÖ **IMPLEMENTED**

### Computational Optimization

- **[optimization/](optimization/)**: Computational optimization framework (renamed from `performance/` to avoid confusion with model performance)
  - `memory_optimizer.py`: Memory management and monitoring with automatic cleanup
  - `data_streaming.py`: Memory-efficient data loading for large datasets
  - `mcmc_optimizer.py`: Memory-optimized MCMC sampling with checkpointing
  - `profiler.py`: Performance profiling and benchmarking tools
  - `config.py`: Auto-configuration for different system capabilities
  - `experiment_mixins.py`: Reusable optimization patterns for experiments ‚úÖ **NEW**

### Experimental Validation

- **[experiments/](experiments/)**: Comprehensive experimental framework with advanced neuroimaging CV
  - `data_validation.py`: Data quality and preprocessing validation ‚úÖ **IMPLEMENTED**
  - `model_comparison.py`: SGFA vs traditional method comparison ‚úÖ **IMPLEMENTED** + üîß **NeuroImagingMetrics NEEDS DEVELOPMENT**
  - `sgfa_parameter_comparison.py`: SGFA variant comparison ‚úÖ **IMPLEMENTED** + üîß **HyperOptimizer NEEDS DEVELOPMENT**
  - `sensitivity_analysis.py`: Hyperparameter sensitivity testing ‚úÖ **IMPLEMENTED**
  - `reproducibility.py`: Reproducibility and robustness validation ‚úÖ **IMPLEMENTED**
  - `performance_benchmarks.py`: Scalability benchmarking ‚úÖ **IMPLEMENTED** + üîß **ClinicalAwareSplitter NEEDS DEVELOPMENT**
  - `clinical_validation.py`: Clinical subtype validation ‚úÖ **IMPLEMENTED** + üîß **Clinical-stratified CV AVAILABLE**

### Models & Implementation

- **[models/](models/)**: Core model implementations
  - `base.py`: Base model class and shared functionality ‚úÖ **IMPLEMENTED**
  - `standard_gfa.py`: Standard Group Factor Analysis with ARD priors ‚ö†Ô∏è **HIGH GPU USAGE**
  - `sparse_gfa.py`: Sparse GFA with regularized horseshoe priors ‚úÖ **IMPLEMENTED**
  - `latent_class_analysis.py`: Patient subtyping and clustering models ‚ö†Ô∏è **VERY HIGH GPU USAGE**
  - `factory.py`: Model factory for instantiation and configuration ‚úÖ **IMPLEMENTED**
  - `variants/neuroimaging_gfa.py`: Specialized neuroimaging variants üîß **NEEDS DEVELOPMENT**

### Visualization & Testing

- **[visualization/](visualization/)**: Result visualization and diagnostic plots
- **[tests/](tests/)**: Comprehensive test suite

## üöÄ Automatic Optimization Features

All experiments now include **automatic computational optimization** through the `@performance_optimized_experiment` decorator:

### **Memory Optimization**

- **Automatic array optimization**: Converts float64‚Üífloat32 when precision allows
- **Real-time memory monitoring**: Tracks usage with automatic cleanup
- **Adaptive batch sizing**: Calculates optimal batch sizes based on available memory
- **Memory pressure handling**: Automatic intervention when memory limits approached

### **Data Streaming**

- **Large dataset support**: Memory-efficient streaming for datasets larger than RAM
- **Chunked processing**: Automatic data chunking for cross-validation and analysis
- **Memory-efficient contexts**: Automatic memory management during processing

### **MCMC Optimization**

- **Gradient checkpointing**: Reduces memory usage during MCMC sampling
- **Adaptive subsampling**: Automatic data subsampling when memory constrained
- **Batch sampling**: Memory-optimized sampling for large datasets

### **System Auto-Configuration**

- **Hardware detection**: Automatically detects system capabilities (RAM, CPU)
- **Optimization strategy selection**: Chooses optimal settings based on available resources
- **Performance monitoring**: Real-time tracking of computational performance

### **Usage**

```python
# All experiments automatically optimized - no code changes needed!
from experiments import ClinicalValidationExperiments

config = ExperimentConfig(auto_configure_system=True)  # Enable auto-optimization
experiment = ClinicalValidationExperiments(config)
# Memory optimization, streaming, and monitoring happen automatically
```

## ‚ö†Ô∏è Development Status & Warnings

### üîß **Features Requiring Development**

The following features have **framework infrastructure** in place but require **substantial development work**:

#### **Advanced Cross-Validation (High Priority)**

- **ClinicalAwareSplitter**: Clinical-aware CV splitting logic
- **NeuroImagingMetrics**: Domain-specific evaluation metrics
- **NeuroImagingHyperOptimizer**: Bayesian hyperparameter optimization
- **Status**: Framework ready, ~600-1000 lines of specialized code needed
- **Estimated Effort**: 2-4 weeks for experienced ML developer

#### **NeuroGFA Models (Research Priority)**

- **neuroimaging_gfa.py**: Specialized spatial priors for neuroimaging
- **Status**: Framework ready, requires deep neuroimaging + Bayesian expertise
- **Estimated Effort**: 1-3 months for neuroimaging researcher

### ‚ö†Ô∏è **GPU Memory Warnings**

**CAUTION**: The following models have **high computational requirements**:

#### **Standard GFA** (`standard_gfa.py`)

- **GPU Memory**: High usage (8-16GB+ depending on data size)
- **Recommendation**: Use sparse_gfa for most applications
- **Alternative**: Reduce K (number of factors) or data size

#### **Latent Class Analysis** (`latent_class_analysis.py`)

- **GPU Memory**: Very high usage (16GB+ typical, may exceed most GPU limits)
- **Recommendation**: Use for small datasets only or powerful GPU clusters
- **Alternative**: Use clustering post-hoc on sparse_gfa factors

#### **Safe Default**: `sparse_gfa.py`

- **GPU Memory**: Moderate usage (2-8GB typical)
- **Recommendation**: Start here for most neuroimaging applications
- **Performance**: Excellent for PD subtype analysis

### üìã **Planning Frameworks**

#### Advanced CV Development Plan

#### Phase 1: ClinicalAwareSplitter (1-2 weeks)

```python
# Core functionality needed:
class ClinicalAwareSplitter:
    def split(self, X, y, groups, clinical_data):
        # 1. Multi-variable stratification logic
        # 2. Site/scanner awareness
        # 3. Group preservation (same subject not in train/test)
        # 4. Minimum sample size constraints
        # 5. Demographics balance validation
```

#### Phase 2: NeuroImagingMetrics (1-2 weeks)

```python
# Core functionality needed:
class NeuroImagingMetrics:
    def calculate_fold_metrics(self, train_result, test_metrics, clinical_data):
        # 1. Factor interpretability scoring
        # 2. Cross-view consistency metrics
        # 3. Clinical association scoring
        # 4. Spatial coherence measures
        # 5. Composite scoring logic

    def evaluate_model_comparison(self, X_data, model_outputs, model_info):
        # 1. Multi-model evaluation framework
        # 2. Statistical significance testing
        # 3. Effect size calculations
```

#### Phase 3: NeuroImagingHyperOptimizer (2-3 weeks)

```python
# Core functionality needed:
class NeuroImagingHyperOptimizer:
    def optimize_hyperparameters(self, X_data, clinical_data, search_space):
        # 1. Bayesian optimization setup (optuna/hyperopt)
        # 2. Clinical-aware objective functions
        # 3. Multi-objective optimization (accuracy + interpretability)
        # 4. Parameter importance analysis
        # 5. Convergence tracking and early stopping
```

**Dependencies**: sklearn, optuna/hyperopt, scipy.stats
**Testing Requirements**: Synthetic + real clinical data validation

---

#### NeuroGFA Development Plan

#### Phase 1: Spatial Prior Framework (3-4 weeks)

```python
# Core research needed:
class NeuroImagingGFA:
    def __init__(self, spatial_prior_type="ising", anatomical_connectivity=None):
        # 1. Ising model priors for spatial coherence
        # 2. Anatomical connectivity integration
        # 3. Multi-scale spatial modeling
        # 4. ROI-aware factor allocation
```

#### Phase 2: Advanced Neuroimaging Features (4-6 weeks)

```python
# Advanced functionality:
- Atlas-based factor initialization
- Symmetric brain modeling
- Multi-modal data fusion (structural + functional)
- Temporal factor evolution modeling
- Group-level + subject-specific factors
```

#### Phase 3: Clinical Integration (2-3 weeks)

```python
# Clinical neuroimaging integration:
- Disease progression modeling
- Biomarker discovery optimization
- Multi-site harmonization
- Scanner-invariant factors
```

**Dependencies**: nibabel, nilearn, advanced Bayesian modeling, neuroimaging expertise
**Data Requirements**: Multi-modal neuroimaging datasets with anatomical atlases

## Installation

### Basic Installation

```bash
# Clone the repository
git clone https://github.com/meeramads/sgfa_qmap-pd
cd sgfa_qmap-pd

# Create and activate virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -e .
```

### Development Installation

```bash
# Install with development tools
pip install -e ".[dev]"

# Run tests
pytest

# Run with coverage
pytest --cov=. --cov-report=html
```

## Quick Start

### Basic Analysis

```python
from core.get_data import qmap_pd, generate_synthetic_data
from core.run_analysis import run_sgfa_analysis

# Load data - Option 1: Real qMAP-PD data
# X_list, metadata = qmap_pd(data_dir='path/to/data')

# Load data - Option 2: Synthetic data
data = generate_synthetic_data(num_sources=3, K=5)
X_list = data['X_list']

# Run SGFA analysis
results = run_sgfa_analysis(X_list, K=5, sparsity_level=0.3)
```

### Using Analysis Pipeline Components

```python
from analysis import quick_sgfa_run, create_analysis_components

# Quick SGFA run with minimal setup
results = quick_sgfa_run(X_list, K=5, percW=25.0)

# Or use the full pipeline components
data_manager, model_runner = create_analysis_components({
    'K': 5, 'num_sources': 3, 'dataset': 'synthetic'
})
X_list, hypers = data_manager.load_data()
results = model_runner.run_standard_analysis(X_list, hypers, {'X_list': X_list})
```

### With Performance Optimization

```python
from performance import auto_configure_for_system, PerformanceManager

# Auto-configure for your system
config = auto_configure_for_system()

# Run with optimization
with PerformanceManager(config) as manager:
    results = run_sgfa_analysis(X_list, K=5, sparsity_level=0.3)
    print(f"Memory used: {manager.get_memory_status()}")
```

### Comprehensive Experiments

```python
from experiments import (
    ExperimentConfig,
    DataValidationExperiments,
    ModelArchitectureComparison,
    SGFAParameterComparison,
    SensitivityAnalysisExperiments,
    ReproducibilityExperiments,
    PerformanceBenchmarkExperiments,
    ClinicalValidationExperiments
)

# Configure experiments
config = ExperimentConfig(output_dir="./results", save_plots=True)

# Run data validation experiments
data_validator = DataValidationExperiments(config)
data_results = data_validator.run_comprehensive_data_validation(X_list)

# Run model comparison experiments
model_comparison = ModelArchitectureComparison(config)
comparison_results = model_comparison.run_full_comparison(X_list)

# Run SGFA parameter optimization
sgfa_optimizer = SGFAParameterComparison(config)
param_results = sgfa_optimizer.run_parameter_sweep(X_list)

# Run sensitivity analysis
sensitivity = SensitivityAnalysisExperiments(config)
sensitivity_results = sensitivity.run_sensitivity_analysis(X_list)

# Run reproducibility validation
reproducibility = ReproducibilityExperiments(config)
repro_results = reproducibility.run_reproducibility_tests(X_list)

# Run performance benchmarks
benchmarks = PerformanceBenchmarkExperiments(config)
perf_results = benchmarks.run_performance_benchmarks(X_list)

# Run clinical validation (if clinical data available)
clinical = ClinicalValidationExperiments(config)

# Basic clinical validation (subtype classification)
clinical_results = clinical.run_clinical_validation(X_list, clinical_data)

# Advanced: Clinical-stratified cross-validation
# Configure validation types to include clinical-stratified CV
config_dict = {
    "clinical_validation": {
        "validation_types": ["subtype_classification", "clinical_stratified_cv"],
        "classification_metrics": ["accuracy", "precision", "recall", "f1_score"]
    }
}
clinical_results_advanced = clinical.run_clinical_validation(X_list, clinical_data, config_dict)
```

### Advanced Neuroimaging Cross-Validation

‚úÖ **NEW**: Clinical-stratified CV now available! Basic neuroimaging CV features work, advanced features require development:

```python
# ‚úÖ THESE WORK - Basic performance benchmarks are fully implemented:
from experiments.performance_benchmarks import PerformanceBenchmarkExperiments

benchmarks = PerformanceBenchmarkExperiments(config)

# Scalability benchmarks
scalability_results = benchmarks.run_scalability_benchmarks(
    X_base=X_list, hypers={"percW": 25.0}, args={"K": 5, "model": "sparseGFA"}
)

# Memory benchmarks
memory_results = benchmarks.run_memory_benchmarks(
    X_base=X_list, hypers={"percW": 25.0}, args={"K": 5, "model": "sparseGFA"}
)

# ‚ùå THIS WILL FAIL - Advanced neuroimaging CV not implemented:
cv_results = benchmarks.run_clinical_aware_cv_benchmarks(
    X_base=X_list,
    hypers={"percW": 25.0},
    args={"K": 5, "model": "sparseGFA"},
    clinical_data=clinical_data
)

# Neuroimaging hyperparameter optimization
from experiments.sgfa_parameter_comparison import SGFAParameterComparison

sgfa_exp = SGFAParameterComparison(config)
# ‚ùå This will fail - NeuroImagingHyperOptimizer not implemented
hyperopt_results = sgfa_exp.run_neuroimaging_hyperparameter_optimization(
    X_list=X_list,
    hypers={"percW": 25.0},
    args={"K": 5},
    clinical_data=clinical_data
)

# ‚úÖ NEW: Clinical-stratified cross-validation works!
from experiments.clinical_validation import ClinicalValidationExperiments

clinical_exp = ClinicalValidationExperiments(config)

# Configure for clinical-stratified CV
config_dict = {
    "clinical_validation": {
        "validation_types": ["clinical_stratified_cv"],
        "cross_validation": {"n_folds": 5, "stratified": True}
    }
}

# This works - Clinical-stratified CV with robust neuroimaging scaling
clinical_stratified_results = clinical_exp.run_clinical_validation(
    X_list=X_list,
    clinical_data=clinical_data,
    config=config_dict
)
```

#### Advanced CV Features Status

- **ClinicalAwareSplitter**: ‚úÖ **Basic implementation available** - Clinical stratification with robust neuroimaging scaling
- **NeuroImagingMetrics**: üîß Infrastructure exists, needs specialized neuroimaging evaluation metrics
- **NeuroImagingHyperOptimizer**: üîß Infrastructure exists, needs Bayesian optimization logic
- **Clinical-Stratified Validation**: ‚úÖ **Available** - Use `validation_types: ["clinical_stratified_cv"]`

#### ‚úÖ Working Alternative: Basic Cross-Validation

The experiments automatically fall back to working basic CV when advanced features fail:

```python
from analysis.cross_validation import CVRunner

# This works - CVRunner automatically falls back to basic CV
cv_runner = CVRunner(config, results_dir="./results")
cv_results, cv_obj = cv_runner.run_cv_analysis(
    X_list=X_list,
    hypers={"percW": 25.0},
    data={"K": 5, "model": "sparseGFA"}
)

# Basic CV uses SparseBayesianGFACrossValidator with sklearn-style splitting
# but adapted for SGFA models
```

## Performance Optimization

The project includes a comprehensive performance optimization framework:

### Memory Management

- **Adaptive memory limits** based on available system resources
- **Real-time monitoring** with automatic cleanup
- **Memory-efficient data structures** and array optimization

### Data Processing

- **Chunked processing** for large datasets (>1GB)
- **HDF5 support** with compression
- **Streaming data loaders** to minimize memory footprint

### MCMC Optimization

- **Memory-efficient sampling** strategies
- **Adaptive batching** for large datasets
- **Gradient checkpointing** to reduce memory usage

### Quick Performance Setup

```python
from performance.config import auto_configure_for_system

# Automatically optimize for your system
config = auto_configure_for_system()

# Or use predefined presets
from performance.config import PerformanceConfig
config = PerformanceConfig().create_preset('memory_efficient')  # For limited RAM
config = PerformanceConfig().create_preset('fast')              # For speed
config = PerformanceConfig().create_preset('balanced')          # Balanced approach
```

## Validation Experiments

The framework includes comprehensive validation experiments:

### Data Validation

- Quality assessment and outlier detection
- Preprocessing strategy comparison
- Multi-view data alignment validation

### Method Comparison

- SGFA variants (sparse vs group vs standard)
- Traditional methods (PCA, ICA, Factor Analysis)
- SGFA parameter comparison and optimization
- Scalability and performance benchmarking

### Clinical Validation

- **PD subtype classification validation** ‚úÖ **IMPLEMENTED**
- **Clinical-stratified cross-validation** ‚úÖ **AVAILABLE** - CV with proper clinical stratification and robust neuroimaging scaling
- Disease progression prediction üîß **PLACEHOLDER**
- Biomarker discovery and validation üîß **PLACEHOLDER**
- External cohort generalization testing üîß **FUTURE WORK**

#### Clinical-Stratified CV Features

**‚úÖ Currently Available:**
- CV folds stratified by clinical variables (diagnosis, subtypes, etc.)
- Robust scaling (median/MAD) optimized for neuroimaging data
- Enhanced convergence checking and timeout handling
- Professional error handling with automatic fallbacks

**üîß Future Development (Advanced Neuroimaging CV):**
- Neuroimaging-specific priors and spatial structure modeling
- Scanner/site effect correction and harmonization
- PD-specific disease progression constraints
- Brain connectivity and anatomical structure integration

### Run Experiments

```bash
# Run all experiments
python run_experiments.py --config config.yaml --experiments all

# Run specific experiments
python run_experiments.py --config config.yaml --experiments data_validation model_comparison

# Run advanced neuroimaging CV experiments
# ‚ö†Ô∏è WARNING: The following advanced experiments require additional development work:
python run_experiments.py --config config.yaml --experiments clinical_validation neuroimaging_cv_benchmarks neuroimaging_hyperopt

# Available experiment types:
# ‚úÖ IMPLEMENTED (with backwards compatibility):
# - data_validation: Data quality and preprocessing validation
# - sgfa_parameter_comparison: SGFA parameter optimization (falls back to basic CV)
# - model_comparison: SGFA vs traditional methods (falls back to basic CV)
# - performance_benchmarks: Scalability and memory benchmarks (most features work)
# - sensitivity_analysis: Hyperparameter sensitivity testing
# - reproducibility: Reproducibility and robustness validation
# - clinical_validation: Basic clinical validation (falls back to basic CV)
#
# üîß ADVANCED FEATURES NEED DEVELOPMENT:
# - neuroimaging_hyperopt: Neuroimaging-specific hyperparameter optimization
# - neuroimaging_cv_benchmarks: Clinical-aware cross-validation benchmarks
# - Advanced neuroimaging CV methods (ClinicalAwareSplitter, NeuroImagingMetrics)

# Run with custom data directory
python run_experiments.py --config config.yaml --data-dir /path/to/data

# Run specific experiments directly (programmatic)
python -c "
from experiments import DataValidationExperiments, ExperimentConfig
config = ExperimentConfig(output_dir='./results')
validator = DataValidationExperiments(config)
# Add your data and run experiments
"
```

## Testing

Run the comprehensive test suite:

```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=. --cov-report=html

# Run specific test modules
pytest tests/data/
pytest tests/analysis/
pytest tests/models/
pytest tests/experiments/
pytest tests/visualization/
```

## Configuration

### Basic Configuration

```python
from core.config_utils import get_default_configuration, validate_configuration
import yaml

# Get default configuration
config = get_default_configuration()

# Modify as needed
config['model']['K'] = 5
config['model']['sparsity_lambda'] = 0.3
config['model']['num_samples'] = 1000
config['model']['num_chains'] = 4

# Validate configuration
if validate_configuration(config):
    print("Configuration is valid!")
```

### Performance Configuration

```yaml
# config.yaml
memory:
  max_memory_gb: 8.0
  warning_threshold: 0.8
  enable_monitoring: true

data:
  enable_chunking: true
  memory_limit_gb: 4.0
  enable_compression: true

mcmc:
  enable_adaptive_batching: true
  enable_checkpointing: true
```

## Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Make your changes
4. Run tests (`pytest`)
5. Run linting (`black . && flake8`)
6. Commit your changes (`git commit -m 'Add amazing feature'`)
7. Push to the branch (`git push origin feature/amazing-feature`)
8. Open a Pull Request

## License

This project is licensed under the MIT License.

## References

- qMAP-PD Study: <https://qmaplab.com/qmap-pd>
- JAX Documentation: <https://jax.readthedocs.io/>
- NumPyro Documentation: <https://num.pyro.ai/>
