# SGFA qMAP-PD Configuration File
# Complete configuration with documentation and examples
# See docs/configuration.md for detailed documentation

# =============================================================================
# REQUIRED CONFIGURATION SECTIONS
# =============================================================================

# Data Configuration (REQUIRED)
# Specifies data sources and loading parameters
data:
  data_dir: "./qMAP-PD_data"              # REQUIRED: Path to data directory (must exist)
  clinical_file: "data_clinical/clinical.tsv"  # OPTIONAL: Clinical data file
  volume_dir: "volume_matrices"            # OPTIONAL: Volume matrices directory
  imaging_as_single_view: true            # OPTIONAL: Concatenate all imaging data (default: true)

# Experiment Configuration (REQUIRED)
# Controls experiment execution and output handling
experiments:
  base_output_dir: "./results"            # REQUIRED: Output directory for all results
  save_intermediate: false                # OPTIONAL: Save intermediate results for debugging (default: false, disk intensive)
  generate_plots: true                    # OPTIONAL: Generate visualization plots (default: true)
  enable_spatial_analysis: false         # OPTIONAL: Enable spatial analysis (requires MRI reference, default: false)
  save_pickle_results: false             # OPTIONAL: Save pickle files (can be large, default: false)
  save_numpy_arrays: false               # OPTIONAL: Save numpy arrays to disk (not human-readable, default: false)
  max_parallel_jobs: 2                   # OPTIONAL: Maximum parallel experiments (1-16, default: 1)

  # NOTE: SGFA results caching is automatically enabled when running multiple experiments
  # Results are cached in: {base_output_dir}/.sgfa_cache/
  # Cache is cleared automatically after pipeline completes
  # This significantly reduces runtime by reusing SGFA results with identical hyperparameters


# Model Configuration (REQUIRED)
# Defines model type and core hyperparameters
model:
  model_type: "sparse_gfa"               # REQUIRED: "sparse_gfa" or "standard_gfa"
  K: 8                                   # REQUIRED: Number of latent factors (1-50 recommended)

  # MCMC Parameters (OPTIONAL with defaults)
  num_samples: 2000                      # MCMC samples (10-50000, default: 1000)
  num_warmup: 1000                       # MCMC warmup steps (default: 500)
  num_chains: 4                          # MCMC chains (1-8, default: 2)

  # Sparsity Parameters (REQUIRED for sparse_gfa)
  sparsity_lambda: 0.05                  # Sparsity penalty (≥0, typically 0.01-1.0)
  group_lambda: 0.1                      # OPTIONAL: Group sparsity penalty

  # Reproducibility (OPTIONAL)
  random_seed: 42                        # Random seed for reproducible results

# =============================================================================
# OPTIONAL CONFIGURATION SECTIONS (All have sensible defaults)
# =============================================================================

# Preprocessing Configuration (OPTIONAL)
# Controls data preprocessing and feature selection
preprocessing:
  strategy: "differentiated_imaging_clinical"  # TEST: differentiated preprocessing for imaging vs clinical
  enable_advanced_preprocessing: true   # Enable smart preprocessing for speed
  enable_spatial_processing: false      # Keep disabled to preserve voxel spatial mapping
  imputation_strategy: "median"         # Handle missing values without removing voxels

  # Feature selection - ROI-aware for interpretability
  feature_selection_method: "variance"  # Use variance-based selection
  variance_threshold: 0.02              # Remove voxels with <2% variance (flat/noisy voxels)
  missing_threshold: 0.8                # Remove voxels with >80% missing data

  # ROI-aware selection (preserves proportional representation from each brain region)
  roi_based_selection: true             # Enable ROI-aware feature selection
  imaging_preserve_voxels: false        # Allow dropping low-variance voxels for speed
  n_top_features: 10000                 # Optional: Keep top 10K voxels total (faster runs)
  min_voxel_distance: 3.0               # Minimum distance between selected voxels (spatial distribution)

# Cross-Validation Configuration (OPTIONAL)
# Configures model evaluation and validation
cross_validation:
  n_folds: 10                           # Number of CV folds (2-20, recommended: 5-10)
  n_repeats: 3                          # Number of CV repeats (1-10, recommended: 1-3)
  stratified: true                      # Use stratified CV for imbalanced data
  group_aware: false                    # Use group-aware CV for multi-site data
  random_seed: 42                       # Random seed for CV splits

# Monitoring Configuration (OPTIONAL)
# Controls logging, checkpointing, and progress monitoring
monitoring:
  checkpoint_dir: "./results/checkpoints"  # Checkpoint directory
  log_level: "INFO"                     # "DEBUG"|"INFO"|"WARNING"|"ERROR"|"CRITICAL"
  save_checkpoints: true                # Save model checkpoints during training
  checkpoint_interval: 200              # Save checkpoint every N samples (≥1)

# System Configuration (OPTIONAL)
# Hardware and performance optimization settings
system:
  use_gpu: true                         # Use GPU acceleration (default: true)
  memory_limit_gb: 32.0                 # Memory limit in GB (auto-detect if not specified)
  n_cpu_cores: 8                        # Number of CPU cores (auto-detect if not specified)

# =============================================================================
# EXPERIMENT-SPECIFIC CONFIGURATIONS
# =============================================================================

# Data Validation Experiment
data_validation:
  preprocessing_strategies:
    minimal:
      enable_advanced_preprocessing: false
      imputation_strategy: "mean"

    standard:
      enable_advanced_preprocessing: true
      imputation_strategy: "median"
      feature_selection_method: "variance"
      variance_threshold: 0.01

    statistical:
      enable_advanced_preprocessing: true
      imputation_strategy: "median"
      feature_selection_method: "statistical"
      n_top_features: 500

    mutual_info:
      enable_advanced_preprocessing: true
      imputation_strategy: "median"
      feature_selection_method: "mutual_info"
      n_top_features: 500

    combined:
      enable_advanced_preprocessing: true
      imputation_strategy: "median"
      feature_selection_method: "combined"
      n_top_features: 500
      variance_threshold: 0.01

    spatial_preserving:
      enable_advanced_preprocessing: false
      enable_spatial_processing: false
      imputation_strategy: "median"
      feature_selection_method: "none"
      variance_threshold: 0.0
      missing_threshold: 1.0

    differentiated_imaging_clinical:
      enable_advanced_preprocessing: true
      enable_spatial_processing: false
      imputation_strategy: "median"
      feature_selection_method: "variance"         # Use variance-based selection
      variance_threshold: 0.01
      missing_threshold: 0.5
      imaging_preserve_voxels: true                # Preserve all imaging voxels for spatial mapping

    optimized:
      enable_advanced_preprocessing: true
      optimize_preprocessing: true
      cross_validate_sources: true

    clinical_focused:
      enable_advanced_preprocessing: true
      imputation_strategy: "median"
      feature_selection_method: "statistical"
      n_top_features: 300
      variance_threshold: 0.005           # More stringent for clinical applications

# Model Comparison Experiment
model_comparison:
  models:
    - name: "sparse_gfa"
      n_factors: [3, 5, 8, 10]           # Unified K values (memory-optimized)
      sparsity_lambda: [0.01, 0.1, 0.5]  # Different sparsity levels
      group_lambda: [0.1, 0.5]           # Group sparsity options

    - name: "standard_gfa"
      n_factors: [3, 5, 8, 10]           # Unified K values for comparison

  cross_validation:
    n_folds: 5                          # Cross-validation for model comparison
    n_repeats: 2

  evaluation_metrics:
    - "reconstruction_error"            # Model fit quality
    - "factor_interpretability"         # How interpretable are the factors
    - "clinical_correlation"            # Correlation with clinical measures

  # Comparative Benchmarks (moved from performance_benchmarks)
  comparative_benchmarks:
    baseline_methods: ["pca", "ica", "factor_analysis", "nmf"]  # Traditional methods for comparison
    test_configurations:
      small_scale:
        n_subjects: 500
        n_features_per_view: 100
        n_views: 2
      medium_scale:
        n_subjects: 1000
        n_features_per_view: 200
        n_views: 3
      large_scale:
        n_subjects: 2000
        n_features_per_view: 150
        n_views: 4
    comparison_metrics:
      - "execution_time"                # Method execution time comparison
      - "memory_usage"                  # Peak memory consumption comparison
      - "success_rate"                  # Method reliability comparison
      - "performance_efficiency"       # Time vs memory trade-offs

# SGFA Hyperparameter Tuning Experiment
sgfa_hyperparameter_tuning:
  parameter_ranges:
    n_factors: [2, 3, 4, 5]             # Range of factor numbers (tailored for 2-4 view experiments)
    sparsity_lambda: [0.1, 0.25, 0.33]  # Element-wise sparsity (percW: 10%, 25%, 33%) - controls sparsity within each factor
    group_lambda: [0.0, 0.05, 0.1, 0.2] # Group sparsity - encourages entire features (rows of W) to be zero across all factors
                                        # 0.0 = no group sparsity (baseline), higher values = stronger shrinkage (useful for multi-view feature selection)
    # learning_rate: [0.005, 0.01, 0.05]  # NOT APPLICABLE - NumPyro uses automatic step size adaptation

  # Comprehensive Scalability Analysis (moved from performance_benchmarks)
  scalability_analysis:
    sample_size_ranges: [50, 100, 250, 500, 1000, 2000, 5000]
    feature_size_ranges: [10, 25, 50, 100, 250, 500, 1000]
    component_ranges: [2, 3, 5, 8, 10, 15, 20]
    chain_ranges: [1, 2, 4, 8]

    benchmark_configs:
      small_scale:
        n_subjects: 50
        n_features_per_view: [500, 300]   # Small dataset for testing
      medium_scale:
        n_subjects: 100
        n_features_per_view: [1000, 800]  # Medium dataset
      full_scale:
        n_subjects: 200
        n_features_per_view: [2000, 1500] # Full dataset size

    scalability_metrics:
      - "training_time"                   # How long does SGFA training take
      - "memory_usage"                    # Peak memory consumption during SGFA
      - "convergence_rate"                # How quickly SGFA converges
      - "factor_extraction_time"          # Time to extract factor scores
      - "mcmc_sampling_efficiency"        # Effective sample size per second

  stability_tests:
    n_random_inits: 5                   # Test stability across random initializations
    convergence_threshold: 1e-5         # Convergence criterion

# Sensitivity Analysis Experiment
sensitivity_analysis:
  parameter_ranges:
    n_factors: [3, 5, 8, 10, 12]        # Range of factor numbers
    sparsity_lambda: [0.01, 0.1, 0.5, 1.0]  # Range of sparsity values
    learning_rate: [0.005, 0.01, 0.05]  # MCMC learning rates

  stability_tests:
    n_random_inits: 5                   # Test stability across random initializations
    convergence_threshold: 1e-5         # Convergence criterion

# Clinical Validation Experiment
clinical_validation:
  validation_types:
    - "pd_subtype_discovery"            # PD subtype discovery using unsupervised clustering
    # NOTE: "subtype_classification" disabled - requires real clinical subtype labels (not available in qMAP-PD)
    # - "subtype_classification"          # PD subtype prediction (requires predefined labels)
    # - "disease_progression"             # Disease progression modeling (placeholder - not implemented)
    # - "biomarker_discovery"             # Novel biomarker identification (placeholder - not implemented)

  # NOTE: Classification metrics only applicable if real clinical labels are available
  # For unsupervised subtype discovery, use clustering metrics instead (silhouette, stability, etc.)
  classification_metrics:
    - "accuracy"                        # Classification accuracy (requires labels)
    - "precision"                       # Precision score (requires labels)
    - "recall"                          # Recall score (requires labels)
    - "f1_score"                        # F1 score (requires labels)
    - "roc_auc"                         # ROC AUC (requires labels)

  cross_validation:
    n_folds: 10                         # Robust CV for clinical validation
    stratified: true                    # Maintain class balance

  # Integrated SGFA + Clinical Optimization (moved from performance_benchmarks)
  integrated_sgfa_clinical_optimization:
    sgfa_performance_metrics:
      - "training_time"                   # How long does SGFA training take
      - "memory_usage"                    # Peak memory consumption during SGFA
      - "convergence_rate"                # How quickly SGFA converges
      - "factor_extraction_time"          # Time to extract factor scores
      - "mcmc_sampling_efficiency"        # Effective sample size per second

    pd_subtype_discovery_metrics:
      - "subtype_stability_time"          # Time to achieve stable cluster assignments
      - "clinical_validation_speed"       # Speed of clinical correlation analysis
      - "optimal_k_detection_time"        # Time to find optimal number of subtypes
      - "clustering_quality_score"        # Silhouette/Calinski-Harabasz scores
      - "subtype_reproducibility"         # Consistency across random seeds

    clinical_translation_metrics:
      - "factor_interpretability_score"   # Clinical meaningfulness of factors
      - "subtype_clinical_separation"     # Clinical difference between subtypes
      - "biomarker_discovery_efficiency"  # Speed of finding clinical correlates
      - "cross_validation_stability"      # Subtype stability in CV folds

    # Performance-Discovery trade-off analysis
    trade_off_analysis:
      - "speed_vs_subtype_quality"        # SGFA speed impact on subtype quality
      - "memory_vs_stability"             # Memory optimization impact on reproducibility
      - "convergence_vs_clinical_validity" # Fast convergence vs clinical correlation

# Reproducibility Experiment
reproducibility:
  test_scenarios:
    - "identical_seeds"                 # Same random seed should give same results
    - "different_hardware"              # Results across different hardware
    - "version_stability"               # Results across software versions

  seed_values: [42, 123, 456, 789]     # Multiple seeds to test
  n_repetitions: 3                     # Repetitions per seed

  convergence_metrics:
    - "factor_correlation"              # Correlation between factor solutions
    - "parameter_stability"             # Stability of model parameters
    - "reconstruction_consistency"      # Consistency of reconstructed data

  tolerance_thresholds:
    correlation_threshold: 0.95         # Minimum correlation for reproducibility
    parameter_relative_error: 0.05      # Maximum relative error in parameters
    reconstruction_error_ratio: 0.02    # Maximum reconstruction error difference

# =============================================================================
# CONFIGURATION EXAMPLES FOR DIFFERENT USE CASES
# =============================================================================

# Example configurations for different scenarios:
#
# DEVELOPMENT (fast testing):
# model:
#   K: 3
#   num_samples: 100
#   num_chains: 1
#
# RESEARCH (balanced performance/quality):
# model:
#   K: 8
#   num_samples: 2000
#   num_chains: 2
#
# PRODUCTION (highest quality):
# model:
#   K: 10
#   num_samples: 5000
#   num_chains: 4
# cross_validation:
#   n_folds: 10
#   n_repeats: 5
#
# HIGH-PERFORMANCE (optimized for speed):
# model:
#   num_samples: 500
#   num_chains: 1
# preprocessing:
#   strategy: "minimal"
# system:
#   use_gpu: true
#   n_cpu_cores: 16