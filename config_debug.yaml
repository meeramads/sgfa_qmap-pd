# =============================================================================
# SGFA qMAP-PD DEBUG CONFIGURATION - OPTIMIZED FOR FAST TESTING
#
# This config provides minimal settings for rapid development and testing:
# üöÄ SPEED OPTIMIZATIONS:
#   - 50 MCMC samples (vs 2000 in production)
#   - 3 factors (vs 10 in production)
#   - No plots or intermediate saves
#   - Minimal preprocessing (no feature selection)
#   - 2 CV folds (vs 10 in production)
#   - CPU-only execution
#
# üß¨ PD SUBTYPE DISCOVERY FOCUS:
#   - Integrated SGFA performance + PD subtype discovery benchmarks
#   - Primary validation: pd_subtype_discovery (unsupervised clustering)
#   - Performance-discovery trade-off analysis
#   - Clinical translation metrics
#   - Multi-layer evaluation framework
#
# ‚è±Ô∏è  Expected runtime: ~1-2 minutes per experiment (vs 20-30 minutes)
# üéØ Use for: debugging, quick tests, CI/CD, PD subtype discovery development
# üìù Usage: python run_experiments.py --config config_debug.yaml
# =============================================================================

# Data Configuration - using synthetic data for speed
data:
  data_dir: "./qMAP-PD_data"              # Will fallback to synthetic if not found
  clinical_file: "data_clinical/clinical.tsv"
  volume_dir: "volume_matrices"
  imaging_as_single_view: true

# Experiment Configuration - minimal settings
experiments:
  base_output_dir: "./debug_results"      # Separate debug output
  save_intermediate: false                # Skip intermediate saves for speed
  generate_plots: false                   # Skip plots for speed
  max_parallel_jobs: 1                    # Single-threaded for easier debugging

# Model Configuration - minimal for fast execution
model:
  model_type: "sparse_gfa"
  K: 3                                    # Very small number of factors

  # MCMC Parameters - minimal for speed
  num_samples: 50                         # Very few samples
  num_warmup: 25                          # Minimal warmup
  num_chains: 1                           # Single chain

  # Sparsity Parameters
  sparsity_lambda: 0.1
  group_lambda: 0.1

  # Reproducibility
  random_seed: 42

# Preprocessing Configuration - MINIMAL FOR FAST TESTING
preprocessing:
  strategy: "minimal"                     # Fastest preprocessing strategy
  enable_advanced_preprocessing: false   # Skip advanced features for speed
  enable_spatial_processing: false       # Skip spatial processing
  imputation_strategy: "mean"            # Fastest imputation method
  feature_selection_method: "none"       # Skip feature selection for speed
  variance_threshold: 0.0                # Don't filter low-variance features
  missing_threshold: 1.0                 # Don't drop any features (keep all data)

# Cross-Validation Configuration - minimal CV
cross_validation:
  n_folds: 2                              # Minimal folds
  n_repeats: 1                            # Single repeat
  stratified: true
  group_aware: false
  random_seed: 42

# Monitoring Configuration
monitoring:
  checkpoint_dir: "./debug_results/checkpoints"
  log_level: "DEBUG"                      # Verbose logging for debugging
  save_checkpoints: false                 # Skip checkpoints for speed
  checkpoint_interval: 50

# System Configuration - CONSERVATIVE FOR TESTING
system:
  use_gpu: false                          # Use CPU for consistent debugging (no GPU variability)
  memory_limit_gb: 4.0                    # Conservative memory limit
  n_cpu_cores: 2                          # Limit CPU usage for reproducible timing

# =============================================================================
# DEBUG EXPERIMENT-SPECIFIC CONFIGURATIONS
# =============================================================================

# Data Validation Experiment - single strategy
data_validation:
  preprocessing_strategies:
    minimal:
      enable_advanced_preprocessing: false
      imputation_strategy: "mean"

    aggressive:
      enable_advanced_preprocessing: true
      enable_spatial_processing: true
      imputation_strategy: "knn"
      feature_selection_method: "variance"
      variance_threshold: 0.005

# Method Comparison Experiment - PD subtype discovery focused
method_comparison:
  models:
    - name: "sparse_gfa"
      n_factors: [3]                      # Single factor count
      sparsity_lambda: [0.1]              # Single sparsity level
      group_lambda: [0.1]

    - name: "standard_gfa"
      n_factors: [3]

  cross_validation:
    n_folds: 2
    n_repeats: 1

  evaluation_metrics:
    - "reconstruction_error"              # Traditional SGFA metric
    - "subtype_discovery_quality"         # NEW: How well does model discover PD subtypes
    - "clinical_correlation"              # NEW: Correlation with clinical measures

# Sensitivity Analysis Experiment - minimal parameter ranges
sensitivity_analysis:
  parameter_ranges:
    n_factors: [2, 3]                     # Minimal range
    sparsity_lambda: [0.05, 0.1]          # Two values only
    learning_rate: [0.01]                 # Single value

  stability_tests:
    n_random_inits: 2                     # Minimal random inits
    convergence_threshold: 1e-3           # Relaxed convergence

# Performance Benchmarks Experiment - integrated SGFA + PD subtype discovery
performance_benchmarks:
  benchmark_configs:
    tiny_scale:
      n_subjects: 20                      # Very small
      n_features_per_view: [50, 30]       # Tiny feature sets

    small_scale:
      n_subjects: 30
      n_features_per_view: [100, 80]

  # Multi-layer metrics framework (debug version)
  sgfa_performance_metrics:
    - "training_time"                     # Basic SGFA timing
    - "memory_usage"                      # Memory consumption
    - "factor_extraction_time"            # Time to extract factors

  pd_subtype_discovery_metrics:
    - "subtype_stability_time"            # Time for stable clustering
    - "clustering_quality_score"          # Silhouette scores
    - "optimal_k_detection_time"          # Time to find best k

  clinical_translation_metrics:
    - "clinical_validation_speed"         # Speed of clinical validation
    - "subtype_clinical_separation"       # Clinical difference between subtypes

  # Performance-Discovery trade-off analysis (debug version)
  trade_off_analysis:
    - "speed_vs_subtype_quality"          # SGFA speed vs clustering quality
    - "memory_vs_stability"               # Memory usage vs reproducibility

# Clinical Validation Experiment - PD subtype discovery focused
clinical_validation:
  validation_types:
    - "pd_subtype_discovery"              # Primary: unsupervised PD subtype discovery
    - "subtype_classification"            # Secondary: supervised classification (if labels available)

  classification_metrics:
    - "accuracy"                          # Basic metric for supervised validation

  # PD subtype discovery specific settings
  subtype_discovery_settings:
    max_subtypes: 4                       # Test 2-4 subtypes (common in PD research)
    clustering_methods: ["kmeans"]        # Just KMeans for debug speed
    stability_runs: 2                     # Minimal stability testing

  cross_validation:
    n_folds: 2                            # Minimal folds
    stratified: true

# Reproducibility Experiment - minimal testing
reproducibility:
  test_scenarios:
    - "identical_seeds"                   # Only test identical seeds

  seed_values: [42, 123]                  # Two seeds only
  n_repetitions: 1                        # Single repetition

  convergence_metrics:
    - "factor_correlation"

  tolerance_thresholds:
    correlation_threshold: 0.8            # Relaxed threshold
    parameter_relative_error: 0.1         # Relaxed error tolerance
    reconstruction_error_ratio: 0.05      # Relaxed reconstruction tolerance

# =============================================================================
# ALTERNATIVE TEST CONFIGURATIONS
# To use these, copy and replace sections above, or create separate config files
# =============================================================================

# ULTRA-FAST CONFIG (for CI/CD or quick smoke tests):
# model:
#   K: 2                                  # Only 2 factors
#   num_samples: 20                       # Only 20 samples
#   num_warmup: 10                        # Only 10 warmup
#   num_chains: 1                         # Single chain
# experiments:
#   run_experiments: ["data_validation"]  # Only run one experiment
#
# MEDIUM SPEED CONFIG (for thorough testing):
# model:
#   K: 5                                  # Medium factors
#   num_samples: 100                      # More samples
#   num_warmup: 50                        # More warmup
# preprocessing:
#   feature_selection_method: "variance"  # Test feature selection
#   variance_threshold: 0.01              # Test thresholding