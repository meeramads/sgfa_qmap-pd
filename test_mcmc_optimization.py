#!/usr/bin/env python3
"""
Test script for MCMC efficiency parameter optimization in remote workstation pipeline.
"""

import sys
import logging
import numpy as np
from pathlib import Path

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def test_mcmc_optimization():
    """Test the MCMC efficiency parameter optimization functionality."""
    print("Testing MCMC efficiency parameter optimization...")

    # Mock data similar to qMAP-PD structure
    np.random.seed(42)
    n_subjects = 50

    # Create multi-view data
    X_list = [
        np.random.randn(n_subjects, 1000),  # View 1: cortical thickness
        np.random.randn(n_subjects, 2000),  # View 2: subcortical volumes
        np.random.randn(n_subjects, 500),   # View 3: DTI metrics
        np.random.randn(n_subjects, 100)    # View 4: clinical features
    ]

    print(f"Mock data created: {len(X_list)} views with shapes {[X.shape for X in X_list]}")

    # Test the enhanced quality evaluation function with MCMC parameters
    try:
        from itertools import product
        from sklearn.decomposition import FactorAnalysis
        from sklearn.metrics import r2_score

        def test_evaluate_mcmc_quality(params, X_list):
            """Test version of the enhanced quality evaluation function with MCMC parameters."""
            try:
                # Extract parameters
                K = params.get('K', 10)
                percW = params.get('percW', 33)
                num_samples = params.get('num_samples', 2000)
                num_warmup = params.get('num_warmup', 1000)
                num_chains = params.get('num_chains', 4)
                target_accept_prob = params.get('target_accept_prob', 0.8)

                X_concat = np.concatenate(X_list, axis=1)
                n_subjects, total_features = X_concat.shape

                # Quick factor analysis
                fa = FactorAnalysis(n_components=K, random_state=42, max_iter=100)
                fa.fit(X_concat)

                # Reconstruction quality
                X_recon = fa.transform(X_concat) @ fa.components_
                recon_r2 = r2_score(X_concat, X_recon)

                # Enhanced sparsity evaluation based on percW
                optimal_percW_range = [25, 33, 40]
                if percW in optimal_percW_range:
                    percW_score = 0.9
                elif percW in [20, 50]:
                    percW_score = 0.8
                elif percW in [15, 60, 67]:
                    percW_score = 0.7
                else:
                    percW_score = 0.5

                # K-based sparsity
                K_sparsity_score = max(0, 1.0 - (K / 20.0))
                sparsity_score = 0.6 * percW_score + 0.4 * K_sparsity_score

                # MCMC EFFICIENCY EVALUATION
                # Sample efficiency
                optimal_samples_range = [1000, 2000]
                if num_samples in optimal_samples_range:
                    sample_efficiency = 0.9
                elif num_samples in [500, 3000]:
                    sample_efficiency = 0.8
                elif num_samples < 500:
                    sample_efficiency = 0.6
                else:
                    sample_efficiency = 0.7

                # Warmup efficiency
                warmup_ratio = num_warmup / num_samples
                if 0.4 <= warmup_ratio <= 0.6:
                    warmup_efficiency = 0.9
                elif 0.3 <= warmup_ratio <= 0.7:
                    warmup_efficiency = 0.8
                else:
                    warmup_efficiency = 0.6

                # Chain efficiency
                available_cores = min(8, max(1, total_features // 1000))
                if num_chains <= available_cores and num_chains >= 2:
                    chain_efficiency = 0.9
                elif num_chains == 1:
                    chain_efficiency = 0.7
                else:
                    chain_efficiency = 0.6

                # Accept probability efficiency
                if 0.75 <= target_accept_prob <= 0.85:
                    accept_efficiency = 0.9
                elif 0.7 <= target_accept_prob <= 0.9:
                    accept_efficiency = 0.8
                else:
                    accept_efficiency = 0.6

                # Combined MCMC efficiency
                mcmc_efficiency = (0.3 * sample_efficiency +
                                 0.25 * warmup_efficiency +
                                 0.25 * chain_efficiency +
                                 0.2 * accept_efficiency)

                # Memory penalty
                estimated_memory_gb = (num_samples * num_chains * K * total_features * 4) / (1024**3)
                memory_penalty = max(0, (estimated_memory_gb - 16) / 16 * 0.2)

                # Clinical relevance
                if K in [8, 10, 12] and percW in optimal_percW_range:
                    clinical_score = 0.9
                elif K in [5, 6, 7, 13, 14, 15] and percW in [20, 25, 40, 50]:
                    clinical_score = 0.8
                else:
                    clinical_score = 0.6

                # Composite interpretability score
                interpretability = (
                    0.35 * sparsity_score +
                    0.25 * 0.8 +  # orthogonality
                    0.2 * 0.6 +   # spatial coherence
                    0.2 * clinical_score
                )

                # Combined quality score (40% interpretability + 30% reconstruction + 30% MCMC efficiency)
                quality_score = (0.4 * interpretability +
                               0.3 * max(0, recon_r2) +
                               0.3 * mcmc_efficiency) - memory_penalty

                quality_score = max(0.0, min(1.0, quality_score))

                return quality_score, mcmc_efficiency, estimated_memory_gb

            except Exception as e:
                logger.warning(f"Quality evaluation failed for K={K}, percW={percW}, samples={num_samples}: {e}")
                return 0.0, 0.0, 0.0

        # Test joint optimization with MCMC parameters
        K_candidates = [5, 10, 12]
        percW_candidates = [25, 33, 40]
        num_samples_candidates = [1000, 2000]
        num_chains_candidates = [2, 4]

        print(f"\nTesting joint K, percW, and MCMC optimization:")
        print(f"K candidates: {K_candidates}")
        print(f"percW candidates: {percW_candidates}")
        print(f"Samples candidates: {num_samples_candidates}")
        print(f"Chains candidates: {num_chains_candidates}")
        print("-" * 80)

        best_score = -1
        best_params = {'K': 10, 'percW': 33, 'num_samples': 2000, 'num_chains': 4}
        all_results = {}

        for K, percW, num_samples, num_chains in product(K_candidates, percW_candidates, num_samples_candidates, num_chains_candidates):
            num_warmup = num_samples // 2  # Auto-determine warmup
            params = {
                'K': K,
                'percW': percW,
                'num_samples': num_samples,
                'num_warmup': num_warmup,
                'num_chains': num_chains,
                'target_accept_prob': 0.8
            }

            score, mcmc_efficiency, estimated_memory = test_evaluate_mcmc_quality(params, X_list)

            result_key = f'K{K}_percW{percW}_samples{num_samples}_chains{num_chains}'
            all_results[result_key] = {
                'K': K, 'percW': percW, 'num_samples': num_samples, 'num_chains': num_chains,
                'score': score, 'mcmc_efficiency': mcmc_efficiency, 'estimated_memory_gb': estimated_memory
            }

            print(f"K={K:2d}, percW={percW:2d}, samples={num_samples:4d}, chains={num_chains}: "
                  f"Score={score:.4f}, MCMC_eff={mcmc_efficiency:.3f}, Mem={estimated_memory:.2f}GB")

            if score > best_score:
                best_score = score
                best_params = params

        print("\n" + "="*80)
        print("JOINT K, PERCW, AND MCMC OPTIMIZATION RESULTS:")
        print(f"OPTIMAL COMBINATION:")
        print(f"  K={best_params['K']}, percW={best_params['percW']}")
        print(f"  num_samples={best_params['num_samples']}, num_chains={best_params['num_chains']}")
        print(f"  Quality Score: {best_score:.4f}")

        # Show top 5 combinations
        sorted_results = sorted(all_results.items(), key=lambda x: x[1]['score'], reverse=True)
        print("\nTop 5 parameter combinations:")
        for i, (key, result) in enumerate(sorted_results[:5]):
            marker = " <-- OPTIMAL" if i == 0 else ""
            print(f"  {i+1}. K={result['K']:2d}, percW={result['percW']:2d}, "
                  f"samples={result['num_samples']:4d}, chains={result['num_chains']}: "
                  f"{result['score']:.4f} (MCMC_eff={result['mcmc_efficiency']:.3f}){marker}")

        print("="*80)

        print(f"\nâœ… Test completed successfully!")
        print(f"   Joint optimization selected: K={best_params['K']}, percW={best_params['percW']}")
        print(f"   MCMC config: {best_params['num_samples']} samples, {best_params['num_chains']} chains")
        print(f"   Quality score: {best_score:.4f}")

        return best_params, best_score

    except Exception as e:
        print(f"\nâŒ Test failed: {str(e)}")
        import traceback
        traceback.print_exc()
        return None, None

if __name__ == "__main__":
    optimal_params, score = test_mcmc_optimization()

    if optimal_params is not None:
        print(f"\nðŸŽ¯ Joint K, percW, and MCMC optimization working correctly!")
        print(f"   The system would select:")
        print(f"   - K={optimal_params['K']} factors")
        print(f"   - percW={optimal_params['percW']}% sparsity")
        print(f"   - {optimal_params['num_samples']} MCMC samples")
        print(f"   - {optimal_params['num_chains']} parallel chains")
        print(f"   This demonstrates comprehensive hyperparameter optimization is ready!")
    else:
        print(f"\nâš ï¸  Test encountered issues - check implementation")