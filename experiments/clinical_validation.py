"""Clinical validation experiments for SGFA qMAP-PD analysis."""

import gc
import logging
import time
from datetime import datetime
from typing import Dict, List, Optional

import jax
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from scipy import stats
from scipy.stats import f_oneway
from sklearn.cluster import KMeans
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (
    accuracy_score,
    calinski_harabasz_score,
    confusion_matrix,
    f1_score,
    precision_score,
    recall_score,
    roc_auc_score,
    silhouette_score,
)
from sklearn.model_selection import StratifiedKFold, cross_val_score
from sklearn.svm import SVC

from core.config_utils import get_data_dir, get_output_dir
from core.experiment_utils import experiment_handler
from core.validation_utils import validate_data_types, validate_parameters
from experiments.framework import (
    ExperimentConfig,
    ExperimentFramework,
    ExperimentResult,
)
from optimization import PerformanceProfiler
from optimization.experiment_mixins import performance_optimized_experiment
from analysis.cross_validation_library import (
    ClinicalAwareSplitter,
    NeuroImagingCVConfig,
    NeuroImagingMetrics
)
from analysis.cv_fallbacks import CVFallbackHandler, MetricsFallbackHandler
from visualization.subtype_plots import PDSubtypeVisualizer

# Import extracted clinical validation modules
from analysis.clinical import (
    ClinicalMetrics,
    ClinicalDataProcessor,
    ClinicalClassifier,
    PDSubtypeAnalyzer,
    DiseaseProgressionAnalyzer,
    BiomarkerAnalyzer,
    ExternalValidator
)


@performance_optimized_experiment()
class ClinicalValidationExperiments(ExperimentFramework):
    """Comprehensive clinical validation experiments for SGFA qMAP-PD analysis."""

    def __init__(
        self, config: ExperimentConfig, logger: Optional[logging.Logger] = None
    ):
        super().__init__(config, None, logger)
        self.profiler = PerformanceProfiler()

        # Performance optimization now handled by @performance_optimized_experiment decorator

        # Initialize neuroimaging-specific cross-validation from config
        from core.config_utils import ConfigHelper
        config_dict = ConfigHelper.to_dict(config)
        cv_settings = config_dict.get("cross_validation", {})

        self.neuroimaging_cv_config = NeuroImagingCVConfig()
        self.neuroimaging_cv_config.outer_cv_folds = cv_settings.get("n_folds", 5)

        # Initialize clinical-aware splitter
        self.clinical_splitter = ClinicalAwareSplitter(config=self.neuroimaging_cv_config)

        # Initialize neuroimaging metrics
        self.neuroimaging_metrics = NeuroImagingMetrics()

        # Initialize fallback handlers
        self.cv_fallback = CVFallbackHandler(self.logger)
        self.metrics_fallback = MetricsFallbackHandler(self.logger)

        # Clinical validation settings
        self.classification_models = {
            "logistic_regression": LogisticRegression(random_state=42, max_iter=1000),
            "random_forest": RandomForestClassifier(random_state=42, n_estimators=100),
            "svm": SVC(random_state=42, probability=True),
        }

        # Clinical metrics to evaluate (from config)
        from core.config_utils import ConfigHelper
        config_dict = ConfigHelper.to_dict(config)
        clinical_config = config_dict.get("clinical_validation", {})
        self.clinical_metrics_list = clinical_config.get("classification_metrics", [
            "accuracy", "precision", "recall", "f1_score", "roc_auc"
        ])

        # Initialize extracted clinical validation modules
        self.metrics_calculator = ClinicalMetrics(
            metrics_list=self.clinical_metrics_list,
            logger=self.logger
        )
        self.data_processor = ClinicalDataProcessor(logger=self.logger)
        self.classifier = ClinicalClassifier(
            metrics_calculator=self.metrics_calculator,
            classification_models=self.classification_models,
            config=config_dict,
            logger=self.logger
        )
        self.subtype_analyzer = PDSubtypeAnalyzer(
            data_processor=self.data_processor,
            logger=self.logger
        )
        self.progression_analyzer = DiseaseProgressionAnalyzer(
            classifier=self.classifier,
            logger=self.logger
        )
        self.biomarker_analyzer = BiomarkerAnalyzer(
            data_processor=self.data_processor,
            config=config_dict,
            logger=self.logger
        )
        self.external_validator = ExternalValidator(logger=self.logger)

    @experiment_handler("neuroimaging_clinical_validation")
    @validate_data_types(
        X_list=list, clinical_data=dict, hypers=dict, args=dict
    )
    @validate_parameters(
        X_list=lambda x: len(x) > 0,
        clinical_data=lambda x: isinstance(x, dict) and len(x.get('diagnosis', [])) > 0,
        hypers=lambda x: isinstance(x, dict),
    )
    def run_neuroimaging_clinical_validation(
        self,
        X_list: List[np.ndarray],
        clinical_data: Dict,
        hypers: Dict,
        args: Dict,
        **kwargs
    ) -> ExperimentResult:
        """Run comprehensive clinical validation using neuroimaging-specific cross-validation."""
        self.logger.info("ðŸ¥ Starting neuroimaging-specific clinical validation")

        # Memory-optimized processing using consolidated pattern
        with self.memory_optimized_context():

            # Optimize arrays using mixin method
            X_list_optimized, total_memory_saved = self.optimize_arrays_for_memory(X_list)

            # Validate clinical data structure
            required_fields = ['diagnosis', 'subject_id']
            for field in required_fields:
                if field not in clinical_data:
                    raise ValueError(f"Clinical data missing required field: {field}")

            diagnoses = np.array(clinical_data['diagnosis'])
            n_subjects = len(diagnoses)

            # Ensure X_list matches clinical data
            if X_list_optimized[0].shape[0] != n_subjects:
                raise ValueError(f"Data size mismatch: X_list has {X_list_optimized[0].shape[0]} subjects, clinical_data has {n_subjects}")

            # Calculate adaptive batch size using mixin method
            batch_size = self.calculate_adaptive_batch_size(
                X_list_optimized, operation_type="cv"
            )

            results = {}

            try:
                # 1. Train SGFA model using neuroimaging-aware CV
                self.logger.info("Training SGFA model with neuroimaging-specific CV")
                sgfa_cv_results = self._train_sgfa_with_neuroimaging_cv(
                    X_list_optimized, clinical_data, hypers, args, batch_size=batch_size
                )
                results["sgfa_cv_results"] = sgfa_cv_results

                # 2. Extract latent factors for clinical prediction
                if sgfa_cv_results.get("success", False):
                    self.logger.info("Extracting latent factors for clinical validation")
                    factors_cv_results = self.classifier.validate_factors_clinical_prediction(
                        sgfa_cv_results, clinical_data
                    )
                    results["factors_cv_results"] = factors_cv_results

                    # 3. Biomarker discovery with neuroimaging metrics
                    self.logger.info("Performing biomarker discovery with neuroimaging metrics")
                    biomarker_results = self.biomarker_analyzer.discover_neuroimaging_biomarkers(
                        sgfa_cv_results, clinical_data
                    )
                    results["biomarker_results"] = biomarker_results

                    # 4. Clinical subtype analysis
                    self.logger.info("Analyzing clinical subtypes with neuroimaging factors")
                    subtype_results = self.subtype_analyzer.analyze_clinical_subtypes_neuroimaging(
                        sgfa_cv_results, clinical_data
                    )
                    results["subtype_results"] = subtype_results

                else:
                    self.logger.warning("SGFA CV training failed, skipping downstream analyses")
                    results["factors_cv_results"] = {"error": "SGFA training failed"}
                    results["biomarker_results"] = {"error": "SGFA training failed"}
                    results["subtype_results"] = {"error": "SGFA training failed"}

                # 5. Overall clinical validation analysis
                analysis = self._analyze_neuroimaging_clinical_validation(results)

                # 6. Generate comprehensive plots
                plots = self._plot_neuroimaging_clinical_validation(results, clinical_data)

                return ExperimentResult(
                    experiment_id="neuroimaging_clinical_validation",
                    config=self.config,
                    model_results=results,
                    diagnostics=analysis,
                    plots=plots,
                    status="completed",
                )

            except Exception as e:
                self.logger.error(f"Neuroimaging clinical validation failed: {str(e)}")
                return ExperimentResult(
                    experiment_id="neuroimaging_clinical_validation",
                    config=self.config,
                    model_results={"error": str(e)},
                    diagnostics={},
                    plots={},
                    status="failed",
                    error_message=str(e),
                )

    def _train_sgfa_with_neuroimaging_cv(
        self, X_list: List[np.ndarray], clinical_data: Dict, hypers: Dict, args: Dict, batch_size: int = None
    ) -> Dict:
        """Train SGFA using neuroimaging-specific cross-validation."""
        try:
            # Generate CV splits with automatic fallback
            splits = self.cv_fallback.with_cv_split_fallback(
                advanced_split_func=self.clinical_splitter.split,
                X=X_list[0],
                y=clinical_data.get("diagnosis"),
                groups=clinical_data.get("subject_id"),
                clinical_data=clinical_data,
                cv_folds=self.neuroimaging_cv_config.outer_cv_folds,
                random_state=42
            )

            self.logger.info(f"Generated {len(splits)} neuroimaging-aware CV folds")

            fold_results = []

            for fold_idx, (train_idx, test_idx) in enumerate(splits):
                self.logger.info(f"Processing fold {fold_idx + 1}/{len(splits)}")

                # Memory-efficient data splitting using mixin methods
                # Split data using memory-efficient indexing
                X_train = [X[train_idx] for X in X_list]
                X_test = [X[test_idx] for X in X_list]

                # Optimize arrays using mixin methods
                X_train, _ = self.optimize_arrays_for_memory(X_train)
                X_test, _ = self.optimize_arrays_for_memory(X_test)

                # Train SGFA with memory monitoring using mixin
                try:
                    with self.profiler.profile(f"sgfa_fold_{fold_idx}") as p:
                        train_result = self.memory_efficient_operation(
                            self._run_sgfa_training, X_train, hypers, args
                        )

                    # Evaluate on test set
                    if train_result.get("convergence", False):
                        test_metrics = self._evaluate_sgfa_test_set(
                            X_test, train_result, test_idx, clinical_data
                        )

                        # Calculate neuroimaging-specific metrics with fallback
                        fold_clinical_data = {
                            key: np.array(values)[test_idx] if isinstance(values, (list, np.ndarray)) else values
                            for key, values in clinical_data.items()
                        }
                        fold_info = {
                            "fold_idx": fold_idx,
                            "train_size": len(train_idx),
                            "test_size": len(test_idx)
                        }

                        neuroimaging_metrics = self.metrics_fallback.with_metrics_fallback(
                            advanced_metrics_func=self.neuroimaging_metrics.calculate_fold_metrics,
                            fallback_metrics=self.metrics_fallback.create_basic_fold_metrics(
                                train_result, test_metrics, fold_idx
                            ),
                            train_result=train_result,
                            test_metrics=test_metrics,
                            clinical_data=fold_clinical_data,
                            fold_info=fold_info
                        )

                        fold_results.append({
                            "fold_idx": fold_idx,
                            "train_idx": train_idx,
                            "test_idx": test_idx,
                            "train_result": train_result,
                            "test_metrics": test_metrics,
                            "neuroimaging_metrics": neuroimaging_metrics,
                            "performance_metrics": self.profiler.get_current_metrics(),
                            "success": True
                        })

                        self.logger.info(
                            f"âœ… Fold {fold_idx + 1}: convergence={train_result.get('convergence', False)}, "
                            f"neuro_score={neuroimaging_metrics.get('overall_score', 'N/A'):.3f}"
                        )

                    else:
                        fold_results.append({
                            "fold_idx": fold_idx,
                            "train_idx": train_idx,
                            "test_idx": test_idx,
                            "error": "Model did not converge",
                            "success": False
                        })
                        self.logger.warning(f"âŒ Fold {fold_idx + 1}: Model did not converge")

                except Exception as e:
                    fold_results.append({
                        "fold_idx": fold_idx,
                        "train_idx": train_idx,
                        "test_idx": test_idx,
                        "error": str(e),
                        "success": False
                    })
                    self.logger.warning(f"âŒ Fold {fold_idx + 1} failed: {str(e)}")

            # Aggregate results
            successful_folds = [f for f in fold_results if f.get("success", False)]
            cv_summary = {
                "n_folds": len(splits),
                "successful_folds": len(successful_folds),
                "success_rate": len(successful_folds) / len(splits) if splits else 0,
                "fold_results": fold_results
            }

            if successful_folds:
                # Aggregate neuroimaging metrics
                neuro_scores = [f["neuroimaging_metrics"].get("overall_score", 0) for f in successful_folds]
                cv_summary["mean_neuroimaging_score"] = np.mean(neuro_scores)
                cv_summary["std_neuroimaging_score"] = np.std(neuro_scores)

                # Aggregate performance metrics
                exec_times = [f["performance_metrics"].get("execution_time", 0) for f in successful_folds]
                cv_summary["mean_execution_time"] = np.mean(exec_times)

                cv_summary["success"] = True
            else:
                cv_summary["success"] = False
                cv_summary["error"] = "No successful folds"

            return cv_summary

        except Exception as e:
            self.logger.error(f"SGFA neuroimaging CV training failed: {str(e)}")

            # Cleanup memory on failure
            jax.clear_caches()
            gc.collect()

            return {"success": False, "error": str(e)}

    def _analyze_neuroimaging_clinical_validation(self, results: Dict) -> Dict:
        """Analyze overall neuroimaging clinical validation results."""
        try:
            analysis = {
                "overall_performance": {},
                "neuroimaging_contribution": {},
                "clinical_impact": {},
                "recommendations": []
            }

            # Overall SGFA performance
            sgfa_results = results.get("sgfa_cv_results", {})
            if sgfa_results.get("success", False):
                analysis["overall_performance"] = {
                    "cv_success_rate": sgfa_results.get("success_rate", 0),
                    "mean_neuroimaging_score": sgfa_results.get("mean_neuroimaging_score", 0),
                    "mean_execution_time": sgfa_results.get("mean_execution_time", 0),
                    "n_successful_folds": sgfa_results.get("successful_folds", 0)
                }

                # Clinical prediction performance
                factors_results = results.get("factors_cv_results", {})
                if factors_results.get("success", False):
                    analysis["clinical_impact"] = {
                        "prediction_accuracy": factors_results.get("accuracy", 0),
                        "prediction_f1_score": factors_results.get("f1_score", 0),
                        "clinical_utility": "high" if factors_results.get("accuracy", 0) > 0.8 else "moderate"
                    }

                # Biomarker discovery
                biomarker_results = results.get("biomarker_results", {})
                if biomarker_results.get("success", False):
                    n_discriminative = len(biomarker_results.get("discriminative_factors", []))
                    analysis["neuroimaging_contribution"] = {
                        "discriminative_factors_found": n_discriminative,
                        "biomarker_potential": "high" if n_discriminative > 0 else "low"
                    }

                # Generate recommendations
                if analysis["overall_performance"]["cv_success_rate"] > 0.8:
                    analysis["recommendations"].append("SGFA model shows robust performance across CV folds")

                if analysis.get("clinical_impact", {}).get("prediction_accuracy", 0) > 0.75:
                    analysis["recommendations"].append("Latent factors have strong clinical predictive value")

                if analysis.get("neuroimaging_contribution", {}).get("discriminative_factors_found", 0) > 0:
                    analysis["recommendations"].append("Potential neuroimaging biomarkers identified")

            analysis["validation_success"] = len(analysis["recommendations"]) > 0

            return analysis

        except Exception as e:
            return {"error": f"Analysis failed: {str(e)}"}

    def _plot_neuroimaging_clinical_validation(self, results: Dict, clinical_data: Dict) -> Dict:
        """Generate plots for neuroimaging clinical validation."""
        self.logger.info("ðŸ“Š Generating neuroimaging clinical validation plots...")
        plots = {}

        try:
            sgfa_results = results.get("sgfa_cv_results", {})
            if not sgfa_results.get("success", False):
                return plots

            self.logger.debug("   Creating 4-panel clinical validation plot...")
            fig, axes = plt.subplots(2, 2, figsize=(15, 12))
            fig.suptitle("Neuroimaging Clinical Validation Results", fontsize=16)

            # Plot 1: CV Performance Across Folds
            fold_results = sgfa_results.get("fold_results", [])
            successful_folds = [f for f in fold_results if f.get("success", False)]

            if successful_folds:
                fold_indices = [f["fold_idx"] for f in successful_folds]
                neuro_scores = [f["neuroimaging_metrics"].get("overall_score", 0) for f in successful_folds]

                axes[0, 0].plot(fold_indices, neuro_scores, 'o-', markersize=8, linewidth=2)
                axes[0, 0].set_xlabel("Fold Index")
                axes[0, 0].set_ylabel("Neuroimaging Score")
                axes[0, 0].set_title("CV Performance Across Folds")
                axes[0, 0].grid(True, alpha=0.3)
                axes[0, 0].set_ylim([0, 1])

            # Plot 2: Clinical Prediction Performance
            factors_results = results.get("factors_cv_results", {})
            if factors_results.get("success", False):
                metrics = ["accuracy", "f1_score", "precision", "recall"]
                values = [factors_results.get(m, 0) for m in metrics]

                bars = axes[0, 1].bar(metrics, values, color=['blue', 'green', 'orange', 'red'], alpha=0.7)
                axes[0, 1].set_ylabel("Score")
                axes[0, 1].set_title("Clinical Prediction Performance")
                axes[0, 1].set_ylim([0, 1])

                # Add value labels
                for bar, value in zip(bars, values):
                    height = bar.get_height()
                    axes[0, 1].text(bar.get_x() + bar.get_width()/2., height + 0.01,
                                   f'{value:.3f}', ha='center', va='bottom')

            # Plot 3: Biomarker Discovery
            biomarker_results = results.get("biomarker_results", {})
            if biomarker_results.get("success", False):
                discriminative_factors = biomarker_results.get("discriminative_factors", [])

                if discriminative_factors:
                    factor_indices = [f["factor_idx"] for f in discriminative_factors]
                    p_values = [f["p_value"] for f in discriminative_factors]

                    # Plot as -log10(p-value) for better visualization
                    neg_log_p = [-np.log10(p) for p in p_values]

                    scatter = axes[1, 0].scatter(factor_indices, neg_log_p, s=100, alpha=0.7)
                    axes[1, 0].axhline(y=-np.log10(0.05), color='red', linestyle='--',
                                     label='p=0.05 threshold')
                    axes[1, 0].set_xlabel("Factor Index")
                    axes[1, 0].set_ylabel("-log10(p-value)")
                    axes[1, 0].set_title("Discriminative Factors")
                    axes[1, 0].legend()
                    axes[1, 0].grid(True, alpha=0.3)

            # Plot 4: Diagnosis Distribution
            diagnoses = clinical_data.get("diagnosis", [])
            if diagnoses:
                unique_diag, counts = np.unique(diagnoses, return_counts=True)
                colors = plt.cm.Set3(np.linspace(0, 1, len(unique_diag)))

                wedges, texts, autotexts = axes[1, 1].pie(counts, labels=unique_diag, colors=colors,
                                                         autopct='%1.1f%%', startangle=90)
                axes[1, 1].set_title("Clinical Diagnosis Distribution")

            plt.tight_layout()
            plots["neuroimaging_clinical_validation"] = fig
            self.logger.debug("   âœ… Neuroimaging clinical validation plot created")

        except Exception as e:
            self.logger.warning(f"Failed to create neuroimaging clinical validation plots: {str(e)}")

        # Add clinical factor loadings visualization
        try:
            sgfa_results = results.get("sgfa_cv_results", {})
            fold_results = sgfa_results.get("fold_results", [])

            # Find a successful fold with W available
            W_for_viz = None
            X_list_for_viz = None

            for fold in fold_results:
                if fold.get("success", False) and fold.get("W") is not None:
                    W_for_viz = fold.get("W")
                    # X_list would need to be passed to the plot method or stored
                    break

            if W_for_viz is not None:
                self.logger.debug("   Creating clinical factor loadings visualization...")
                from visualization.factor_plots import FactorVisualizer

                # Prepare data dict for visualizer
                # Try to get view_names and feature_names from config._shared_data first, then from results
                config_dict = self.config.to_dict() if hasattr(self.config, "to_dict") else self.config.__dict__
                shared_data = config_dict.get("_shared_data", {})

                # Get X_list, view_names, and feature_names
                X_list_for_viz = shared_data.get("X_list") if shared_data else results.get("X_list")
                view_names_for_viz = shared_data.get("view_names") if shared_data else results.get("view_names")
                feature_names_for_viz = shared_data.get("feature_names") if shared_data else results.get("feature_names", {})

                if X_list_for_viz is not None and view_names_for_viz is not None:
                    viz_data = {
                        "X_list": X_list_for_viz,
                        "view_names": view_names_for_viz,
                        "feature_names": feature_names_for_viz,
                    }

                    # Create visualizer
                    visualizer = FactorVisualizer(self.config)

                    # Create clinical factor loadings plot
                    fig_clinical = plt.figure(figsize=(18, 12))
                    visualizer.plot_clinical_factor_loadings(
                        W_for_viz, viz_data, save_path=None
                    )
                    plots["clinical_factor_loadings"] = plt.gcf()

                    self.logger.debug("   âœ… Clinical factor loadings plot created")
                    self.logger.info(f"   Views used: {view_names_for_viz}")
                else:
                    self.logger.info(f"   âš ï¸  X_list or view_names not available for clinical factor loadings plot")
                    self.logger.info(f"      X_list available: {X_list_for_viz is not None}, view_names available: {view_names_for_viz is not None}")
            else:
                self.logger.debug("   âš ï¸  No successful fold with W matrix found for clinical factor loadings plot")

        except Exception as e:
            self.logger.warning(f"Failed to create clinical factor loadings plot: {str(e)}")

        self.logger.info(f"ðŸ“Š Neuroimaging clinical validation plots completed: {len(plots)} plots generated")
        return plots

    @experiment_handler("subtype_classification_validation")
    @validate_data_types(
        X_list=list, clinical_labels=np.ndarray, hypers=dict, args=dict
    )
    @validate_parameters(
        X_list=lambda x: len(x) > 0,
        clinical_labels=lambda x: len(x) > 0,
        hypers=lambda x: isinstance(x, dict),
    )
    def run_subtype_classification_validation(
        self,
        X_list: List[np.ndarray],
        clinical_labels: np.ndarray,
        hypers: Dict,
        args: Dict,
        **kwargs,
    ) -> ExperimentResult:
        """Validate SGFA factors for PD subtype classification."""
        self.logger.info("Running subtype classification validation")

        results = {}

        # Run SGFA to get factor scores
        self.logger.info("Extracting SGFA factors")
        sgfa_result = self.data_processor.run_sgfa_analysis(X_list, hypers, args, **kwargs)

        if "error" in sgfa_result:
            raise ValueError(f"SGFA analysis failed: {sgfa_result['error']}")

        Z_sgfa = sgfa_result["Z"]  # Factor scores

        # Test different classification approaches
        classification_results = {}

        # 1. Direct factor-based classification
        self.logger.info("Testing direct factor-based classification")
        direct_results = self.classifier.test_factor_classification(
            Z_sgfa, clinical_labels, "sgfa_factors"
        )
        classification_results["sgfa_factors"] = direct_results

        # 2. Compare with raw data classification
        self.logger.info("Testing raw data classification")
        X_concat = np.hstack(X_list)
        raw_results = self.classifier.test_factor_classification(
            X_concat, clinical_labels, "raw_data"
        )
        classification_results["raw_data"] = raw_results

        # 3. Compare with PCA features
        self.logger.info("Testing PCA-based classification")
        from sklearn.decomposition import PCA

        pca = PCA(n_components=Z_sgfa.shape[1])
        Z_pca = pca.fit_transform(X_concat)
        pca_results = self.classifier.test_factor_classification(
            Z_pca, clinical_labels, "pca_features"
        )
        classification_results["pca_features"] = pca_results

        results["classification_comparison"] = classification_results

        # 4. Clinical interpretation analysis
        self.logger.info("Analyzing clinical interpretability")
        interpretation_results = self.metrics_calculator.analyze_clinical_interpretability(
            Z_sgfa, clinical_labels, sgfa_result
        )
        results["clinical_interpretation"] = interpretation_results

        # 5. Subtype stability analysis
        self.logger.info("Analyzing subtype stability")
        stability_results = self.subtype_analyzer.analyze_subtype_stability(
            X_list, clinical_labels, hypers, args, **kwargs
        )
        results["subtype_stability"] = stability_results

        # Analyze validation results
        analysis = self._analyze_subtype_classification(results)

        # Generate basic plots
        plots = self._plot_subtype_classification(results, clinical_labels)

        # Add comprehensive clinical visualizations (focus on subtypes + brain maps)
        advanced_plots = self._create_comprehensive_clinical_visualizations(
            X_list, results, clinical_labels, "subtype_classification"
        )
        plots.update(advanced_plots)

        return ExperimentResult(
            experiment_id="subtype_classification_validation",
            config=self.config,
            model_results=results,
            diagnostics=analysis,
            plots=plots,
            status="completed",
        )

    @experiment_handler("pd_subtype_discovery")
    @validate_data_types(
        X_list=list,
        clinical_data=dict,
        hypers=dict,
        args=dict,
    )
    @validate_parameters(
        X_list=lambda x: len(x) > 0,
        clinical_data=lambda x: x is not None,
        hypers=lambda x: isinstance(x, dict),
        args=lambda x: isinstance(x, dict),
    )
    def run_pd_subtype_discovery(
        self,
        X_list: List[np.ndarray],
        clinical_data: Dict,
        hypers: Dict,
        args: Dict,
        **kwargs,
    ) -> ExperimentResult:
        """Discover PD subtypes using SGFA factors from neuroimaging data.

        This function performs unsupervised PD subtype discovery by:
        1. Extracting SGFA factors from multi-modal neuroimaging data
        2. Applying clustering to discover potential PD subtypes
        3. Validating discovered subtypes against available clinical measures
        4. Analyzing stability and interpretability of discovered subtypes

        Parameters
        ----------
        X_list : List[np.ndarray]
            List of data matrices (one per imaging modality)
        clinical_data : Dict
            Dictionary containing clinical measures for validation
        hypers : Dict
            SGFA hyperparameters
        args : Dict
            Analysis arguments
        **kwargs
            Additional keyword arguments

        Returns
        -------
        ExperimentResult
            Results containing discovered subtypes, validation metrics, and plots
        """
        self.logger.info("Running PD subtype discovery")

        results = {}

        # Extract SGFA factors
        self.logger.info("Extracting SGFA factors for subtype discovery")
        sgfa_result = self.data_processor.run_sgfa_analysis(X_list, hypers, args, **kwargs)

        if "error" in sgfa_result:
            raise ValueError(f"SGFA analysis failed: {sgfa_result['error']}")

        Z_sgfa = sgfa_result["Z"]  # Factor scores [n_subjects x n_factors]
        n_subjects, n_factors = Z_sgfa.shape

        self.logger.info(f"Extracted {n_factors} factors for {n_subjects} subjects")

        # Discover PD subtypes using clustering
        self.logger.info("Discovering PD subtypes using clustering")
        subtype_results = self.subtype_analyzer.discover_pd_subtypes(Z_sgfa)
        results["subtype_discovery"] = subtype_results

        # Validate discovered subtypes against clinical measures
        if clinical_data:
            self.logger.info("Validating discovered subtypes against clinical measures")
            validation_results = self.subtype_analyzer.validate_subtypes_clinical(
                subtype_results, clinical_data, Z_sgfa
            )
            results["clinical_validation"] = validation_results

        # Analyze subtype stability across multiple runs
        self.logger.info("Analyzing subtype stability")
        stability_results = self.subtype_analyzer.analyze_pd_subtype_stability(
            X_list, hypers, args, n_runs=5, **kwargs
        )
        results["stability_analysis"] = stability_results

        # Factor interpretation for each discovered subtype
        self.logger.info("Analyzing factor patterns for each subtype")
        interpretation_results = self.subtype_analyzer.analyze_subtype_factor_patterns(
            Z_sgfa, subtype_results, sgfa_result
        )
        results["factor_interpretation"] = interpretation_results

        # Laterality pattern validation (PD-specific clinical validation)
        self.logger.info("Validating laterality patterns in factor loadings")
        try:
            W_sgfa = sgfa_result.get("W")
            if W_sgfa is not None:
                # Convert W to DataFrame with feature names if available
                feature_names = kwargs.get("feature_names", {})
                view_names = kwargs.get("view_names", [])

                # Get clinical feature names
                clinical_view_idx = None
                for idx, view_name in enumerate(view_names):
                    if "clinical" in view_name.lower():
                        clinical_view_idx = idx
                        break

                if clinical_view_idx is not None and clinical_view_idx in feature_names:
                    clinical_features = feature_names[clinical_view_idx]

                    # Extract clinical view loadings from W
                    # W shape: [total_features, n_factors]
                    # Need to find which rows correspond to clinical view
                    start_idx = sum(X_list[i].shape[1] for i in range(clinical_view_idx))
                    end_idx = start_idx + X_list[clinical_view_idx].shape[1]

                    W_clinical = W_sgfa[start_idx:end_idx, :]

                    # Create DataFrame
                    factor_loadings_df = pd.DataFrame(
                        W_clinical,
                        index=clinical_features,
                        columns=[f"Factor_{i+1}" for i in range(W_clinical.shape[1])]
                    )

                    # Determine ROI name from view_names
                    imaging_views = [v for v in view_names if "clinical" not in v.lower()]
                    roi_name = imaging_views[0] if len(imaging_views) > 0 else "Unknown"

                    # Validate laterality patterns
                    laterality_results = self.validate_laterality_patterns(
                        factor_loadings_df,
                        roi_name=roi_name
                    )
                    results["laterality_validation"] = laterality_results

                    # Save laterality validation results to CSV
                    self._save_laterality_validation_csv(laterality_results, roi_name)

                else:
                    self.logger.debug("   âš ï¸  No clinical features found for laterality validation")
            else:
                self.logger.debug("   âš ï¸  No factor loadings (W) available for laterality validation")
        except Exception as e:
            self.logger.warning(f"Laterality validation failed: {e}")
            results["laterality_validation"] = None

        # Comprehensive analysis
        analysis = self._analyze_pd_subtype_discovery(results)

        # Generate plots using PDSubtypeVisualizer
        visualizer = PDSubtypeVisualizer()
        # Use experiment-specific output directory if available
        if hasattr(self, 'base_output_dir') and self.base_output_dir:
            from pathlib import Path
            plot_dir = Path(self.base_output_dir) / "pd_subtype_plots"
        else:
            plot_dir = get_output_dir(self.config) / "pd_subtype_plots"
        plot_dir.mkdir(exist_ok=True, parents=True)
        plots = visualizer.create_pd_subtype_plots(results, Z_sgfa, clinical_data, plot_dir)

        return ExperimentResult(
            experiment_id="pd_subtype_discovery",
            config=self.config,
            model_results=results,
            diagnostics=analysis,
            plots=plots,
            status="completed",
        )

    def _save_laterality_validation_csv(self, laterality_results: Dict, roi_name: str) -> None:
        """
        Save laterality validation results to CSV format with detailed features and pattern descriptions.

        Args:
            laterality_results: Results from validate_laterality_patterns()
            roi_name: Name of the ROI being analyzed
        """
        try:
            from pathlib import Path
            import csv

            # Use experiment-specific output directory if available
            if hasattr(self, 'base_output_dir') and self.base_output_dir:
                output_dir = Path(self.base_output_dir) / "laterality_validation"
            else:
                output_dir = get_output_dir(self.config) / "laterality_validation"
            output_dir.mkdir(parents=True, exist_ok=True)

            # Create filename with ROI name
            safe_roi_name = roi_name.replace("/", "_").replace(" ", "_")
            csv_path = output_dir / f"laterality_validation_{safe_roi_name}.csv"

            # Prepare data for CSV
            csv_data = []

            # Add legend/header section first
            csv_data.append({
                'Type': 'LEGEND',
                'Factor': 'Pattern Descriptions',
                'Pattern_Name': 'Pattern 1: Bradykinesia + Contralateral Mirror Alignment',
                'Description': 'Left bradykinesia co-occurs with right mirror movements (or vice versa)',
                'Threshold': 'Strong tendency: score > 0.5, Moderate: score > 0.3',
            })
            csv_data.append({
                'Type': 'LEGEND',
                'Factor': '',
                'Pattern_Name': 'Pattern 2: Opposite Bradykinesia Tendencies',
                'Description': 'Left and right bradykinesia features tend in opposite directions',
                'Threshold': 'Strong tendency: score > 0.5, Moderate: score > 0.3',
            })
            csv_data.append({
                'Type': 'LEGEND',
                'Factor': '',
                'Pattern_Name': 'Pattern 3: Tremor Split',
                'Description': 'Left and right tremor features tend in opposite directions (optional)',
                'Threshold': 'Strong tendency: score > 0.5',
            })
            csv_data.append({
                'Type': 'LEGEND',
                'Factor': '',
                'Pattern_Name': 'Loading Threshold',
                'Description': 'Features with |loading| < 0.01 are excluded from analysis',
                'Threshold': 'Minimum loading: 0.01',
            })
            csv_data.append({'Type': '', 'Factor': '', 'Pattern_Name': '', 'Description': '', 'Threshold': ''})  # Blank row

            # Add summary row
            summary = laterality_results['summary']
            csv_data.append({
                'Type': 'SUMMARY',
                'Factor': 'All',
                'Validation_Score': f"{summary['average_validation_score']:.3f}",
                'Pattern1_Score': f"{summary['avg_pattern1_score']:.3f}",
                'Pattern2_Score': f"{summary['avg_pattern2_score']:.3f}",
                'Pattern3_Score': f"{summary['avg_pattern3_score']:.3f}",
                'Validation_Status': summary['validation_status'],
                'Total_Factors': summary['total_factors'],
                'Factors_Pattern1': summary['factors_with_pattern1_tendency'],
                'Factors_Pattern2': summary['factors_with_pattern2_tendency'],
                'Factors_Pattern3': summary['factors_with_pattern3_tendency'],
            })
            csv_data.append({'Type': '', 'Factor': '', 'Validation_Score': '', 'Pattern1_Score': '', 'Pattern2_Score': '', 'Pattern3_Score': ''})  # Blank row

            # Add detailed per-factor rows with features
            for factor_name, factor_data in laterality_results['factors'].items():
                pattern1 = factor_data['pattern1_left_brady_right_mirror']
                pattern2 = factor_data['pattern2_opposite_bradykinesia']
                pattern3 = factor_data['pattern3_tremor_split']

                # Get all categorized features
                all_features = factor_data.get('all_categorized', {})

                # Main factor row
                csv_data.append({
                    'Type': 'FACTOR',
                    'Factor': factor_name,
                    'Validation_Score': f"{factor_data['validation_score']:.3f}",
                    'Pattern1_Score': f"{pattern1.get('alignment_score', 0):.3f}",
                    'Pattern2_Score': f"{pattern2.get('opposition_score', 0):.3f}",
                    'Pattern3_Score': f"{pattern3.get('split_score', 0):.3f}",
                    'Pattern1_Dominant': pattern1.get('dominant_pattern', 'N/A'),
                    'Pattern2_Opposite': pattern2.get('opposite_tendency', False),
                    'Pattern3_Split': pattern3.get('split_tendency', False),
                })

                # Pattern 1 features
                left_brady_features = pattern1.get('left_brady_features', [])
                right_mirror_features = pattern1.get('right_mirror', [])
                if left_brady_features:
                    features_str = '; '.join([f"{feat[0]}={feat[1]:.3f}" for feat in left_brady_features])
                    csv_data.append({
                        'Type': 'FEATURES',
                        'Factor': factor_name,
                        'Pattern': 'Pattern1_Left_Bradykinesia',
                        'Features': features_str,
                        'Count': len(left_brady_features),
                    })
                if right_mirror_features:
                    features_str = '; '.join([f"{feat[0]}={feat[1]:.3f}" for feat in right_mirror_features])
                    csv_data.append({
                        'Type': 'FEATURES',
                        'Factor': factor_name,
                        'Pattern': 'Pattern1_Right_Mirror',
                        'Features': features_str,
                        'Count': len(right_mirror_features),
                    })

                # Pattern 2 features
                left_features = pattern2.get('left_features', [])
                right_features = pattern2.get('right_features', [])
                if left_features:
                    features_str = '; '.join([f"{feat[0]}={feat[1]:.3f}" for feat in left_features])
                    csv_data.append({
                        'Type': 'FEATURES',
                        'Factor': factor_name,
                        'Pattern': 'Pattern2_Left_Bradykinesia',
                        'Features': features_str,
                        'Count': len(left_features),
                        'Mean_Loading': f"{pattern2.get('left_mean', 0):.3f}",
                    })
                if right_features:
                    features_str = '; '.join([f"{feat[0]}={feat[1]:.3f}" for feat in right_features])
                    csv_data.append({
                        'Type': 'FEATURES',
                        'Factor': factor_name,
                        'Pattern': 'Pattern2_Right_Bradykinesia',
                        'Features': features_str,
                        'Count': len(right_features),
                        'Mean_Loading': f"{pattern2.get('right_mean', 0):.3f}",
                    })

                # Pattern 3 features
                left_tremor = pattern3.get('left_tremor', [])
                right_tremor = pattern3.get('right_tremor', [])
                if left_tremor:
                    features_str = '; '.join([f"{feat[0]}={feat[1]:.3f}" for feat in left_tremor])
                    csv_data.append({
                        'Type': 'FEATURES',
                        'Factor': factor_name,
                        'Pattern': 'Pattern3_Left_Tremor',
                        'Features': features_str,
                        'Count': len(left_tremor),
                        'Mean_Loading': f"{pattern3.get('left_mean', 0):.3f}",
                    })
                if right_tremor:
                    features_str = '; '.join([f"{feat[0]}={feat[1]:.3f}" for feat in right_tremor])
                    csv_data.append({
                        'Type': 'FEATURES',
                        'Factor': factor_name,
                        'Pattern': 'Pattern3_Right_Tremor',
                        'Features': features_str,
                        'Count': len(right_tremor),
                        'Mean_Loading': f"{pattern3.get('right_mean', 0):.3f}",
                    })

                # Add blank row between factors
                csv_data.append({})

            # Write to CSV (get all possible fieldnames from all rows)
            if csv_data:
                all_fieldnames = set()
                for row in csv_data:
                    all_fieldnames.update(row.keys())
                # Order fieldnames logically
                ordered_fieldnames = ['Type', 'Factor', 'Pattern', 'Features', 'Count', 'Mean_Loading',
                                     'Pattern_Name', 'Description', 'Threshold',
                                     'Validation_Score', 'Pattern1_Score', 'Pattern2_Score', 'Pattern3_Score',
                                     'Pattern1_Dominant', 'Pattern2_Opposite', 'Pattern3_Split',
                                     'Validation_Status', 'Total_Factors',
                                     'Factors_Pattern1', 'Factors_Pattern2', 'Factors_Pattern3']
                # Keep only fieldnames that exist in our data
                fieldnames = [f for f in ordered_fieldnames if f in all_fieldnames]
                # Add any remaining fieldnames not in our ordered list
                fieldnames.extend([f for f in all_fieldnames if f not in fieldnames])

                with open(csv_path, 'w', newline='') as f:
                    writer = csv.DictWriter(f, fieldnames=fieldnames)
                    writer.writeheader()
                    writer.writerows(csv_data)

                self.logger.info(f"   âœ… Laterality validation saved to: {csv_path}")
            else:
                self.logger.warning("   âš ï¸  No laterality validation data to save")

        except Exception as e:
            self.logger.warning(f"Failed to save laterality validation CSV: {e}")

    @experiment_handler("disease_progression_validation")
    @validate_data_types(
        X_list=list,
        progression_scores=np.ndarray,
        time_points=np.ndarray,
        subject_ids=np.ndarray,
        hypers=dict,
        args=dict,
    )
    @validate_parameters(
        X_list=lambda x: len(x) > 0,
        progression_scores=lambda x: len(x) > 0,
        time_points=lambda x: len(x) > 0,
        subject_ids=lambda x: len(x) > 0,
        hypers=lambda x: isinstance(x, dict),
    )
    def run_disease_progression_validation(
        self,
        X_list: List[np.ndarray],
        progression_scores: np.ndarray,
        time_points: np.ndarray,
        subject_ids: np.ndarray,
        hypers: Dict,
        args: Dict,
        **kwargs,
    ) -> ExperimentResult:
        """Validate SGFA factors for disease progression prediction."""
        self.logger.info("Running disease progression validation")

        results = {}

        # Run SGFA to get factor scores
        self.logger.info("Extracting SGFA factors for progression analysis")
        sgfa_result = self.data_processor.run_sgfa_analysis(X_list, hypers, args, **kwargs)

        if "error" in sgfa_result:
            raise ValueError(f"SGFA analysis failed: {sgfa_result['error']}")

        Z_sgfa = sgfa_result["Z"]

        # 1. Cross-sectional correlation analysis
        self.logger.info("Analyzing cross-sectional correlations")
        cross_sectional_results = self.progression_analyzer.analyze_cross_sectional_correlations(
            Z_sgfa, progression_scores
        )
        results["cross_sectional_correlations"] = cross_sectional_results

        # 2. Longitudinal progression modeling
        if len(np.unique(subject_ids)) < len(
            subject_ids
        ):  # Longitudinal data available
            self.logger.info("Analyzing longitudinal progression")
            longitudinal_results = self.progression_analyzer.analyze_longitudinal_progression(
                Z_sgfa, progression_scores, time_points, subject_ids
            )
            results["longitudinal_analysis"] = longitudinal_results
        else:
            self.logger.info(
                "No longitudinal data available, skipping longitudinal analysis"
            )
            results["longitudinal_analysis"] = {"status": "no_longitudinal_data"}

        # 3. Progression prediction validation
        self.logger.info("Validating progression prediction")
        prediction_results = self.progression_analyzer.validate_progression_prediction(
            Z_sgfa, progression_scores, time_points
        )
        results["progression_prediction"] = prediction_results

        # 4. Clinical milestone prediction
        self.logger.info("Analyzing clinical milestone prediction")
        milestone_results = self.progression_analyzer.analyze_clinical_milestones(
            Z_sgfa, progression_scores, time_points
        )
        results["clinical_milestones"] = milestone_results

        # Analyze progression validation results
        analysis = self._analyze_disease_progression_validation(results)

        # Generate basic plots
        plots = self._plot_disease_progression_validation(results, progression_scores)

        # Add comprehensive clinical visualizations (focus on progression + brain maps)
        advanced_plots = self._create_comprehensive_clinical_visualizations(
            X_list, results, progression_scores, "disease_progression"
        )
        plots.update(advanced_plots)

        return ExperimentResult(
            experiment_id="disease_progression_validation",
            config=self.config,
            model_results=results,
            diagnostics=analysis,
            plots=plots,
            status="completed",
        )

    @experiment_handler("biomarker_discovery_validation")
    @validate_data_types(X_list=list, clinical_outcomes=dict, hypers=dict, args=dict)
    @validate_parameters(
        X_list=lambda x: len(x) > 0,
        clinical_outcomes=lambda x: isinstance(x, dict) and len(x) > 0,
        hypers=lambda x: isinstance(x, dict),
    )
    def run_biomarker_discovery_validation(
        self,
        X_list: List[np.ndarray],
        clinical_outcomes: Dict[str, np.ndarray],
        hypers: Dict,
        args: Dict,
        **kwargs,
    ) -> ExperimentResult:
        """Validate SGFA factors as potential biomarkers."""
        self.logger.info("Running biomarker discovery validation")

        results = {}

        # Run SGFA to get factors and loadings
        self.logger.info("Extracting SGFA factors and loadings")
        sgfa_result = self.data_processor.run_sgfa_analysis(X_list, hypers, args, **kwargs)

        if "error" in sgfa_result:
            raise ValueError(f"SGFA analysis failed: {sgfa_result['error']}")

        Z_sgfa = sgfa_result["Z"]
        W_sgfa = sgfa_result["W"]

        # 1. Factor-outcome associations
        self.logger.info("Analyzing factor-outcome associations")
        association_results = self.biomarker_analyzer.analyze_factor_outcome_associations(
            Z_sgfa, clinical_outcomes
        )
        results["factor_associations"] = association_results

        # 2. Feature importance analysis
        self.logger.info("Analyzing feature importance")
        importance_results = self.biomarker_analyzer.analyze_feature_importance(
            W_sgfa, X_list, clinical_outcomes
        )
        results["feature_importance"] = importance_results

        # 3. Biomarker panel validation
        self.logger.info("Validating biomarker panels")
        panel_results = self.biomarker_analyzer.validate_biomarker_panels(Z_sgfa, clinical_outcomes)
        results["biomarker_panels"] = panel_results

        # 4. Cross-validation robustness
        self.logger.info("Testing biomarker robustness")
        robustness_results = self.biomarker_analyzer.test_biomarker_robustness(
            X_list, clinical_outcomes, hypers, args, **kwargs
        )
        results["robustness_analysis"] = robustness_results

        # Analyze biomarker validation results
        analysis = self._analyze_biomarker_discovery(results)

        # Generate plots
        plots = self._plot_biomarker_discovery(results, clinical_outcomes)

        return ExperimentResult(
            experiment_id="biomarker_discovery_validation",
            config=self.config,
            model_results=results,
            diagnostics=analysis,
            plots=plots,
            status="completed",
        )

    @experiment_handler("external_cohort_validation")
    @validate_data_types(
        X_train_list=list,
        X_test_list=list,
        clinical_labels_train=np.ndarray,
        clinical_labels_test=np.ndarray,
        hypers=dict,
        args=dict,
    )
    @validate_parameters(
        X_train_list=lambda x: len(x) > 0,
        X_test_list=lambda x: len(x) > 0,
        clinical_labels_train=lambda x: len(x) > 0,
        clinical_labels_test=lambda x: len(x) > 0,
        hypers=lambda x: isinstance(x, dict),
    )
    def run_external_cohort_validation(
        self,
        X_train_list: List[np.ndarray],
        X_test_list: List[np.ndarray],
        clinical_labels_train: np.ndarray,
        clinical_labels_test: np.ndarray,
        hypers: Dict,
        args: Dict,
        **kwargs,
    ) -> ExperimentResult:
        """Validate SGFA model on external cohort."""
        self.logger.info("Running external cohort validation")

        results = {}

        # 1. Train model on training cohort
        self.logger.info("Training SGFA model on training cohort")
        # Use model_comparison's SGFA runner
        from experiments.model_comparison import ModelArchitectureComparison
        model_exp = ModelArchitectureComparison(self.config, self.logger)
        train_result = model_exp._run_sgfa_analysis(X_train_list, hypers, args, **kwargs)

        if "error" in train_result:
            raise ValueError(f"Training failed: {train_result['error']}")

        results["training_result"] = train_result

        # 2. Apply trained model to test cohort
        self.logger.info("Applying model to test cohort")
        test_result = self._apply_trained_model(
            X_test_list, train_result, hypers, args, **kwargs
        )
        results["test_application"] = test_result

        # 3. Compare factor distributions
        self.logger.info("Comparing factor distributions")
        distribution_comparison = self.external_validator.compare_factor_distributions(
            train_result["Z"], test_result["Z"]
        )
        results["distribution_comparison"] = distribution_comparison

        # 4. Cross-cohort classification
        self.logger.info("Testing cross-cohort classification")
        classification_results = self.classifier.test_cross_cohort_classification(
            train_result["Z"],
            test_result["Z"],
            clinical_labels_train,
            clinical_labels_test,
        )
        results["cross_cohort_classification"] = classification_results

        # 5. Model transferability analysis
        self.logger.info("Analyzing model transferability")
        transferability_results = self.external_validator.analyze_model_transferability(
            train_result, test_result, clinical_labels_train, clinical_labels_test
        )
        results["transferability_analysis"] = transferability_results

        # Analyze external validation results
        analysis = self._analyze_external_cohort_validation(results)

        # Generate plots
        plots = self._plot_external_cohort_validation(
            results, clinical_labels_train, clinical_labels_test
        )

        return ExperimentResult(
            experiment_id="external_cohort_validation",
            config=self.config,
            model_results=results,
            diagnostics=analysis,
            plots=plots,
            status="completed",
        )

    def _analyze_subtype_classification(self, results: Dict) -> Dict:
        """Analyze subtype classification validation results.

        WARNING: This function is for supervised classification with real labels.
        If using random/mock labels, results will be meaningless.
        """
        analysis = {
            "classification_performance": {},
            "sgfa_advantage": {},
            "clinical_utility": {},
        }

        # Extract classification results
        classification_results = results.get("classification_comparison", {})

        # Performance summary
        for method_name, method_results in classification_results.items():
            if "error" not in method_results:
                best_model = None
                best_accuracy = 0

                for model_name, model_results in method_results.items():
                    if "error" not in model_results:
                        cv_results = model_results.get("cross_validation", {})
                        accuracy = cv_results.get("accuracy", {}).get("mean", 0)

                        if accuracy > best_accuracy:
                            best_accuracy = accuracy
                            best_model = model_name

                analysis["classification_performance"][method_name] = {
                    "best_model": best_model,
                    "best_accuracy": best_accuracy,
                    "performance_tier": (
                        "high"
                        if best_accuracy > 0.8
                        else "medium" if best_accuracy > 0.65 else "low"
                    ),
                }

        # SGFA advantage analysis
        sgfa_performance = analysis["classification_performance"].get(
            "sgfa_factors", {}
        )
        raw_performance = analysis["classification_performance"].get("raw_data", {})
        pca_performance = analysis["classification_performance"].get("pca_features", {})

        if sgfa_performance and raw_performance:
            sgfa_accuracy = sgfa_performance.get("best_accuracy", 0)
            raw_accuracy = raw_performance.get("best_accuracy", 0)
            pca_accuracy = pca_performance.get("best_accuracy", 0)

            analysis["sgfa_advantage"] = {
                "vs_raw_data": sgfa_accuracy - raw_accuracy,
                "vs_pca": sgfa_accuracy - pca_accuracy,
                "relative_improvement_vs_raw": (
                    ((sgfa_accuracy / raw_accuracy) - 1) * 100
                    if raw_accuracy > 0
                    else np.nan
                ),
                "clinical_significance": (
                    "significant" if sgfa_accuracy - raw_accuracy > 0.05 else "marginal"
                ),
            }

        # Clinical utility assessment
        interpretation_results = results.get("clinical_interpretation", {})
        if interpretation_results:
            factor_associations = interpretation_results.get(
                "factor_label_associations", []
            )
            significant_factors = [
                f for f in factor_associations if f.get("p_value", 1) < 0.05
            ]

            analysis["clinical_utility"] = {
                "n_significant_factors": len(significant_factors),
                "clinical_relevance_score": (
                    len(significant_factors) / len(factor_associations)
                    if factor_associations
                    else 0
                ),
                "interpretability_assessment": (
                    "high"
                    if len(significant_factors) >= 2
                    else "moderate" if len(significant_factors) >= 1 else "low"
                ),
            }

        return analysis

    def _analyze_disease_progression_validation(self, results: Dict) -> Dict:
        """Analyze disease progression validation results."""
        analysis = {
            "predictive_performance": {},
            "clinical_correlations": {},
            "longitudinal_insights": {},
        }

        # Predictive performance
        prediction_results = results.get("progression_prediction", {})
        if prediction_results:
            best_model = None
            best_r2 = -np.inf

            for model_name, metrics in prediction_results.items():
                if "error" not in metrics:
                    r2 = metrics.get("r2_score", -np.inf)
                    if r2 > best_r2:
                        best_r2 = r2
                        best_model = model_name

            analysis["predictive_performance"] = {
                "best_model": best_model,
                "best_r2_score": best_r2,
                "prediction_quality": (
                    "excellent"
                    if best_r2 > 0.7
                    else (
                        "good"
                        if best_r2 > 0.5
                        else "moderate" if best_r2 > 0.3 else "poor"
                    )
                ),
            }

        # Clinical correlations
        correlation_results = results.get("cross_sectional_correlations", {})
        if correlation_results:
            factor_correlations = correlation_results.get(
                "factor_progression_correlations", []
            )
            strong_correlations = [
                f
                for f in factor_correlations
                if abs(f.get("pearson_correlation", 0)) > 0.5
            ]

            analysis["clinical_correlations"] = {
                "n_strong_correlations": len(strong_correlations),
                "max_correlation": (
                    max(
                        [
                            abs(f.get("pearson_correlation", 0))
                            for f in factor_correlations
                        ]
                    )
                    if factor_correlations
                    else 0
                ),
                "correlation_significance": len(
                    [
                        f
                        for f in factor_correlations
                        if f.get("pearson_p_value", 1) < 0.05
                    ]
                ),
            }

        # Longitudinal insights
        longitudinal_results = results.get("longitudinal_analysis", {})
        if longitudinal_results and "error" not in longitudinal_results:
            n_longitudinal = longitudinal_results.get("n_longitudinal_subjects", 0)

            analysis["longitudinal_insights"] = {
                "longitudinal_data_available": n_longitudinal > 0,
                "n_longitudinal_subjects": n_longitudinal,
                "longitudinal_utility": (
                    "high"
                    if n_longitudinal > 50
                    else "moderate" if n_longitudinal > 20 else "low"
                ),
            }

        return analysis

    def _analyze_biomarker_discovery(self, results: Dict) -> Dict:
        """Analyze biomarker discovery validation results."""
        analysis = {
            "biomarker_potential": {},
            "discovery_robustness": {},
            "clinical_relevance": {},
        }

        # Biomarker potential
        association_results = results.get("factor_associations", {})
        if association_results:
            total_associations = 0
            significant_associations = 0

            for outcome_name, outcome_associations in association_results.items():
                for factor_assoc in outcome_associations:
                    total_associations += 1
                    p_value = factor_assoc.get("p_value") or factor_assoc.get(
                        "pearson_p_value", 1
                    )
                    if p_value < 0.05:
                        significant_associations += 1

            analysis["biomarker_potential"] = {
                "total_factor_outcome_tests": total_associations,
                "significant_associations": significant_associations,
                "discovery_rate": (
                    significant_associations / total_associations
                    if total_associations > 0
                    else 0
                ),
                "biomarker_promise": (
                    "high"
                    if significant_associations > 5
                    else "moderate" if significant_associations > 2 else "low"
                ),
            }

        # Discovery robustness
        robustness_results = results.get("robustness_analysis", {})
        if robustness_results:
            robust_discoveries = 0
            total_tested = 0

            for outcome_name, outcome_robustness in robustness_results.items():
                if "error" not in outcome_robustness:
                    factor_stability = outcome_robustness.get("factor_stability", [])
                    for factor_result in factor_stability:
                        total_tested += 1
                        sig_rate = factor_result.get("significance_rate", 0)
                        if sig_rate > 0.7:  # Significant in >70% of bootstrap samples
                            robust_discoveries += 1

            analysis["discovery_robustness"] = {
                "robust_discoveries": robust_discoveries,
                "total_tested": total_tested,
                "robustness_rate": (
                    robust_discoveries / total_tested if total_tested > 0 else 0
                ),
                "robustness_level": (
                    "high"
                    if robust_discoveries > 3
                    else "moderate" if robust_discoveries > 1 else "low"
                ),
            }

        # Clinical relevance
        panel_results = results.get("biomarker_panels", {})
        if panel_results:
            best_panel_performance = 0

            for outcome_name, panel_data in panel_results.items():
                for panel_size, panel_metrics in panel_data.items():
                    if "error" not in panel_metrics:
                        performance = panel_metrics.get(
                            "cv_accuracy_mean"
                        ) or panel_metrics.get("cv_r2_mean", 0)
                        best_panel_performance = max(
                            best_panel_performance, performance
                        )

            analysis["clinical_relevance"] = {
                "best_panel_performance": best_panel_performance,
                "clinical_utility": (
                    "high"
                    if best_panel_performance > 0.75
                    else "moderate" if best_panel_performance > 0.65 else "limited"
                ),
            }

        return analysis

    def _analyze_external_cohort_validation(self, results: Dict) -> Dict:
        """Analyze external cohort validation results."""
        analysis = {
            "generalizability": {},
            "domain_transfer": {},
            "model_robustness": {},
        }

        # Generalizability assessment
        classification_results = results.get("cross_cohort_classification", {})
        if classification_results:
            performance_drops = []
            cross_cohort_accuracies = []

            for model_name, model_results in classification_results.items():
                if "error" not in model_results:
                    performance_drop = model_results.get("performance_drop", np.nan)
                    cross_cohort_acc = model_results.get(
                        "cross_cohort_performance", {}
                    ).get("accuracy", np.nan)

                    if not np.isnan(performance_drop):
                        performance_drops.append(performance_drop)
                    if not np.isnan(cross_cohort_acc):
                        cross_cohort_accuracies.append(cross_cohort_acc)

            if performance_drops:
                mean_performance_drop = np.mean(performance_drops)
                analysis["generalizability"] = {
                    "mean_performance_drop": mean_performance_drop,
                    "mean_cross_cohort_accuracy": np.mean(cross_cohort_accuracies),
                    "generalizability_level": (
                        "excellent"
                        if mean_performance_drop < 0.05
                        else (
                            "good"
                            if mean_performance_drop < 0.1
                            else "moderate" if mean_performance_drop < 0.2 else "poor"
                        )
                    ),
                }

        # Domain transfer analysis
        transferability_results = results.get("transferability_analysis", {})
        if transferability_results:
            factor_similarity = transferability_results.get(
                "factor_space_similarity", {}
            )
            transferability_score = factor_similarity.get("transferability_score", 0)

            analysis["domain_transfer"] = {
                "transferability_score": transferability_score,
                "transfer_quality": (
                    "excellent"
                    if transferability_score > 0.8
                    else (
                        "good"
                        if transferability_score > 0.6
                        else "moderate" if transferability_score > 0.4 else "poor"
                    )
                ),
            }

        # Model robustness
        distribution_comparison = results.get("distribution_comparison", {})
        if distribution_comparison:
            similarity_score = distribution_comparison.get(
                "overall_similarity", {}
            ).get("similarity_score", 0)

            analysis["model_robustness"] = {
                "distribution_similarity": similarity_score,
                "robustness_level": (
                    "high"
                    if similarity_score > 0.8
                    else "moderate" if similarity_score > 0.6 else "low"
                ),
            }

        return analysis

    def _plot_subtype_classification(
        self, results: Dict, clinical_labels: np.ndarray
    ) -> Dict:
        """Generate plots for subtype classification validation.

        WARNING: This function requires real clinical subtype labels.
        Using random/mock labels will produce meaningless accuracy metrics.
        Consider using pd_subtype_discovery with clustering quality metrics instead.
        """
        plots = {}

        try:
            classification_results = results.get("classification_comparison", {})

            if not classification_results:
                return plots

            # Create comprehensive classification plots
            fig, axes = plt.subplots(2, 2, figsize=(15, 12))
            fig.suptitle("Subtype Classification Validation", fontsize=16)

            # Plot 1: Classification accuracy comparison
            methods = list(classification_results.keys())
            best_accuracies = []

            for method in methods:
                method_results = classification_results[method]
                if "error" not in method_results:
                    best_acc = 0
                    for model_name, model_results in method_results.items():
                        if "error" not in model_results:
                            cv_results = model_results.get("cross_validation", {})
                            accuracy = cv_results.get("accuracy", {}).get("mean", 0)
                            best_acc = max(best_acc, accuracy)
                    best_accuracies.append(best_acc)
                else:
                    best_accuracies.append(0)

            colors = [
                "red" if method == "sgfa_factors" else "skyblue" for method in methods
            ]
            axes[0, 0].bar(methods, best_accuracies, color=colors)
            axes[0, 0].set_ylabel("Best Classification Accuracy")
            axes[0, 0].set_title("Classification Performance by Method")
            axes[0, 0].tick_params(axis="x", rotation=45)
            axes[0, 0].grid(True, alpha=0.3)

            # Plot 2: Factor-label associations
            interpretation_results = results.get("clinical_interpretation", {})
            if interpretation_results:
                factor_associations = interpretation_results.get(
                    "factor_label_associations", []
                )

                factors = [f["factor"] for f in factor_associations]
                p_values = [f.get("p_value", 1) for f in factor_associations]
                effect_sizes = [f.get("effect_size", 0) for f in factor_associations]

                scatter = axes[0, 1].scatter(
                    effect_sizes,
                    [-np.log10(p) for p in p_values],
                    c=factors,
                    cmap="viridis",
                    s=100,
                )
                axes[0, 1].axhline(
                    -np.log10(0.05), color="red", linestyle="--", label="p=0.05"
                )
                axes[0, 1].set_xlabel("Effect Size")
                axes[0, 1].set_ylabel("-log10(p-value)")
                axes[0, 1].set_title("Factor-Clinical Label Associations")
                axes[0, 1].legend()
                axes[0, 1].grid(True, alpha=0.3)

                plt.colorbar(scatter, ax=axes[0, 1], label="Factor")

            # Plot 3: Class distribution
            unique_labels, label_counts = np.unique(clinical_labels, return_counts=True)
            axes[1, 0].pie(
                label_counts,
                labels=[f"Class {label}" for label in unique_labels],
                autopct="%1.1f%%",
            )
            axes[1, 0].set_title("Clinical Label Distribution")

            # Plot 4: Model comparison detailed
            if len(methods) > 1:
                sgfa_results = classification_results.get("sgfa_factors", {})
                comparison_method = next(
                    (m for m in methods if m != "sgfa_factors"), None
                )

                if sgfa_results and comparison_method:
                    comparison_results = classification_results[comparison_method]

                    metrics = [
                        "accuracy",
                        "precision_macro",
                        "recall_macro",
                        "f1_macro",
                    ]
                    sgfa_scores = []
                    comparison_scores = []

                    # Get best model performance for each metric
                    for metric in metrics:
                        sgfa_best = 0
                        comp_best = 0

                        for model_results in sgfa_results.values():
                            if "error" not in model_results:
                                cv_results = model_results.get("cross_validation", {})
                                score = cv_results.get(metric, {}).get("mean", 0)
                                sgfa_best = max(sgfa_best, score)

                        for model_results in comparison_results.values():
                            if "error" not in model_results:
                                cv_results = model_results.get("cross_validation", {})
                                score = cv_results.get(metric, {}).get("mean", 0)
                                comp_best = max(comp_best, score)

                        sgfa_scores.append(sgfa_best)
                        comparison_scores.append(comp_best)

                    x = np.arange(len(metrics))
                    width = 0.35

                    axes[1, 1].bar(
                        x - width / 2,
                        sgfa_scores,
                        width,
                        label="SGFA",
                        color="red",
                        alpha=0.8,
                    )
                    axes[1, 1].bar(
                        x + width / 2,
                        comparison_scores,
                        width,
                        label=comparison_method,
                        color="blue",
                        alpha=0.8,
                    )

                    axes[1, 1].set_xlabel("Metrics")
                    axes[1, 1].set_ylabel("Score")
                    axes[1, 1].set_title("SGFA vs Comparison Method")
                    axes[1, 1].set_xticks(x)
                    axes[1, 1].set_xticklabels(metrics)
                    axes[1, 1].legend()
                    axes[1, 1].grid(True, alpha=0.3)

            plt.tight_layout()
            plots["subtype_classification_validation"] = fig

        except Exception as e:
            self.logger.warning(
                f"Failed to create subtype classification plots: {str(e)}"
            )

        return plots

    def _plot_disease_progression_validation(
        self, results: Dict, progression_scores: np.ndarray
    ) -> Dict:
        """Generate plots for disease progression validation."""
        plots = {}

        try:
            fig, axes = plt.subplots(2, 2, figsize=(15, 12))
            fig.suptitle("Disease Progression Validation", fontsize=16)

            # Plot 1: Progression prediction performance
            prediction_results = results.get("progression_prediction", {})
            if prediction_results:
                models = []
                r2_scores = []

                for model_name, metrics in prediction_results.items():
                    if "error" not in metrics:
                        models.append(model_name)
                        r2_scores.append(metrics.get("r2_score", 0))

                if models:
                    axes[0, 0].bar(models, r2_scores)
                    axes[0, 0].set_ylabel("RÂ² Score")
                    axes[0, 0].set_title("Progression Prediction Performance")
                    axes[0, 0].tick_params(axis="x", rotation=45)
                    axes[0, 0].grid(True, alpha=0.3)

            # Plot 2: Factor-progression correlations
            correlation_results = results.get("cross_sectional_correlations", {})
            if correlation_results:
                factor_correlations = correlation_results.get(
                    "factor_progression_correlations", []
                )

                if factor_correlations:
                    factors = [f["factor"] for f in factor_correlations]
                    correlations = [
                        f.get("pearson_correlation", 0) for f in factor_correlations
                    ]
                    p_values = [
                        f.get("pearson_p_value", 1) for f in factor_correlations
                    ]

                    colors = ["red" if p < 0.05 else "gray" for p in p_values]
                    bars = axes[0, 1].bar(factors, correlations, color=colors)
                    axes[0, 1].set_xlabel("Factor")
                    axes[0, 1].set_ylabel("Correlation with Progression")
                    axes[0, 1].set_title("Factor-Progression Correlations")
                    axes[0, 1].axhline(0, color="black", linestyle="-", alpha=0.5)
                    axes[0, 1].grid(True, alpha=0.3)

                    # Add significance indicators
                    from matplotlib.patches import Patch

                    legend_elements = [
                        Patch(facecolor="red", label="p < 0.05"),
                        Patch(facecolor="gray", label="p â‰¥ 0.05"),
                    ]
                    axes[0, 1].legend(handles=legend_elements)

            # Plot 3: Progression score distribution
            axes[1, 0].hist(
                progression_scores,
                bins=20,
                alpha=0.7,
                color="skyblue",
                edgecolor="black",
            )
            axes[1, 0].set_xlabel("Progression Score")
            axes[1, 0].set_ylabel("Frequency")
            axes[1, 0].set_title("Progression Score Distribution")
            axes[1, 0].grid(True, alpha=0.3)

            # Plot 4: Longitudinal analysis (if available)
            longitudinal_results = results.get("longitudinal_analysis", {})
            if longitudinal_results and "error" not in longitudinal_results:
                factor_change_analysis = longitudinal_results.get(
                    "factor_change_analysis", {}
                )

                if factor_change_analysis:
                    factor_correlations = factor_change_analysis.get(
                        "factor_progression_correlations", []
                    )

                    if factor_correlations:
                        factors = [f["factor"] for f in factor_correlations]
                        change_correlations = [
                            f.get("correlation", 0) for f in factor_correlations
                        ]

                        axes[1, 1].bar(
                            factors, change_correlations, alpha=0.8, color="green"
                        )
                        axes[1, 1].set_xlabel("Factor")
                        axes[1, 1].set_ylabel("Change-Progression Correlation")
                        axes[1, 1].set_title("Factor Change vs Progression Rate")
                        axes[1, 1].axhline(0, color="black", linestyle="-", alpha=0.5)
                        axes[1, 1].grid(True, alpha=0.3)
            else:
                axes[1, 1].text(
                    0.5,
                    0.5,
                    "No Longitudinal Data Available",
                    ha="center",
                    va="center",
                    transform=axes[1, 1].transAxes,
                )
                axes[1, 1].set_title("Longitudinal Analysis")

            plt.tight_layout()
            plots["disease_progression_validation"] = fig

        except Exception as e:
            self.logger.warning(f"Failed to create disease progression plots: {str(e)}")

        return plots

    def _plot_biomarker_discovery(
        self, results: Dict, clinical_outcomes: Dict[str, np.ndarray]
    ) -> Dict:
        """Generate plots for biomarker discovery validation."""
        plots = {}

        try:
            fig, axes = plt.subplots(2, 2, figsize=(15, 12))
            fig.suptitle("Biomarker Discovery Validation", fontsize=16)

            # Plot 1: Association significance heatmap
            association_results = results.get("factor_associations", {})
            if association_results:
                outcome_names = list(association_results.keys())
                n_factors = (
                    len(association_results[outcome_names[0]]) if outcome_names else 0
                )

                if outcome_names and n_factors > 0:
                    # Create p-value matrix
                    p_value_matrix = np.ones((len(outcome_names), n_factors))

                    for i, outcome_name in enumerate(outcome_names):
                        outcome_associations = association_results[outcome_name]
                        for j, factor_assoc in enumerate(outcome_associations):
                            p_value = factor_assoc.get("p_value") or factor_assoc.get(
                                "pearson_p_value", 1
                            )
                            p_value_matrix[i, j] = -np.log10(p_value)

                    im = axes[0, 0].imshow(p_value_matrix, cmap="Reds", aspect="auto")
                    axes[0, 0].set_xticks(range(n_factors))
                    axes[0, 0].set_yticks(range(len(outcome_names)))
                    axes[0, 0].set_xticklabels(
                        [f"Factor {i}" for i in range(n_factors)]
                    )
                    axes[0, 0].set_yticklabels(outcome_names)
                    axes[0, 0].set_title("Association Significance (-log10 p-value)")
                    plt.colorbar(im, ax=axes[0, 0])

                    # Add significance threshold line
                    threshold = -np.log10(0.05)
                    axes[0, 0].contour(
                        p_value_matrix,
                        levels=[threshold],
                        colors="blue",
                        linestyles="--",
                    )

            # Plot 2: Feature importance
            importance_results = results.get("feature_importance", {})
            if importance_results:
                feature_rankings = importance_results.get("feature_rankings", {})
                top_features = feature_rankings.get("top_features", [])

                if top_features:
                    # Show top 10 features
                    top_10 = top_features[:10]
                    importance_scores = [f["importance_score"] for f in top_10]
                    feature_labels = [
                        f"V{f['view']}F{f['feature_index']}" for f in top_10
                    ]

                    axes[0, 1].barh(range(len(top_10)), importance_scores)
                    axes[0, 1].set_yticks(range(len(top_10)))
                    axes[0, 1].set_yticklabels(feature_labels)
                    axes[0, 1].set_xlabel("Importance Score")
                    axes[0, 1].set_title("Top 10 Most Important Features")
                    axes[0, 1].grid(True, alpha=0.3)

            # Plot 3: Biomarker panel performance
            panel_results = results.get("biomarker_panels", {})
            if panel_results:
                panel_performance_data = []

                for outcome_name, panel_data in panel_results.items():
                    for panel_size, panel_metrics in panel_data.items():
                        if "error" not in panel_metrics:
                            performance = panel_metrics.get(
                                "cv_accuracy_mean"
                            ) or panel_metrics.get("cv_r2_mean", 0)
                            size = int(panel_size.split("_")[-1])
                            panel_performance_data.append(
                                {
                                    "outcome": outcome_name,
                                    "panel_size": size,
                                    "performance": performance,
                                }
                            )

                if panel_performance_data:
                    # Group by panel size
                    panel_sizes = sorted(
                        list(set([d["panel_size"] for d in panel_performance_data]))
                    )
                    avg_performance = []

                    for size in panel_sizes:
                        size_performances = [
                            d["performance"]
                            for d in panel_performance_data
                            if d["panel_size"] == size
                        ]
                        avg_performance.append(np.mean(size_performances))

                    axes[1, 0].plot(
                        panel_sizes, avg_performance, "o-", linewidth=2, markersize=8
                    )
                    axes[1, 0].set_xlabel("Panel Size (Number of Factors)")
                    axes[1, 0].set_ylabel("Average Performance")
                    axes[1, 0].set_title("Biomarker Panel Performance vs Size")
                    axes[1, 0].grid(True, alpha=0.3)

            # Plot 4: Robustness analysis
            robustness_results = results.get("robustness_analysis", {})
            if robustness_results:
                outcome_names = []
                significance_rates = []

                for outcome_name, outcome_robustness in robustness_results.items():
                    if "error" not in outcome_robustness:
                        factor_stability = outcome_robustness.get(
                            "factor_stability", []
                        )

                        # Average significance rate across factors
                        sig_rates = [
                            f.get("significance_rate", 0)
                            for f in factor_stability
                            if "significance_rate" in f
                        ]
                        if sig_rates:
                            avg_sig_rate = np.mean(sig_rates)
                            outcome_names.append(outcome_name)
                            significance_rates.append(avg_sig_rate)

                if outcome_names:
                    axes[1, 1].bar(outcome_names, significance_rates, alpha=0.7)
                    axes[1, 1].set_ylabel("Average Significance Rate")
                    axes[1, 1].set_title("Biomarker Discovery Robustness")
                    axes[1, 1].tick_params(axis="x", rotation=45)
                    axes[1, 1].axhline(
                        0.7, color="red", linestyle="--", label="Robustness Threshold"
                    )
                    axes[1, 1].legend()
                    axes[1, 1].grid(True, alpha=0.3)

            plt.tight_layout()
            plots["biomarker_discovery_validation"] = fig

        except Exception as e:
            self.logger.warning(f"Failed to create biomarker discovery plots: {str(e)}")

        return plots

    def _plot_external_cohort_validation(
        self,
        results: Dict,
        clinical_labels_train: np.ndarray,
        clinical_labels_test: np.ndarray,
    ) -> Dict:
        """Generate plots for external cohort validation."""
        plots = {}

        try:
            fig, axes = plt.subplots(2, 2, figsize=(15, 12))
            fig.suptitle("External Cohort Validation", fontsize=16)

            # Plot 1: Cross-cohort vs within-cohort performance
            classification_results = results.get("cross_cohort_classification", {})
            if classification_results:
                models = []
                cross_cohort_acc = []
                within_cohort_acc = []

                for model_name, model_results in classification_results.items():
                    if "error" not in model_results:
                        models.append(model_name)
                        cross_cohort_acc.append(
                            model_results.get("cross_cohort_performance", {}).get(
                                "accuracy", 0
                            )
                        )
                        within_cohort_acc.append(
                            model_results.get("within_cohort_performance", {}).get(
                                "accuracy", 0
                            )
                        )

                if models:
                    x = np.arange(len(models))
                    width = 0.35

                    axes[0, 0].bar(
                        x - width / 2,
                        cross_cohort_acc,
                        width,
                        label="Cross-cohort",
                        alpha=0.8,
                    )
                    axes[0, 0].bar(
                        x + width / 2,
                        within_cohort_acc,
                        width,
                        label="Within-cohort",
                        alpha=0.8,
                    )

                    axes[0, 0].set_xlabel("Model")
                    axes[0, 0].set_ylabel("Accuracy")
                    axes[0, 0].set_title("Cross-cohort vs Within-cohort Performance")
                    axes[0, 0].set_xticks(x)
                    axes[0, 0].set_xticklabels(models)
                    axes[0, 0].legend()
                    axes[0, 0].grid(True, alpha=0.3)

            # Plot 2: Factor distribution comparison
            distribution_comparison = results.get("distribution_comparison", {})
            if distribution_comparison:
                factor_comparisons = distribution_comparison.get(
                    "factor_comparisons", []
                )

                if factor_comparisons:
                    factors = [f["factor"] for f in factor_comparisons]
                    train_means = [f["train_mean"] for f in factor_comparisons]
                    test_means = [f["test_mean"] for f in factor_comparisons]

                    axes[0, 1].scatter(train_means, test_means, s=100, alpha=0.7)

                    # Add diagonal line for perfect correspondence
                    min_val = min(min(train_means), min(test_means))
                    max_val = max(max(train_means), max(test_means))
                    axes[0, 1].plot(
                        [min_val, max_val], [min_val, max_val], "r--", alpha=0.7
                    )

                    axes[0, 1].set_xlabel("Training Cohort Factor Mean")
                    axes[0, 1].set_ylabel("Test Cohort Factor Mean")
                    axes[0, 1].set_title("Factor Distribution Comparison")
                    axes[0, 1].grid(True, alpha=0.3)

                    # Annotate points with factor numbers
                    for i, factor in enumerate(factors):
                        axes[0, 1].annotate(
                            f"F{factor}",
                            (train_means[i], test_means[i]),
                            xytext=(5, 5),
                            textcoords="offset points",
                        )

            # Plot 3: Cohort label distributions
            train_unique, train_counts = np.unique(
                clinical_labels_train, return_counts=True
            )
            test_unique, test_counts = np.unique(
                clinical_labels_test, return_counts=True
            )

            # Normalize to proportions
            train_props = train_counts / len(clinical_labels_train)
            test_props = test_counts / len(clinical_labels_test)

            x = np.arange(len(train_unique))
            width = 0.35

            axes[1, 0].bar(
                x - width / 2, train_props, width, label="Training Cohort", alpha=0.8
            )
            axes[1, 0].bar(
                x + width / 2, test_props, width, label="Test Cohort", alpha=0.8
            )

            axes[1, 0].set_xlabel("Clinical Label")
            axes[1, 0].set_ylabel("Proportion")
            axes[1, 0].set_title("Clinical Label Distribution Comparison")
            axes[1, 0].set_xticks(x)
            axes[1, 0].set_xticklabels([f"Class {label}" for label in train_unique])
            axes[1, 0].legend()
            axes[1, 0].grid(True, alpha=0.3)

            # Plot 4: Transferability metrics
            transferability_results = results.get("transferability_analysis", {})
            if transferability_results:
                metrics = []
                values = []

                # Model fit comparison
                model_fit = transferability_results.get("model_fit_comparison", {})
                if model_fit:
                    train_ll = model_fit.get("train_likelihood", 0)
                    test_ll = model_fit.get("test_likelihood", 0)
                    if train_ll != 0:
                        metrics.append("Likelihood Ratio")
                        values.append(test_ll / train_ll)

                # Factor space similarity
                factor_similarity = transferability_results.get(
                    "factor_space_similarity", {}
                )
                if factor_similarity:
                    transferability_score = factor_similarity.get(
                        "transferability_score", 0
                    )
                    metrics.append("Factor Similarity")
                    values.append(transferability_score)

                # Demographic transferability
                demo_transfer = transferability_results.get(
                    "demographic_transferability", {}
                )
                if demo_transfer:
                    distribution_similarity = demo_transfer.get(
                        "distribution_similarity", 0
                    )
                    metrics.append("Demographic Similarity")
                    values.append(distribution_similarity)

                if metrics:
                    colors = [
                        "green" if v > 0.7 else "orange" if v > 0.5 else "red"
                        for v in values
                    ]
                    axes[1, 1].bar(metrics, values, color=colors, alpha=0.8)
                    axes[1, 1].set_ylabel("Score")
                    axes[1, 1].set_title("Transferability Metrics")
                    axes[1, 1].tick_params(axis="x", rotation=45)
                    axes[1, 1].axhline(
                        0.7, color="green", linestyle="--", alpha=0.7, label="Good"
                    )
                    axes[1, 1].axhline(
                        0.5, color="orange", linestyle="--", alpha=0.7, label="Moderate"
                    )
                    axes[1, 1].legend()
                    axes[1, 1].grid(True, alpha=0.3)

            plt.tight_layout()
            plots["external_cohort_validation"] = fig

        except Exception as e:
            self.logger.warning(
                f"Failed to create external cohort validation plots: {str(e)}"
            )

        return plots

    def _create_comprehensive_clinical_visualizations(
        self,
        X_list: List[np.ndarray],
        results: Dict,
        clinical_data: np.ndarray,
        experiment_name: str,
    ) -> Dict:
        """Create comprehensive clinical visualizations focusing on subtypes and brain maps."""
        advanced_plots = {}

        try:
            self.logger.info(
                f"ðŸŽ¨ Creating comprehensive clinical visualizations for {experiment_name}"
            )

            # Import visualization system
            from core.config_utils import ConfigAccessor
            from visualization.manager import VisualizationManager

            # Create a clinical-focused config for visualization
            viz_config = ConfigAccessor(
                {
                    "visualization": {
                        "create_brain_viz": True,
                        "output_format": ["png", "pdf"],
                        "dpi": 300,
                        "clinical_focus": True,
                    },
                    "output_dir": f"/tmp/clinical_viz_{experiment_name}",
                }
            )

            # Initialize visualization manager
            viz_manager = VisualizationManager(viz_config)

            # Prepare clinical data structure for visualizations
            data = {
                "X_list": X_list,
                "view_names": [f"view_{i}" for i in range(len(X_list))],
                "n_subjects": X_list[0].shape[0],
                "view_dimensions": [X.shape[1] for X in X_list],
                "clinical_labels": clinical_data,
                "preprocessing": {
                    "status": "completed",
                    "strategy": "clinical_neuroimaging",
                },
            }

            # Extract the best clinical result for detailed analysis
            best_clinical_result = self._extract_best_clinical_result(results)

            if best_clinical_result:
                # Prepare clinical analysis results
                analysis_results = {
                    "best_run": best_clinical_result,
                    "all_runs": results,
                    "model_type": "clinical_sparseGFA",
                    "convergence": best_clinical_result.get("convergence", False),
                    "clinical_validation": True,
                }

                # Add cross-validation results for subtype consensus if available
                cv_results = None
                if "subtype_stability" in results:
                    cv_results = {
                        "subtype_analysis": results["subtype_stability"],
                        "consensus_subtypes": results.get("consensus_subtypes", {}),
                        "stability_metrics": results.get("stability_analysis", {}),
                    }

                # Create all comprehensive visualizations with clinical focus
                viz_manager.create_all_visualizations(
                    data=data, analysis_results=analysis_results, cv_results=cv_results
                )

                # Extract the generated plots and convert to matplotlib figures
                if hasattr(viz_manager, "plot_dir") and viz_manager.plot_dir.exists():
                    plot_files = list(viz_manager.plot_dir.glob("**/*.png"))

                    for plot_file in plot_files:
                        plot_name = f"clinical_{plot_file.stem}"

                        # Load the saved plot as a matplotlib figure
                        try:
                            import matplotlib.image as mpimg
                            import matplotlib.pyplot as plt

                            fig, ax = plt.subplots(figsize=(12, 8))
                            img = mpimg.imread(str(plot_file))
                            ax.imshow(img)
                            ax.axis("off")
                            ax.set_title(f"Clinical Analysis: {plot_name}", fontsize=14)

                            advanced_plots[plot_name] = fig

                        except Exception as e:
                            self.logger.warning(
                                f"Could not load clinical plot {plot_name}: {e}"
                            )

                    self.logger.info(
                        f"âœ… Created { len(plot_files)} comprehensive clinical visualizations"
                    )

                    # Additional clinical-specific summary
                    if cv_results and "subtype_analysis" in cv_results:
                        self.logger.info(
                            "   â†’ Subtype discovery and consensus plots generated"
                        )
                    if "brain_maps" in [f.stem for f in plot_files]:
                        self.logger.info(
                            "   â†’ Clinical brain mapping visualizations generated"
                        )

                else:
                    self.logger.warning(
                        "Clinical visualization manager did not create plot directory"
                    )
            else:
                self.logger.warning(
                    "No converged clinical results found for comprehensive visualization"
                )

        except Exception as e:
            self.logger.warning(
                f"Failed to create comprehensive clinical visualizations: {e}"
            )
            # Don't fail the experiment if advanced visualizations fail

        return advanced_plots

    def _extract_best_clinical_result(self, results: Dict) -> Optional[Dict]:
        """Extract the best clinical analysis result from experiment results."""
        best_result = None
        best_score = float("-inf")

        # Look for clinical results with SGFA data
        if "sgfa_factors" in results:
            # From subtype classification
            result = results["sgfa_factors"]
            if result and result.get("convergence", False):
                score = result.get(
                    "accuracy", result.get("log_likelihood", float("-inf"))
                )
                if score > best_score:
                    best_score = score
                    best_result = result

        # Look in other result structures
        for key, result in results.items():
            if (
                isinstance(result, dict)
                and "Z" in result
                and result.get("convergence", False)
            ):
                score = result.get("log_likelihood", float("-inf"))
                if score > best_score:
                    best_score = score
                    best_result = result

        return best_result

    def _analyze_pd_subtype_discovery(self, results: Dict) -> Dict:
        """Comprehensive analysis of PD subtype discovery results."""
        analysis = {}

        # Clustering quality analysis
        subtype_discovery = results["subtype_discovery"]
        analysis["clustering_quality"] = {
            "optimal_k": subtype_discovery["optimal_k"],
            "best_silhouette": max(subtype_discovery["silhouette_scores"]),
            "quality_grade": "Excellent" if max(subtype_discovery["silhouette_scores"]) > 0.7 else
                           "Good" if max(subtype_discovery["silhouette_scores"]) > 0.5 else
                           "Moderate" if max(subtype_discovery["silhouette_scores"]) > 0.3 else "Poor"
        }

        # Clinical validation analysis
        if "clinical_validation" in results:
            cv = results["clinical_validation"]
            analysis["clinical_validation"] = {
                "validation_score": cv["validation_score"],
                "validation_grade": "Strong" if cv["validation_score"] > 0.5 else
                                  "Moderate" if cv["validation_score"] > 0.3 else "Weak"
            }

        # Stability analysis
        if "stability_analysis" in results:
            sa = results["stability_analysis"]
            if "error" not in sa:
                analysis["stability"] = {
                    "mean_ari": sa["mean_ari"],
                    "stability_grade": sa["stability_grade"]
                }

        # Overall assessment
        grades = [analysis["clustering_quality"]["quality_grade"]]
        if "clinical_validation" in analysis:
            grades.append(analysis["clinical_validation"]["validation_grade"])
        if "stability" in analysis:
            grades.append(analysis["stability"]["stability_grade"])

        # Convert grades to numeric for overall score
        grade_scores = {"Excellent": 4, "Strong": 4, "High": 4, "Good": 3, "Moderate": 2, "Medium": 2, "Poor": 1, "Weak": 1, "Low": 1}
        numeric_scores = [grade_scores.get(grade, 1) for grade in grades]
        overall_score = np.mean(numeric_scores)

        analysis["overall_assessment"] = {
            "score": overall_score,
            "grade": "Excellent" if overall_score >= 3.5 else "Good" if overall_score >= 2.5 else "Moderate" if overall_score >= 1.5 else "Poor",
            "components_evaluated": len(grades)
        }

        return analysis

    @experiment_handler("integrated_sgfa_clinical_optimization")
    @validate_data_types(
        X_list=list,
        clinical_data=dict,
        hypers=dict,
        args=dict,
    )
    @validate_parameters(
        X_list=lambda x: len(x) > 0,
        clinical_data=lambda x: x is not None,
        hypers=lambda x: isinstance(x, dict),
        args=lambda x: isinstance(x, dict),
    )
    def run_integrated_sgfa_clinical_optimization(
        self,
        X_list: List[np.ndarray],
        clinical_data: Dict,
        hypers: Dict,
        args: Dict,
        **kwargs,
    ) -> ExperimentResult:
        """Run integrated SGFA performance + PD subtype discovery optimization.

        This method measures both computational performance (SGFA metrics) and
        clinical research utility (PD subtype discovery quality) simultaneously
        to find optimal K values for clinical validation.

        Parameters
        ----------
        X_list : List[np.ndarray]
            Multi-modal neuroimaging data
        clinical_data : Dict
            Clinical measures for validation
        hypers : Dict
            SGFA hyperparameters
        args : Dict
            Analysis arguments
        **kwargs
            Additional arguments

        Returns
        -------
        ExperimentResult
            Integrated performance and subtype discovery results
        """
        self.logger.info("Running integrated SGFA performance + PD subtype discovery optimization")

        results = {}

        # Test multiple K values to assess performance-discovery trade-offs
        K_test_values = [3, 5, 8, 10]  # Unified K values across experiments
        integrated_results = {}

        for K in K_test_values:
            self.logger.info(f"Testing K={K} factors for clinical optimization")

            try:
                with self.profiler.profile(f"clinical_optimization_K{K}") as p:
                    # === LAYER 1: SGFA PERFORMANCE METRICS ===
                    sgfa_start = time.time()

                    # Update hyperparameters for this K
                    test_hypers = hypers.copy()
                    test_hypers["K"] = K
                    test_args = args.copy()
                    test_args["K"] = K

                    # Run SGFA with performance monitoring
                    sgfa_result = self._run_sgfa_training(X_list, test_hypers, test_args)

                    if "error" in sgfa_result:
                        integrated_results[f"K{K}"] = {"error": sgfa_result["error"]}
                        continue

                    sgfa_time = time.time() - sgfa_start
                    Z_sgfa = sgfa_result["Z"]  # Factor scores [n_subjects x n_factors]

                    # === LAYER 2: PD SUBTYPE DISCOVERY METRICS ===
                    subtype_start = time.time()

                    # Test clustering with different numbers of subtypes
                    best_silhouette = -1
                    best_k = 2
                    clustering_attempts = {}

                    for n_clusters in range(2, min(7, Z_sgfa.shape[0]//5)):  # Ensure enough samples
                        try:
                            kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
                            cluster_labels = kmeans.fit_predict(Z_sgfa)
                            sil_score = silhouette_score(Z_sgfa, cluster_labels)
                            cal_score = calinski_harabasz_score(Z_sgfa, cluster_labels)

                            clustering_attempts[n_clusters] = {
                                "silhouette_score": sil_score,
                                "calinski_score": cal_score,
                                "inertia": kmeans.inertia_
                            }

                            if sil_score > best_silhouette:
                                best_silhouette = sil_score
                                best_k = n_clusters

                        except Exception as e:
                            clustering_attempts[n_clusters] = {"error": str(e)}

                    subtype_discovery_time = time.time() - subtype_start

                    # === LAYER 3: CLINICAL TRANSLATION METRICS ===
                    clinical_start = time.time()
                    clinical_separation_score = 0
                    significant_measures = 0
                    total_measures = 0

                    if clinical_data and best_silhouette > 0:
                        # Use best clustering for clinical validation
                        kmeans = KMeans(n_clusters=best_k, random_state=42, n_init=10)
                        subtype_labels = kmeans.fit_predict(Z_sgfa)

                        # Convert clinical data to DataFrame if needed
                        if isinstance(clinical_data, dict) and 'clinical' in clinical_data:
                            clinical_df = clinical_data['clinical']
                        else:
                            clinical_df = pd.DataFrame(clinical_data)

                        # Align data sizes
                        n_imaging = len(subtype_labels)
                        n_clinical = len(clinical_df)
                        clinical_aligned = clinical_df.iloc[:n_imaging] if n_clinical > n_imaging else clinical_df

                        # Test clinical separation
                        clinical_measures = ['age'] + [col for col in clinical_aligned.columns
                                                     if any(keyword in col.lower() for keyword in
                                                           ['updrs', 'motor', 'duration', 'stage'])]

                        for measure in clinical_measures[:5]:  # Test top 5 measures
                            if measure in clinical_aligned.columns:
                                total_measures += 1
                                try:
                                    groups = [clinical_aligned[measure][subtype_labels == i].dropna()
                                            for i in range(best_k)]
                                    if all(len(g) >= 3 for g in groups):  # Need minimum samples
                                        f_stat, p_val = f_oneway(*groups)
                                        if p_val < 0.05:
                                            significant_measures += 1
                                except Exception as e:
                                    self.logger.debug(f"Clinical validation failed for {measure}: {e}")

                        clinical_separation_score = significant_measures / total_measures if total_measures > 0 else 0

                    clinical_validation_time = time.time() - clinical_start

                    # === LAYER 4: PERFORMANCE-DISCOVERY TRADE-OFF ANALYSIS ===
                    metrics = p.get_current_metrics()

                    integrated_results[f"K{K}"] = {
                        # SGFA Performance Layer
                        "sgfa_time_seconds": sgfa_time,
                        "sgfa_memory_mb": metrics.peak_memory_gb * 1024,
                        "sgfa_convergence_rate": sgfa_result.get("convergence_rate", 0),
                        "factor_extraction_time": sgfa_result.get("factor_extraction_time", 0),

                        # PD Subtype Discovery Layer
                        "subtype_discovery_time_seconds": subtype_discovery_time,
                        "optimal_subtypes_detected": best_k,
                        "best_silhouette_score": best_silhouette,
                        "clustering_quality_grade": "Excellent" if best_silhouette > 0.7 else
                                                  "Good" if best_silhouette > 0.5 else
                                                  "Moderate" if best_silhouette > 0.3 else "Poor",
                        "clustering_attempts": clustering_attempts,

                        # Clinical Translation Layer
                        "clinical_validation_time_seconds": clinical_validation_time,
                        "clinical_separation_score": clinical_separation_score,
                        "significant_clinical_measures": significant_measures,
                        "total_clinical_measures": total_measures,
                        "clinical_validation_grade": "Strong" if clinical_separation_score > 0.5 else
                                                   "Moderate" if clinical_separation_score > 0.3 else "Weak",

                        # Integrated Clinical-Performance Metrics
                        "clinical_quality_per_second": best_silhouette / sgfa_time if sgfa_time > 0 else 0,
                        "clinical_efficiency": clinical_separation_score / clinical_validation_time if clinical_validation_time > 0 else 0,
                        "total_time_seconds": sgfa_time + subtype_discovery_time + clinical_validation_time,

                        # Overall Clinical Assessment
                        "integrated_clinical_score": (best_silhouette + clinical_separation_score) / 2,
                        "clinical_performance_efficiency": best_silhouette / (sgfa_time/60) if sgfa_time > 0 else 0  # Quality per minute
                    }

            except Exception as e:
                self.logger.error(f"Clinical optimization K={K} failed: {e}")
                integrated_results[f"K{K}"] = {"error": str(e)}

        results["clinical_optimization"] = integrated_results

        # === CLINICAL TRADE-OFF ANALYSIS ACROSS K VALUES ===
        successful_runs = {k: v for k, v in integrated_results.items() if "error" not in v}

        if len(successful_runs) > 1:
            # Clinical Quality vs Performance trade-offs
            speeds = [(k, v["sgfa_time_seconds"]) for k, v in successful_runs.items()]
            clinical_qualities = [(k, v["best_silhouette_score"]) for k, v in successful_runs.items()]
            clinical_validations = [(k, v["clinical_separation_score"]) for k, v in successful_runs.items()]

            if speeds and clinical_qualities:
                fastest_k = min(speeds, key=lambda x: x[1])[0]
                highest_clinical_quality_k = max(clinical_qualities, key=lambda x: x[1])[0]
                best_clinical_validation_k = max(clinical_validations, key=lambda x: x[1])[0]

                results["clinical_trade_off_analysis"] = {
                    "fastest_model": fastest_k,
                    "fastest_time": min(speeds, key=lambda x: x[1])[1],
                    "highest_clinical_quality_model": highest_clinical_quality_k,
                    "highest_clinical_quality_score": max(clinical_qualities, key=lambda x: x[1])[1],
                    "best_clinical_validation_model": best_clinical_validation_k,
                    "best_clinical_validation_score": max(clinical_validations, key=lambda x: x[1])[1],
                    "speed_quality_tradeoff": fastest_k != highest_clinical_quality_k,
                    "speed_validation_tradeoff": fastest_k != best_clinical_validation_k,
                    "quality_validation_alignment": highest_clinical_quality_k == best_clinical_validation_k
                }

        # Comprehensive clinical analysis
        analysis = self._analyze_integrated_clinical_optimization(results)

        # Generate plots using PDSubtypeVisualizer
        visualizer = PDSubtypeVisualizer(self.config)
        # Use experiment-specific output directory if available
        if hasattr(self, 'base_output_dir') and self.base_output_dir:
            from pathlib import Path
            plot_dir = Path(self.base_output_dir) / "clinical_optimization_plots"
        else:
            plot_dir = get_output_dir(self.config) / "clinical_optimization_plots"
        plot_dir.mkdir(exist_ok=True, parents=True)

        # Pass only the clinical_optimization data to the visualizer
        optimization_results = results.get("clinical_optimization", {})
        if optimization_results:
            plots = visualizer.create_performance_subtype_plots(optimization_results, plot_dir)
        else:
            self.logger.warning("No clinical optimization results available for plotting")
            plots = {}

        return ExperimentResult(
            experiment_id="integrated_sgfa_clinical_optimization",
            config=self.config,
            start_time=datetime.now(),
            end_time=datetime.now(),
            status="completed",
            model_results=results,
            diagnostics=analysis,
            plots=plots,
        )

    def _analyze_integrated_clinical_optimization(self, results: Dict) -> Dict:
        """Analyze integrated SGFA + PD subtype discovery clinical optimization results."""
        analysis = {
            "performance_summary": {},
            "clinical_discovery_summary": {},
            "clinical_validation_summary": {},
            "clinical_trade_off_insights": {},
            "clinical_recommendations": {}
        }

        optimization_results = results.get("clinical_optimization", {})
        successful_runs = {k: v for k, v in optimization_results.items() if "error" not in v}

        if not successful_runs:
            analysis["error"] = "No successful clinical optimization runs"
            return analysis

        # Performance summary
        sgfa_times = [v["sgfa_time_seconds"] for v in successful_runs.values()]
        memory_usage = [v["sgfa_memory_mb"] for v in successful_runs.values()]

        analysis["performance_summary"] = {
            "avg_sgfa_time": np.mean(sgfa_times),
            "min_sgfa_time": np.min(sgfa_times),
            "max_sgfa_time": np.max(sgfa_times),
            "avg_memory_usage_mb": np.mean(memory_usage),
            "performance_scalability": "Good" if np.max(sgfa_times) / np.min(sgfa_times) < 3 else "Poor"
        }

        # Clinical discovery summary
        silhouette_scores = [v["best_silhouette_score"] for v in successful_runs.values()]
        optimal_ks = [v["optimal_subtypes_detected"] for v in successful_runs.values()]

        analysis["clinical_discovery_summary"] = {
            "avg_silhouette_score": np.mean(silhouette_scores),
            "best_silhouette_score": np.max(silhouette_scores),
            "most_common_optimal_k": max(set(optimal_ks), key=optimal_ks.count),
            "discovery_consistency": "High" if np.std(silhouette_scores) < 0.1 else "Moderate" if np.std(silhouette_scores) < 0.2 else "Low"
        }

        # Clinical validation summary
        clinical_scores = [v["clinical_separation_score"] for v in successful_runs.values()]
        clinical_efficiency = [v["clinical_efficiency"] for v in successful_runs.values()]

        analysis["clinical_validation_summary"] = {
            "avg_clinical_separation": np.mean(clinical_scores),
            "best_clinical_separation": np.max(clinical_scores),
            "avg_clinical_efficiency": np.mean(clinical_efficiency),
            "clinical_validation_quality": "Strong" if np.mean(clinical_scores) > 0.5 else
                                          "Moderate" if np.mean(clinical_scores) > 0.3 else "Weak"
        }

        # Clinical trade-off insights
        if "clinical_trade_off_analysis" in results:
            trade_offs = results["clinical_trade_off_analysis"]
            analysis["clinical_trade_off_insights"] = {
                "speed_quality_conflict": trade_offs.get("speed_quality_tradeoff", False),
                "speed_validation_conflict": trade_offs.get("speed_validation_tradeoff", False),
                "quality_validation_alignment": trade_offs.get("quality_validation_alignment", False),
                "optimal_clinical_model": trade_offs.get("highest_clinical_quality_model", "Unknown")
            }

        # Clinical recommendations
        best_clinical_k = max(successful_runs.keys(),
                           key=lambda k: successful_runs[k]["integrated_clinical_score"])

        analysis["clinical_recommendations"] = {
            "recommended_k_clinical": best_clinical_k,
            "reason": f"Best integrated clinical score: {successful_runs[best_clinical_k]['integrated_clinical_score']:.3f}",
            "clinical_performance_grade": "Excellent" if analysis["performance_summary"]["performance_scalability"] == "Good" and
                                                       analysis["clinical_discovery_summary"]["discovery_consistency"] == "High" else "Good",
            "clinical_readiness": analysis["clinical_validation_summary"]["clinical_validation_quality"]
        }

        return analysis

    def validate_laterality_patterns(
        self,
        factor_loadings: pd.DataFrame,
        roi_name: str = "Unknown"
    ) -> Dict:
        """
        Validate expected clinical laterality patterns in factor loadings.

        Args:
            factor_loadings: DataFrame with features as rows, factors as columns
            roi_name: Name of the ROI being analyzed

        Returns:
            Dictionary with validation results and scores
        """
        self.logger.info(f"ðŸ” Validating laterality patterns for {roi_name}")

        results = {
            'roi_name': roi_name,
            'factors': {},
            'summary': {}
        }

        for factor_col in factor_loadings.columns:
            loadings = factor_loadings[factor_col]

            # Categorize features by symptom type and laterality
            left_brady_rate = []
            left_brady_speed = []
            right_brady_rate = []
            right_brady_speed = []
            left_rigidity = []
            right_rigidity = []
            left_tremor = []
            right_tremor = []
            right_mirror = []
            left_mirror = []

            for feature, loading in loadings.items():
                if abs(loading) < 0.01:  # Skip very small loadings
                    continue

                feature_lower = str(feature).lower()

                # Determine laterality
                is_left = '-left' in feature_lower or '_left' in feature_lower
                is_right = '-right' in feature_lower or '_right' in feature_lower

                # Categorize by symptom and laterality
                if 'bradykinesia' in feature_lower:
                    if 'rate' in feature_lower:
                        if is_left:
                            left_brady_rate.append((feature, loading))
                        elif is_right:
                            right_brady_rate.append((feature, loading))
                    elif 'speed' in feature_lower:
                        if is_left:
                            left_brady_speed.append((feature, loading))
                        elif is_right:
                            right_brady_speed.append((feature, loading))
                elif 'rigidity' in feature_lower:
                    if is_left:
                        left_rigidity.append((feature, loading))
                    elif is_right:
                        right_rigidity.append((feature, loading))
                elif 'tremor' in feature_lower:
                    if is_left:
                        left_tremor.append((feature, loading))
                    elif is_right:
                        right_tremor.append((feature, loading))
                elif 'mirror' in feature_lower:
                    if is_left:
                        left_mirror.append((feature, loading))
                    elif is_right:
                        right_mirror.append((feature, loading))

            # Pattern 1: Bradykinesia + contralateral mirror movements align
            # 1a: Left bradykinesia + right mirror
            # 1b: Right bradykinesia + left mirror (symmetric pattern)
            left_brady_features = left_brady_rate + left_brady_speed
            right_brady_features = right_brady_rate + right_brady_speed

            pattern1a_score = 0.0
            pattern1b_score = 0.0

            # Pattern 1a: left brady + right mirror
            if len(left_brady_features) >= 1 and len(right_mirror) > 0:
                left_brady_values = [x[1] for x in left_brady_features]
                right_mirror_values = [x[1] for x in right_mirror]

                left_brady_mean = np.mean(left_brady_values)
                right_mirror_mean = np.mean(right_mirror_values)

                same_sign_1a = (left_brady_mean * right_mirror_mean) > 0

                if abs(left_brady_mean) > 0.01 and abs(right_mirror_mean) > 0.01:
                    if same_sign_1a:
                        left_consistency = 1.0 - min(1.0, np.std(left_brady_values) / (abs(left_brady_mean) + 1e-6))
                        pattern1a_score = 0.5 + 0.5 * max(0, left_consistency)
                    else:
                        pattern1a_score = 0.1

            # Pattern 1b: right brady + left mirror (symmetric)
            if len(right_brady_features) >= 1 and len(left_mirror) > 0:
                right_brady_values = [x[1] for x in right_brady_features]
                left_mirror_values = [x[1] for x in left_mirror]

                right_brady_mean = np.mean(right_brady_values)
                left_mirror_mean = np.mean(left_mirror_values)

                same_sign_1b = (right_brady_mean * left_mirror_mean) > 0

                if abs(right_brady_mean) > 0.01 and abs(left_mirror_mean) > 0.01:
                    if same_sign_1b:
                        right_consistency = 1.0 - min(1.0, np.std(right_brady_values) / (abs(right_brady_mean) + 1e-6))
                        pattern1b_score = 0.5 + 0.5 * max(0, right_consistency)
                    else:
                        pattern1b_score = 0.1

            # Take max since they're symmetric alternatives (usually only one is dominant)
            pattern1_score = max(pattern1a_score, pattern1b_score)

            pattern1_details = {
                'left_brady_right_mirror': {
                    'left_brady_features': left_brady_features,
                    'right_mirror': right_mirror,
                    'score': pattern1a_score
                },
                'right_brady_left_mirror': {
                    'right_brady_features': right_brady_features,
                    'left_mirror': left_mirror,
                    'score': pattern1b_score
                },
                'alignment_score': pattern1_score,
                'dominant_pattern': 'left_brady_right_mirror' if pattern1a_score > pattern1b_score else 'right_brady_left_mirror'
            }

            # Pattern 2: Opposite bradykinesia sides tend opposite directions
            pattern2_score = 0.0
            pattern2_details = {}

            if len(left_brady_features) >= 1 and len(right_brady_features) >= 1:
                left_brady_values = [x[1] for x in left_brady_features]
                right_brady_values = [x[1] for x in right_brady_features]

                left_mean = np.mean(left_brady_values)
                right_mean = np.mean(right_brady_values)

                # Check if they tend to have opposite signs
                opposite_sign = (left_mean * right_mean) < 0

                if abs(left_mean) > 0.01 and abs(right_mean) > 0.01:
                    if opposite_sign:
                        # Score based on consistency within each side
                        left_consistency = 1.0 - min(1.0, np.std(left_brady_values) / (abs(left_mean) + 1e-6))
                        right_consistency = 1.0 - min(1.0, np.std(right_brady_values) / (abs(right_mean) + 1e-6))

                        avg_consistency = (max(0, left_consistency) + max(0, right_consistency)) / 2
                        pattern2_score = 0.5 + 0.5 * avg_consistency
                    else:
                        # Partial credit for presence
                        pattern2_score = 0.1

                pattern2_details = {
                    'left_mean': left_mean,
                    'right_mean': right_mean,
                    'opposite_tendency': opposite_sign,
                    'opposition_score': pattern2_score,
                    'left_features': left_brady_features,
                    'right_features': right_brady_features
                }

            # Pattern 3: Tremor splitting (optional tendency)
            pattern3_score = 0.0
            pattern3_details = {}

            if len(left_tremor) > 0 and len(right_tremor) > 0:
                left_tremor_mean = np.mean([x[1] for x in left_tremor])
                right_tremor_mean = np.mean([x[1] for x in right_tremor])

                # Check if they tend to have opposite signs
                opposite_tremor = (left_tremor_mean * right_tremor_mean) < 0

                if abs(left_tremor_mean) > 0.01 and abs(right_tremor_mean) > 0.01:
                    pattern3_score = 1.0 if opposite_tremor else 0.2  # Partial credit

                pattern3_details = {
                    'left_tremor': left_tremor,
                    'right_tremor': right_tremor,
                    'left_mean': left_tremor_mean,
                    'right_mean': right_tremor_mean,
                    'split_tendency': opposite_tremor,
                    'split_score': pattern3_score
                }

            # Calculate overall validation score (0 to 2.5)
            # Pattern 1: 0-1, Pattern 2: 0-1, Pattern 3: 0-0.5 (optional)
            validation_score = pattern1_score + pattern2_score + (0.5 * pattern3_score)

            results['factors'][factor_col] = {
                'validation_score': validation_score,
                'pattern1_left_brady_right_mirror': pattern1_details,
                'pattern2_opposite_bradykinesia': pattern2_details,
                'pattern3_tremor_split': pattern3_details,
                'all_categorized': {
                    'left_brady_rate': left_brady_rate,
                    'left_brady_speed': left_brady_speed,
                    'right_brady_rate': right_brady_rate,
                    'right_brady_speed': right_brady_speed,
                    'left_rigidity': left_rigidity,
                    'right_rigidity': right_rigidity,
                    'left_tremor': left_tremor,
                    'right_tremor': right_tremor,
                    'left_mirror': left_mirror,
                    'right_mirror': right_mirror,
                }
            }

        # Compute summary statistics based on tendencies (not strict pass/fail)
        total_factors = len(results['factors'])

        # Count factors showing strong tendencies (score > 0.5 for each pattern)
        factors_with_pattern1_tendency = sum(
            1 for f in results['factors'].values()
            if f['pattern1_left_brady_right_mirror'].get('alignment_score', 0) > 0.5
        )
        factors_with_pattern2_tendency = sum(
            1 for f in results['factors'].values()
            if f['pattern2_opposite_bradykinesia'].get('opposition_score', 0) > 0.5
        )
        factors_with_pattern3_tendency = sum(
            1 for f in results['factors'].values()
            if f['pattern3_tremor_split'].get('split_score', 0) > 0.5
        )

        # Average scores across all factors
        avg_pattern1 = np.mean([
            f['pattern1_left_brady_right_mirror'].get('alignment_score', 0)
            for f in results['factors'].values()
        ]) if total_factors > 0 else 0

        avg_pattern2 = np.mean([
            f['pattern2_opposite_bradykinesia'].get('opposition_score', 0)
            for f in results['factors'].values()
        ]) if total_factors > 0 else 0

        avg_pattern3 = np.mean([
            f['pattern3_tremor_split'].get('split_score', 0)
            for f in results['factors'].values()
        ]) if total_factors > 0 else 0

        avg_score = np.mean([f['validation_score'] for f in results['factors'].values()]) if total_factors > 0 else 0

        # Determine validation status based on tendencies
        # STRONG: Both patterns show strong tendency (avg > 0.5)
        # MODERATE: At least one pattern shows tendency
        # WEAK: No strong tendencies detected
        if avg_pattern1 > 0.5 and avg_pattern2 > 0.5:
            validation_status = 'STRONG'
        elif avg_pattern1 > 0.3 or avg_pattern2 > 0.3:
            validation_status = 'MODERATE'
        else:
            validation_status = 'WEAK'

        results['summary'] = {
            'total_factors': total_factors,
            'factors_with_pattern1_tendency': factors_with_pattern1_tendency,
            'factors_with_pattern2_tendency': factors_with_pattern2_tendency,
            'factors_with_pattern3_tendency': factors_with_pattern3_tendency,
            'avg_pattern1_score': avg_pattern1,
            'avg_pattern2_score': avg_pattern2,
            'avg_pattern3_score': avg_pattern3,
            'average_validation_score': avg_score,
            'validation_status': validation_status
        }

        self.logger.info(f"  Total factors: {total_factors}")
        self.logger.info(f"  Pattern 1 tendency (Left brady + right mirror): {factors_with_pattern1_tendency} factors (avg: {avg_pattern1:.2f})")
        self.logger.info(f"  Pattern 2 tendency (Opposite bradykinesia): {factors_with_pattern2_tendency} factors (avg: {avg_pattern2:.2f})")
        self.logger.info(f"  Pattern 3 tendency (Tremor split): {factors_with_pattern3_tendency} factors (avg: {avg_pattern3:.2f})")
        self.logger.info(f"  Average validation score: {avg_score:.2f}/2.5")
        self.logger.info(f"  Status: {results['summary']['validation_status']}")

        return results


def run_clinical_validation(config):
    """Run clinical validation experiments with remote workstation integration."""
    import logging
    import os
    import sys

    import numpy as np

    logger = logging.getLogger(__name__)
    logger.info("Starting Clinical Validation Experiments")

    try:
        # Add project root to path for imports
        current_file = os.path.abspath(__file__)
        project_root = os.path.dirname(os.path.dirname(current_file))
        if project_root not in sys.path:
            sys.path.insert(0, project_root)

        # Check if shared data is available from previous experiments
        from data.preprocessing_integration import apply_preprocessing_to_pipeline
        from experiments.framework import ExperimentConfig, ExperimentFramework
        from core.config_utils import ConfigHelper

        config_dict = ConfigHelper.to_dict(config)

        # Check for shared data from previous experiments (data_validation, robustness, or factor_stability)
        if "_shared_data" in config_dict and config_dict["_shared_data"].get("X_list") is not None:
            logger.info("ðŸ”— Using shared preprocessed data from previous experiments...")
            X_list = config_dict["_shared_data"]["X_list"]
            preprocessing_info = config_dict["_shared_data"].get("preprocessing_info", {})
            logger.info(f"âœ… Shared data: {len(X_list)} views for clinical validation")
            for i, X in enumerate(X_list):
                logger.info(f"   View {i}: {X.shape}")
        else:
            # Load data with advanced preprocessing for clinical validation
            logger.info("ðŸ”§ Loading data for clinical validation...")
            # Get preprocessing strategy from config, with clinical validation override
            preprocessing_config = config_dict.get("preprocessing", {})
            clinical_config = config_dict.get("clinical_validation", {})

            # Clinical validation can override preprocessing strategy
            strategy = clinical_config.get("preprocessing_strategy",
                                         preprocessing_config.get("strategy", "clinical_focused"))

            X_list, preprocessing_info = apply_preprocessing_to_pipeline(
                config=config_dict,
                data_dir=get_data_dir(config_dict),
                auto_select_strategy=False,
                preferred_strategy=strategy,  # Use strategy from config
            )

            logger.info(f"âœ… Data loaded: {len(X_list)} views for clinical validation")
            for i, X in enumerate(X_list):
                logger.info(f"   View {i}: {X.shape}")

        # Load clinical data
        try:
            from data.qmap_pd import load_qmap_pd

            load_qmap_pd(get_data_dir(config))

            # Extract clinical labels (using mock labels as fallback - real clinical
            # subtype data would be preferred)
            n_subjects = X_list[0].shape[0]
            clinical_labels = np.random.randint(0, 3, n_subjects)  # Mock: 3 PD subtypes
            logger.info(
                f"âœ… Clinical labels loaded: {len(np.unique(clinical_labels))} unique subtypes"
            )

            # Create clinical data dictionary for CV-aware methods
            clinical_data = {
                "diagnosis": clinical_labels,
                "subject_id": np.arange(n_subjects)  # Mock subject IDs
            }

        except Exception as e:
            logger.warning(f"Clinical data loading failed: {e}")
            # Create mock clinical labels for testing
            n_subjects = X_list[0].shape[0]
            clinical_labels = np.random.randint(0, 3, n_subjects)
            logger.info("Using mock clinical labels for testing")

            # Create clinical data dictionary for CV-aware methods
            clinical_data = {
                "diagnosis": clinical_labels,
                "subject_id": np.arange(n_subjects)  # Mock subject IDs
            }

        # Initialize experiment framework
        framework = ExperimentFramework(get_output_dir(config))

        exp_config = ExperimentConfig(
            experiment_name="clinical_validation",
            description="Clinical validation of SGFA factors for PD subtypes",
            dataset="qmap_pd",
            data_dir=get_data_dir(config),
        )

        # Create clinical validation experiment instance
        clinical_exp = ClinicalValidationExperiments(exp_config, logger)

        # Setup base hyperparameters
        base_hypers = {
            "Dm": [X.shape[1] for X in X_list],
            "a_sigma": 1.0,
            "b_sigma": 1.0,
            "slab_df": 4.0,
            "slab_scale": 2.0,
            "percW": 33.0,
            "K": 10,  # Base number of factors
        }

        # Setup base args
        base_args = {
            "K": 10,
            "num_warmup": 100,  # Moderate sampling for clinical validation
            "num_samples": 300,  # More samples for robust clinical results
            "num_chains": 1,
            "target_accept_prob": 0.8,
            "reghsZ": True,
        }

        # Run the experiment
        def clinical_validation_experiment(config, output_dir, **kwargs):
            logger.info("ðŸ¥ Running comprehensive clinical validation...")

            # Normalize config input using standard ConfigHelper
            from core.config_utils import ConfigHelper
            config_dict = ConfigHelper.to_dict(config)

            # Get clinical validation configuration
            clinical_config = config_dict.get("clinical_validation", {})
            validation_types = clinical_config.get("validation_types", ["subtype_classification"])
            classification_metrics = clinical_config.get("classification_metrics", ["accuracy"])

            # Use global cross-validation config with experiment-specific overrides
            global_cv_config = config_dict.get("cross_validation", {})
            experiment_cv_config = clinical_config.get("cross_validation", {})
            cv_config = {**global_cv_config, **experiment_cv_config}
            cv_config.setdefault("n_folds", 5)
            cv_config.setdefault("stratified", True)

            logger.info(f"ðŸ¥ Clinical validation types: {validation_types}")
            logger.info(f"ðŸ“Š Metrics to evaluate: {classification_metrics}")

            results = {}
            total_tests = 0
            successful_tests = 0

            # 1. Run SGFA to extract factors
            logger.info("ðŸ“Š Extracting SGFA factors...")
            try:
                # Use model_comparison's SGFA runner
                from experiments.model_comparison import ModelArchitectureComparison
                model_exp = ModelArchitectureComparison(clinical_exp.config, clinical_exp.logger)

                with clinical_exp.profiler.profile("sgfa_extraction") as p:
                    sgfa_result = model_exp._run_sgfa_analysis(
                        X_list, base_hypers, base_args
                    )

                if "error" in sgfa_result:
                    raise ValueError(f"SGFA failed: {sgfa_result['error']}")

                Z_sgfa = sgfa_result["Z"]  # Factor scores
                metrics = clinical_exp.profiler.get_current_metrics()

                results["sgfa_extraction"] = {
                    "result": sgfa_result,
                    "performance": {
                        "execution_time": metrics.execution_time,
                        "peak_memory_gb": metrics.peak_memory_gb,
                        "convergence": sgfa_result.get("convergence", False),
                        "log_likelihood": sgfa_result.get(
                            "log_likelihood", float("-inf")
                        ),
                    },
                    "factor_info": {
                        "n_factors": Z_sgfa.shape[1],
                        "n_subjects": Z_sgfa.shape[0],
                    },
                }
                successful_tests += 1
                logger.info(
                    f"âœ… SGFA extraction: { metrics.execution_time:.1f}s, { Z_sgfa.shape[1]} factors"
                )

            except Exception as e:
                logger.error(f"âŒ SGFA extraction failed: {e}")
                results["sgfa_extraction"] = {"error": str(e)}
                # Return early if SGFA fails
                return {
                    "status": "failed",
                    "error": "SGFA extraction failed",
                    "results": results,
                }

            total_tests += 1

            # 2. Run configured validation types
            for validation_type in validation_types:
                logger.info(f"ðŸ“Š Testing {validation_type}...")
                try:
                    if validation_type == "subtype_classification":
                        classification_results = clinical_exp.classifier.test_factor_classification(
                            Z_sgfa, clinical_labels, "sgfa_factors"
                        )
                        results[validation_type] = classification_results

                        # Log classification performance using configured metrics
                        for metric in classification_metrics:
                            if metric in ["accuracy", "precision", "recall", "f1_score"]:
                                best_score = max(
                                    [
                                        model_result.get(metric, 0)
                                        for model_result in classification_results.values()
                                    ]
                                )
                                logger.info(f"âœ… Best {metric}: {best_score:.3f}")

                        successful_tests += 1

                    elif validation_type == "disease_progression":
                        # Placeholder for disease progression validation
                        logger.info("âš ï¸  Disease progression validation not yet implemented")
                        results[validation_type] = {"status": "not_implemented"}

                    elif validation_type == "biomarker_discovery":
                        # Placeholder for biomarker discovery validation
                        logger.info("âš ï¸  Biomarker discovery validation not yet implemented")
                        results[validation_type] = {"status": "not_implemented"}

                    elif validation_type == "pd_subtype_discovery":
                        # PD subtype discovery using unsupervised clustering
                        logger.info("ðŸ§¬ Running PD subtype discovery validation...")
                        try:
                            pd_discovery_result = clinical_exp.run_pd_subtype_discovery(
                                X_list=X_list,
                                clinical_data=clinical_data,
                                hypers=base_hypers,
                                args=base_args
                            )
                            results[validation_type] = pd_discovery_result.data

                            # Log discovery results
                            if "subtype_discovery" in pd_discovery_result.data:
                                optimal_k = pd_discovery_result.data["subtype_discovery"]["optimal_k"]
                                best_silhouette = pd_discovery_result.data["subtype_discovery"]["best_solution"]["silhouette_score"]
                                logger.info(f"âœ… Discovered {optimal_k} PD subtypes (silhouette: {best_silhouette:.3f})")

                            if "clinical_validation" in pd_discovery_result.data:
                                validation_score = pd_discovery_result.data["clinical_validation"]["validation_score"]
                                logger.info(f"âœ… Clinical validation score: {validation_score:.3f}")

                            successful_tests += 1

                        except Exception as e:
                            logger.error(f"âŒ PD subtype discovery failed: {e}")
                            results[validation_type] = {"error": str(e)}

                    elif validation_type == "clinical_stratified_cv":
                        # Clinical-stratified cross-validation (using basic SGFA with clinical stratification)
                        #
                        # What this provides:
                        # - CV folds stratified by clinical variables (diagnosis, etc.)
                        # - Robust scaling (median/MAD) for neuroimaging data
                        # - Enhanced convergence checking and timeout handling
                        #
                        # Current limitations (future work):
                        # - Uses basic SGFA model (no neuroimaging-specific priors)
                        # - No spatial coherence modeling or brain connectivity structure
                        # - No scanner/site effect correction
                        # - No PD-specific disease modeling constraints
                        #
                        logger.info("ðŸ“Š Running clinical-stratified cross-validation pipeline...")
                        try:
                            # Convert clinical_data to DataFrame format expected by CV library
                            import pandas as pd
                            clinical_df = pd.DataFrame(clinical_data)

                            # Convert args dict to namespace object expected by CV library
                            import argparse
                            args_namespace = argparse.Namespace(**base_args)

                            # Initialize full neuroimaging CV pipeline
                            from analysis.cross_validation_library import NeuroImagingCrossValidator, NeuroImagingCVConfig, ParkinsonsConfig

                            cv_config_obj = NeuroImagingCVConfig(
                                outer_cv_folds=cv_config.get("n_folds", 5),
                                inner_cv_folds=3,  # Reduced for speed
                                stratified=cv_config.get("stratified", True)
                            )
                            pd_config = ParkinsonsConfig()

                            neuroimaging_cv = NeuroImagingCrossValidator(
                                config=cv_config_obj,
                                parkinsons_config=pd_config
                            )

                            # Run clinical-aware CV
                            with clinical_exp.profiler.profile("clinical_aware_cv") as p:
                                cv_results = neuroimaging_cv.neuroimaging_cross_validate(
                                    X_list=X_list,
                                    args=args_namespace,
                                    hypers=base_hypers,
                                    model_results={"clinical": clinical_df},
                                    cv_type="clinical_stratified"
                                )

                            cv_metrics = clinical_exp.profiler.get_current_metrics()
                            results[validation_type] = {
                                "cv_results": cv_results,
                                "performance": {
                                    "execution_time": cv_metrics.execution_time,
                                    "peak_memory_gb": cv_metrics.peak_memory_gb,
                                },
                                "n_folds": cv_config.get("n_folds", 5),
                                "cv_type": "clinical_stratified"
                            }

                            logger.info(f"âœ… Clinical-stratified CV completed: {cv_metrics.execution_time:.1f}s")
                            successful_tests += 1

                        except Exception as e:
                            logger.error(f"âŒ Clinical-stratified CV failed: {e}")
                            results[validation_type] = {"error": str(e)}

                    else:
                        logger.warning(f"âš ï¸  Unknown validation type: {validation_type}")
                        results[validation_type] = {"error": f"Unknown validation type: {validation_type}"}

                except Exception as e:
                    logger.error(f"âŒ {validation_type} failed: {e}")
                    results[validation_type] = {"error": str(e)}

                total_tests += 1

            # 3. Compare with baseline methods
            logger.info("ðŸ“Š Comparing with baseline methods...")
            try:
                # PCA comparison
                from sklearn.decomposition import PCA

                X_concat = np.hstack(X_list)
                pca = PCA(n_components=Z_sgfa.shape[1])
                Z_pca = pca.fit_transform(X_concat)

                pca_results = clinical_exp.classifier.test_factor_classification(
                    Z_pca, clinical_labels, "pca_features"
                )

                # Raw data comparison
                raw_results = clinical_exp.classifier.test_factor_classification(
                    X_concat, clinical_labels, "raw_data"
                )

                results["baseline_comparison"] = {
                    "pca": pca_results,
                    "raw_data": raw_results,
                }
                successful_tests += 1

                # Log comparison (use the first successful validation type)
                sgfa_acc = 0
                for validation_type in validation_types:
                    if validation_type in results and isinstance(results[validation_type], dict) and "error" not in results[validation_type]:
                        sgfa_acc = max(
                            [
                                r.get("accuracy", 0)
                                for r in results[validation_type].values()
                                if isinstance(r, dict)
                            ]
                        )
                        break
                pca_acc = max([r.get("accuracy", 0) for r in pca_results.values()])
                raw_acc = max([r.get("accuracy", 0) for r in raw_results.values()])

                logger.info(
                    f"âœ… Classification comparison - SGFA: { sgfa_acc:.3f}, PCA: { pca_acc:.3f}, Raw: { raw_acc:.3f}"
                )

            except Exception as e:
                logger.error(f"âŒ Baseline comparison failed: {e}")
                results["baseline_comparison"] = {"error": str(e)}

            total_tests += 1

            logger.info("ðŸ¥ Clinical validation completed!")
            logger.info(f"   Successful tests: {successful_tests}/{total_tests}")

            # Generate clinical validation plots
            import matplotlib.pyplot as plt
            plots = {}

            try:
                # Create clinical validation summary plot
                fig, axes = plt.subplots(1, 2, figsize=(14, 6))
                fig.suptitle("Clinical Validation Summary", fontsize=16)

                # Plot 1: Classification accuracy comparison
                methods = []
                accuracies = []

                # SGFA results
                for validation_type in validation_types:
                    if validation_type in results and isinstance(results[validation_type], dict) and "error" not in results[validation_type]:
                        for method_name, method_results in results[validation_type].items():
                            if isinstance(method_results, dict) and "error" not in method_results:
                                # Extract accuracy from nested structure
                                if "cross_validation" in method_results and "accuracy" in method_results["cross_validation"]:
                                    acc = method_results["cross_validation"]["accuracy"].get("mean", 0)
                                    methods.append(f"SGFA_{validation_type}_{method_name}")
                                    accuracies.append(acc)

                # Baseline results
                if "baseline_comparison" in results and "error" not in results["baseline_comparison"]:
                    baseline = results["baseline_comparison"]
                    if "pca" in baseline:
                        for method_name, pca_res in baseline["pca"].items():
                            if isinstance(pca_res, dict) and "error" not in pca_res:
                                # Extract accuracy from nested structure
                                if "cross_validation" in pca_res and "accuracy" in pca_res["cross_validation"]:
                                    acc = pca_res["cross_validation"]["accuracy"].get("mean", 0)
                                    methods.append(f"PCA_{method_name}")
                                    accuracies.append(acc)

                if methods:
                    axes[0].barh(methods, accuracies, color=['#1f77b4' if 'SGFA' in m else '#ff7f0e' for m in methods])
                    axes[0].set_xlabel("Accuracy")
                    axes[0].set_title("Classification Performance")
                    axes[0].set_xlim([0, 1])
                    axes[0].grid(True, alpha=0.3, axis='x')

                # Plot 2: Test success summary
                test_types = ["SGFA Factor Extraction", "Clinical Validation", "Baseline Comparison"]
                test_success = [1 if "error" not in sgfa_result else 0,
                               sum(1 for vt in validation_types if vt in results and "error" not in results.get(vt, {})),
                               1 if "baseline_comparison" in results and "error" not in results.get("baseline_comparison", {}) else 0]
                axes[1].bar(test_types, test_success, color=['green' if s > 0 else 'red' for s in test_success])
                axes[1].set_ylabel("Successful Tests")
                axes[1].set_title(f"Validation Tests ({successful_tests}/{total_tests} passed)")
                axes[1].set_ylim([0, max(test_success) + 1])
                axes[1].tick_params(axis='x', rotation=15)
                axes[1].grid(True, alpha=0.3, axis='y')

                plt.tight_layout()
                plots["clinical_validation_summary"] = fig

            except Exception as e:
                logger.warning(f"Failed to create clinical validation plots: {e}")

            return {
                "status": "completed",
                "clinical_results": results,
                "plots": plots,
                "summary": {
                    "total_tests": total_tests,
                    "successful_tests": successful_tests,
                    "success_rate": (
                        successful_tests / total_tests if total_tests > 0 else 0
                    ),
                    "data_characteristics": {
                        "n_subjects": X_list[0].shape[0],
                        "n_views": len(X_list),
                        "view_dimensions": [X.shape[1] for X in X_list],
                        "n_clinical_subtypes": len(np.unique(clinical_labels)),
                    },
                },
            }

        # Run experiment using framework
        result = framework.run_experiment(
            experiment_function=clinical_validation_experiment,
            config=exp_config,
            model_results={
                "X_list": X_list,
                "preprocessing_info": preprocessing_info,
                "clinical_labels": clinical_labels,
            },
        )

        logger.info("âœ… Clinical validation completed successfully")

        # Final memory cleanup
        jax.clear_caches()
        gc.collect()
        logger.info("ðŸ§¹ Memory cleanup completed")

        return result

    except Exception as e:
        logger.error(f"Clinical validation failed: {e}")

        # Cleanup memory on failure
        jax.clear_caches()
        gc.collect()
        logger.info("ðŸ§¹ Memory cleanup on failure completed")

        return None
