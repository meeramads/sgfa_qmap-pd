{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2943f201",
   "metadata": {},
   "source": [
    "# SGFA qMAP-PD Analysis on GPU\n",
    "\n",
    "**Updated for comprehensive experimental framework**\n",
    "\n",
    "This notebook provides GPU-accelerated analysis of Parkinson's disease neuroimaging data using Sparse Group Factor Analysis (SGFA). The codebase now includes a comprehensive experimental validation framework.\n",
    "\n",
    "## Key Updates:\n",
    "- **Experimental Framework**: Comprehensive validation suite\n",
    "- **Modular Analysis**: Data validation, method comparison, clinical validation\n",
    "- **Performance Optimization**: Memory management and streaming\n",
    "- **Cross-Validation**: Advanced CV strategies and nested validation\n",
    "- **Brain Mapping**: Factor-to-neuroimaging visualization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cwRO_dZf_R8l",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Creates Python 3.10 kernel with JAX CUDA support and mounts Google Drive for persistent results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a10592",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, subprocess, sys, stat, textwrap, pathlib, shutil, json\n",
    "\n",
    "REPO = \"/content/sgfa_qmap-pd\"\n",
    "REPO_URL = \"https://github.com/meeramads/sgfa_qmap-pd.git\"  # Update with your repo URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92ce943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "os.chdir(\"/content\")\n",
    "subprocess.run([\"rm\", \"-rf\", \"sgfa_qmap-pd\"])\n",
    "subprocess.check_call([\"git\", \"clone\", REPO_URL])\n",
    "os.chdir(REPO)\n",
    "\n",
    "print(f\"Repository cloned to: {REPO}\")\n",
    "print(f\"Current directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ipzIkySm4Ja",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "from pathlib import Path\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "g89Hv7Er-Pad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up persistent results directory\n",
    "drive_path = \"/content/drive/MyDrive/sgfa_experiments\"\n",
    "\n",
    "if os.path.exists(drive_path):\n",
    "    if os.path.isdir(drive_path):\n",
    "        print(f\"Found existing experiments directory with {len(os.listdir(drive_path))} items\")\n",
    "    else:\n",
    "        print(\"Found file named 'sgfa_experiments' - backing up and creating directory\")\n",
    "        shutil.move(drive_path, f\"{drive_path}_backup\")\n",
    "        os.makedirs(drive_path)\n",
    "else:\n",
    "    print(\"Creating new experiments directory\")\n",
    "    os.makedirs(drive_path)\n",
    "\n",
    "# Create symlinks for results\n",
    "for link_name in [\"experiment_results\", \"results\"]:\n",
    "    link_path = f\"../{link_name}\"\n",
    "    if os.path.exists(link_path):\n",
    "        if os.path.islink(link_path):\n",
    "            os.unlink(link_path)\n",
    "        else:\n",
    "            shutil.rmtree(link_path)\n",
    "    os.symlink(drive_path, link_path)\n",
    "\n",
    "print(f\"Results will be saved to: {drive_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83w0lfou_0NP",
   "metadata": {},
   "source": [
    "## Python and CUDA Environment\n",
    "\n",
    "**Must be connected to a GPU runtime.** Sets up Python 3.10 with JAX CUDA support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bea82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Python 3.10\n",
    "subprocess.check_call([\"wget\", \"-q\", \"https://github.com/korakot/kora/releases/download/v0.10/py310.sh\"])\n",
    "subprocess.check_call([\"bash\", \"./py310.sh\", \"-b\", \"-f\", \"-p\", \"/usr/local\"])\n",
    "subprocess.check_call([\"python3.10\", \"-V\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ebe654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "subprocess.check_call([\"python3.10\", \"-m\", \"pip\", \"install\", \"-U\", \"pip\"])\n",
    "subprocess.check_call([\"python3.10\", \"-m\", \"pip\", \"install\", \"-r\", \"requirements.txt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ab768e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up JAX with CUDA support\n",
    "subprocess.run([\"python3.10\", \"-m\", \"pip\", \"uninstall\", \"-y\", \"jax\", \"jaxlib\"])\n",
    "subprocess.check_call([\n",
    "    \"python3.10\", \"-m\", \"pip\", \"install\", \"-U\",\n",
    "    \"jax[cuda12_pip]==0.4.20\", \"-f\", \"https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\"\n",
    "])\n",
    "subprocess.check_call([\"python3.10\", \"-m\", \"pip\", \"install\", \"numpyro==0.13.2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061c1aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install NVIDIA CUDA libraries\n",
    "subprocess.check_call([\"python3.10\", \"-m\", \"pip\", \"install\", \"-q\",\n",
    "    \"nvidia-cudnn-cu12>=8.9,<9\",\n",
    "    \"nvidia-cublas-cu12>=12.2\",\n",
    "    \"nvidia-cuda-runtime-cu12>=12.2\",\n",
    "    \"nvidia-cusolver-cu12>=11.4\",\n",
    "    \"nvidia-cusparse-cu12>=12.1\",\n",
    "    \"nvidia-cufft-cu12>=11.0\",\n",
    "    \"nvidia-cuda-cupti-cu12>=12.2\",\n",
    "    \"nvidia-nvjitlink-cu12>=12.2\",\n",
    "    \"nvidia-nccl-cu12>=2.18\",\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f38bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CUDA-enabled Python launcher\n",
    "py310_site = subprocess.check_output(\n",
    "    [\"python3.10\", \"-c\", \"import site; print(site.getsitepackages()[0])\"],\n",
    "    text=True\n",
    ").strip()\n",
    "\n",
    "subdirs = [\"cudnn/lib\", \"cublas/lib\", \"cufft/lib\", \"cusolver/lib\", \"cusparse/lib\",\n",
    "           \"cuda_runtime/lib\", \"cuda_cupti/lib\", \"nvjitlink/lib\", \"nccl/lib\"]\n",
    "lib_paths = [os.path.join(py310_site, \"nvidia\", d) for d in subdirs]\n",
    "lib_paths = [p for p in lib_paths if os.path.isdir(p)]\n",
    "LD = \":\".join(lib_paths)\n",
    "\n",
    "wrapper = \"/usr/local/bin/py310cuda\"\n",
    "pathlib.Path(wrapper).write_text(textwrap.dedent(f\"\"\"\\\n",
    "#!/bin/bash\n",
    "export LD_LIBRARY_PATH=\"{LD}:$LD_LIBRARY_PATH\"\n",
    "export XLA_PYTHON_CLIENT_PREALLOCATE=false\n",
    "export XLA_PYTHON_CLIENT_MEM_FRACTION=0.75\n",
    "export JAX_PLATFORM_NAME=gpu\n",
    "export JAX_ENABLE_X64=true\n",
    "exec python3.10 \"$@\"\n",
    "\"\"\"))\n",
    "os.chmod(wrapper, os.stat(wrapper).st_mode | stat.S_IEXEC)\n",
    "\n",
    "print(\"CUDA launcher created successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442b29cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify JAX GPU setup\n",
    "!py310cuda -c \"import jax; print('Platform:', jax.lib.xla_bridge.get_backend().platform); print('Devices:', jax.devices())\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602a9caa",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Experimental Framework Usage\n",
    "\n",
    "The codebase now includes a comprehensive experimental framework. You can either use:\n",
    "\n",
    "1. **Legacy Analysis**: `run_analysis.py` for direct model runs\n",
    "2. **Experimental Framework**: Structured experiments with validation and comparison\n",
    "\n",
    "## Quick Status Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YX_waabPPWWD",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git pull\n",
    "!ls -la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "status_check",
   "metadata": {},
   "outputs": [],
   "source": "# Quick system check\nprint(\"=== System Status ===\")\n!py310cuda -c \"import jax; print(f'JAX backend: {jax.lib.xla_bridge.get_backend().platform}')\"\n!py310cuda -c \"import jax; print(f'JAX devices: {jax.devices()}')\"\n!py310cuda -c \"import numpy as np; print(f'NumPy version: {np.__version__}')\"\n!py310cuda -c \"import numpyro; print(f'NumPyro version: {numpyro.__version__}')\"\n\nprint(\"\\n=== Repository Structure ===\")\n!find . -maxdepth 2 -type d | head -15\n\nprint(\"\\n=== Available Datasets ===\")\n!ls -la qMAP-PD_data/ 2>/dev/null || echo \"qMAP-PD data directory not found\"\n\nprint(\"\\n=== Testing Framework Imports ===\")\n\n# Create import test script\nimport_test_script = '''\nimport sys\nimport os\nsys.path.insert(0, '.')\n\nFRAMEWORK_AVAILABLE = True\nerror_messages = []\n\n# Test core module imports\ntry:\n    from core import run_analysis, utils, get_data, visualization\n    print(\"Core modules imported\")\nexcept Exception as e:\n    print(f\"Core modules failed: {e}\")\n    error_messages.append(f\"Core: {e}\")\n\n# Test experimental framework imports\ntry:\n    from experiments.framework import ExperimentConfig, ExperimentFramework\n    print(\"ExperimentFramework imported\")\nexcept Exception as e:\n    print(f\"ExperimentFramework failed: {e}\")\n    error_messages.append(f\"ExperimentFramework: {e}\")\n    FRAMEWORK_AVAILABLE = False\n\ntry:\n    from experiments.data_validation import DataValidationExperiments\n    print(\"DataValidationExperiments imported\")\nexcept Exception as e:\n    print(f\"DataValidationExperiments failed: {e}\")\n    error_messages.append(f\"DataValidationExperiments: {e}\")\n    FRAMEWORK_AVAILABLE = False\n\n# Test legacy analysis imports\ntry:\n    from analysis.cross_validation import CVRunner\n    print(\"Legacy CVRunner imported\")\nexcept Exception as e:\n    print(f\"Legacy CVRunner failed: {e}\")\n    error_messages.append(f\"CVRunner: {e}\")\n\n# Test basic model imports\ntry:\n    from models import create_model\n    print(\"Model factory imported\")\nexcept Exception as e:\n    print(f\"Model factory failed: {e}\")\n\n# Save detailed status\nstatus_info = {\n    \"framework_available\": FRAMEWORK_AVAILABLE,\n    \"errors\": error_messages,\n    \"recommendation\": \"experimental\" if FRAMEWORK_AVAILABLE else \"legacy\"\n}\n\nwith open(\"framework_status.txt\", \"w\") as f:\n    f.write(\"available\" if FRAMEWORK_AVAILABLE else \"unavailable\")\n\nwith open(\"import_status.txt\", \"w\") as f:\n    f.write(str(status_info))\n\nif FRAMEWORK_AVAILABLE:\n    print(\"\\\\nExperimental framework ready!\")\n    print(\"   → You can use all experimental modules\")\nelse:\n    print(\"\\\\nExperimental framework has issues\")\n    print(\"   → Use legacy analysis methods instead\")\n    print(\"   → core/run_analysis.py should still work\")\n\nprint(f\"\\\\nStatus saved to framework_status.txt\")\n'''\n\n# Write and execute the test\nwith open(\"test_imports.py\", \"w\") as f:\n    f.write(import_test_script)\n\n!py310cuda test_imports.py"
  },
  {
   "cell_type": "markdown",
   "id": "experimental_framework",
   "metadata": {},
   "source": [
    "# Experimental Framework\n",
    "\n",
    "## Module 1: Data Validation Experiments\n",
    "\n",
    "Comprehensive data quality assessment and preprocessing validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "y1dzp01m7qg",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Troubleshooting & Fallback Guide\n",
    "print(\"=== Import Status Check ===\")\n",
    "\n",
    "# Check framework status\n",
    "framework_available = False\n",
    "try:\n",
    "    with open(\"framework_status.txt\", \"r\") as f:\n",
    "        status = f.read().strip()\n",
    "        framework_available = (status == \"available\")\n",
    "        print(f\"Framework status: {status}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Framework status: not tested yet - run the system check cell above\")\n",
    "\n",
    "if framework_available:\n",
    "    print(\" Experimental framework is available!\")\n",
    "    print(\"  You can run the experimental framework cells below\")\n",
    "    print(\"   Advanced validation, comparison, and clinical experiments\")\n",
    "else:\n",
    "    print(\"  Experimental framework has import issues\")\n",
    "    print(\"   SOLUTION: Use legacy analysis instead\")\n",
    "    print(\"   run_analysis.py should work fine\")\n",
    "    print(\"   All core SGFA functionality available\")\n",
    "\n",
    "print(\"\\n=== Recommended Workflow ===\")\n",
    "if framework_available:\n",
    "    print(\"1. Run experimental framework cells (data validation, method comparison, etc.)\")\n",
    "    print(\"2. OR run legacy analysis for reliable results\")\n",
    "else:\n",
    "    print(\"1. Skip experimental framework cells (they will show warnings)\")\n",
    "    print(\"2. Use legacy analysis cells - these are very reliable\")\n",
    "    print(\"3. Start with 'Quick smoke test' cell to verify everything works\")\n",
    "\n",
    "print(\"\\n=== Legacy Analysis Available ===\")\n",
    "print(\"• Basic qMAP-PD analysis\")  \n",
    "print(\"• Preprocessing pipeline tests\")\n",
    "print(\"• Cross-validation experiments\")\n",
    "print(\"• Comprehensive qMAP-PD analysis\")\n",
    "print(\"• All core SGFA functionality\")\n",
    "\n",
    "print(\"\\nBoth approaches provide quality results!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data_validation_setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Validation Experiments - with enhanced error handling\n",
    "import os\n",
    "\n",
    "# Check if experimental framework is available\n",
    "framework_available = True\n",
    "if os.path.exists('framework_status.txt'):\n",
    "    with open('framework_status.txt', 'r') as f:\n",
    "        framework_available = f.read().strip() == 'available'\n",
    "\n",
    "if framework_available:\n",
    "    print(\"🚀 Running experimental framework data validation...\")\n",
    "    \n",
    "    data_validation_script = \"\"\"\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "import traceback\n",
    "\n",
    "try:\n",
    "    from experiments.data_validation import DataValidationExperiments\n",
    "    from experiments.framework import ExperimentConfig, ExperimentFramework\n",
    "    from pathlib import Path\n",
    "\n",
    "    # Create experiment framework with output directory\n",
    "    framework = ExperimentFramework(\n",
    "        base_output_dir=Path(\"../experiment_results\")\n",
    "    )\n",
    "\n",
    "    # Configure experiment with proper data directory\n",
    "    config = ExperimentConfig(\n",
    "        experiment_name=\"data_validation_colab\",\n",
    "        description=\"Comprehensive data validation on qMAP-PD dataset\",\n",
    "        dataset=\"qmap_pd\",\n",
    "        data_dir=\"qMAP-PD_data\"\n",
    "    )\n",
    "\n",
    "    # Create validator with framework\n",
    "    validator = DataValidationExperiments(framework)\n",
    "\n",
    "    print(\"Running data quality assessment...\")\n",
    "    try:\n",
    "        quality_result = validator.run_data_quality_assessment(config)\n",
    "        print(f\"Data quality assessment completed: {quality_result.status}\")\n",
    "        \n",
    "        # Try to access results\n",
    "        if hasattr(quality_result, 'results') and quality_result.results:\n",
    "            print(f\" Generated results with {len(quality_result.results)} metrics\")\n",
    "        if hasattr(quality_result, 'artifacts') and quality_result.artifacts:\n",
    "            print(f\" Created {len(quality_result.artifacts)} artifacts\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\" Data quality assessment failed with detailed error:\")\n",
    "        print(f\"Error type: {type(e).__name__}\")\n",
    "        print(f\"Error message: {str(e)}\")\n",
    "        print(\"Full traceback:\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "    # Test simpler preprocessing comparison\n",
    "    print(\"\\\\nRunning preprocessing comparison...\")\n",
    "    try:\n",
    "        preprocessing_result = validator.run_preprocessing_comparison(config)\n",
    "        print(f\"Preprocessing comparison completed: {preprocessing_result.status}\")\n",
    "        \n",
    "        if hasattr(preprocessing_result, 'results') and preprocessing_result.results:\n",
    "            print(f\" Generated results with {len(preprocessing_result.results)} comparisons\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\" Preprocessing comparison failed:\")\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "    print(\"\\\\nData validation experiments completed!\")\n",
    "    print(\" Check ../experiment_results/ for any generated plots and reports\")\n",
    "    \n",
    "    # Test direct data loading to verify the issue\n",
    "    print(\"\\\\n=== Testing Direct Data Loading ===\")\n",
    "    try:\n",
    "        from data.qmap_pd import load_qmap_pd\n",
    "        \n",
    "        print(\"Loading qMAP-PD data directly...\")\n",
    "        test_data = load_qmap_pd(\n",
    "            data_dir=\"qMAP-PD_data\",\n",
    "            enable_advanced_preprocessing=False\n",
    "        )\n",
    "        \n",
    "        print(f\" Direct data loading successful!\")\n",
    "        print(f\"   - Data keys: {list(test_data.keys())}\")\n",
    "        print(f\"   - X_list length: {len(test_data['X_list'])}\")\n",
    "        print(f\"   - Shapes: {[X.shape for X in test_data['X_list']]}\")\n",
    "        print(f\"   - View names: {test_data.get('view_names', [])}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Direct data loading failed: {e}\")\n",
    "        traceback.print_exc()\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\" Cannot run experimental framework: {e}\")\n",
    "    print(\" Try the legacy analysis cells instead\")\n",
    "except Exception as e:\n",
    "    print(f\" Experiment execution failed: {e}\")\n",
    "    traceback.print_exc()\n",
    "\"\"\"\n",
    "\n",
    "    # Write and run the script\n",
    "    with open(\"data_validation_run.py\", \"w\") as f:\n",
    "        f.write(data_validation_script)\n",
    "\n",
    "    !py310cuda data_validation_run.py\n",
    "    \n",
    "else:\n",
    "    print(\"  Experimental framework not available.\")\n",
    "    print(\" Use legacy analysis instead:\")\n",
    "    print(\"   • Basic qMAP-PD analysis\")\n",
    "    print(\"   • Preprocessing pipeline tests\") \n",
    "    print(\"   • Cross-validation experiments\")\n",
    "    print(\"\\nRun the legacy analysis cells below for reliable results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "method_comparison",
   "metadata": {},
   "source": [
    "## Module 2: Method Comparison Experiments\n",
    "\n",
    "Compare different SGFA variants and traditional methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "method_comparison_run",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method Comparison Experiments - with correct API usage\n",
    "import os\n",
    "\n",
    "framework_available = True\n",
    "if os.path.exists('framework_status.txt'):\n",
    "    with open('framework_status.txt', 'r') as f:\n",
    "        framework_available = f.read().strip() == 'available'\n",
    "\n",
    "if framework_available:\n",
    "    print(\" Running method comparison experiments...\")\n",
    "    \n",
    "    method_comparison_script = \"\"\"\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "\n",
    "try:\n",
    "    from experiments.method_comparison import MethodComparisonExperiments\n",
    "    from experiments.framework import ExperimentConfig, ExperimentFramework\n",
    "    from pathlib import Path\n",
    "\n",
    "    # Create experiment framework\n",
    "    framework = ExperimentFramework(\n",
    "        base_output_dir=Path(\"../experiment_results\")\n",
    "    )\n",
    "\n",
    "    # Configure experiment with Colab-friendly parameters\n",
    "    config = ExperimentConfig(\n",
    "        experiment_name=\"method_comparison_colab\",\n",
    "        description=\"SGFA variants and traditional method comparison\",\n",
    "        dataset=\"qmap_pd\",\n",
    "        # Reduced parameters for Colab\n",
    "        K_values=[8, 12],\n",
    "        num_samples=500,\n",
    "        num_warmup=250,\n",
    "        num_chains=2,\n",
    "        num_runs_per_config=1\n",
    "    )\n",
    "\n",
    "    # Create comparator\n",
    "    comparator = MethodComparisonExperiments(framework)\n",
    "\n",
    "    print(\"Running GFA variant comparison...\")\n",
    "    try:\n",
    "        variant_result = comparator.run_gfa_variant_comparison(config)\n",
    "        print(f\"GFA variant comparison completed: {variant_result.status}\")\n",
    "    except Exception as e:\n",
    "        print(f\"GFA variant comparison failed: {e}\")\n",
    "\n",
    "    print(\"\\\\nRunning traditional method comparison...\")\n",
    "    try:\n",
    "        traditional_result = comparator.run_traditional_method_comparison(config)\n",
    "        print(f\"Traditional method comparison completed: {traditional_result.status}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Traditional method comparison failed: {e}\")\n",
    "\n",
    "    print(\"\\\\nMethod comparison experiments completed!\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\" Cannot run experimental framework: {e}\")\n",
    "    print(\" Try the legacy analysis cells instead\")\n",
    "except Exception as e:\n",
    "    print(f\" Experiment execution failed: {e}\")\n",
    "\"\"\"\n",
    "\n",
    "    with open(\"method_comparison_run.py\", \"w\") as f:\n",
    "        f.write(method_comparison_script)\n",
    "\n",
    "    !py310cuda method_comparison_run.py\n",
    "    \n",
    "else:\n",
    "    print(\"  Experimental framework not available - using legacy analysis instead\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "performance_benchmarks",
   "metadata": {},
   "source": [
    "## Module 3: Performance Benchmarks\n",
    "\n",
    "Scalability and efficiency testing optimized for GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "performance_benchmarks_run",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Benchmarks - with correct API usage  \n",
    "import os\n",
    "\n",
    "framework_available = True\n",
    "if os.path.exists('framework_status.txt'):\n",
    "    with open('framework_status.txt', 'r') as f:\n",
    "        framework_available = f.read().strip() == 'available'\n",
    "\n",
    "if framework_available:\n",
    "    print(\" Running performance benchmarks...\")\n",
    "    \n",
    "    performance_script = \"\"\"\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "\n",
    "try:\n",
    "    from experiments.performance_benchmarks import PerformanceBenchmarkExperiments\n",
    "    from experiments.framework import ExperimentConfig, ExperimentFramework\n",
    "    from pathlib import Path\n",
    "\n",
    "    # Create experiment framework\n",
    "    framework = ExperimentFramework(\n",
    "        base_output_dir=Path(\"../experiment_results\")\n",
    "    )\n",
    "\n",
    "    # Configure for GPU performance testing\n",
    "    config = ExperimentConfig(\n",
    "        experiment_name=\"performance_benchmarks_gpu\",\n",
    "        description=\"GPU performance benchmarking for SGFA\",\n",
    "        dataset=\"synthetic\",  # Use synthetic for controlled benchmarking\n",
    "        K_values=[5, 10, 15],\n",
    "        num_samples=400,\n",
    "        num_warmup=200,\n",
    "        num_chains=2,\n",
    "        num_runs_per_config=1\n",
    "    )\n",
    "\n",
    "    # Create benchmarker\n",
    "    benchmarker = PerformanceBenchmarkExperiments(framework)\n",
    "\n",
    "    print(\"Running scalability benchmarks...\")\n",
    "    try:\n",
    "        scalability_result = benchmarker.run_scalability_benchmarks(config)\n",
    "        print(f\"Scalability benchmarks completed: {scalability_result.status}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Scalability benchmarks failed: {e}\")\n",
    "\n",
    "    print(\"\\\\nRunning memory benchmarks...\")\n",
    "    try:\n",
    "        memory_result = benchmarker.run_memory_benchmarks(config)\n",
    "        print(f\"Memory benchmarks completed: {memory_result.status}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Memory benchmarks failed: {e}\")\n",
    "\n",
    "    print(\"\\\\nPerformance benchmarking completed!\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\" Cannot run experimental framework: {e}\")\n",
    "    print(\" Try the legacy analysis cells instead\")\n",
    "except Exception as e:\n",
    "    print(f\" Experiment execution failed: {e}\")\n",
    "\"\"\"\n",
    "\n",
    "    with open(\"performance_run.py\", \"w\") as f:\n",
    "        f.write(performance_script)\n",
    "\n",
    "    !py310cuda performance_run.py\n",
    "    \n",
    "else:\n",
    "    print(\"  Experimental framework not available - using legacy analysis instead\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clinical_validation",
   "metadata": {},
   "source": [
    "## Module 4: Clinical Validation\n",
    "\n",
    "Parkinson's disease subtype classification and clinical validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clinical_validation_run",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clinical Validation - with correct API usage\n",
    "import os\n",
    "\n",
    "framework_available = True\n",
    "if os.path.exists('framework_status.txt'):\n",
    "    with open('framework_status.txt', 'r') as f:\n",
    "        framework_available = f.read().strip() == 'available'\n",
    "\n",
    "if framework_available:\n",
    "    print(\" Running clinical validation...\")\n",
    "    \n",
    "    clinical_script = \"\"\"\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "\n",
    "try:\n",
    "    from experiments.clinical_validation import ClinicalValidationExperiments\n",
    "    from experiments.framework import ExperimentConfig, ExperimentFramework\n",
    "    from pathlib import Path\n",
    "\n",
    "    # Create experiment framework\n",
    "    framework = ExperimentFramework(\n",
    "        base_output_dir=Path(\"../experiment_results\")\n",
    "    )\n",
    "\n",
    "    # Configure clinical validation\n",
    "    config = ExperimentConfig(\n",
    "        experiment_name=\"clinical_validation_pd\",\n",
    "        description=\"Parkinson's disease clinical validation\",\n",
    "        dataset=\"qmap_pd\",\n",
    "        K_values=[12, 18],\n",
    "        num_samples=1000,\n",
    "        num_warmup=500,\n",
    "        num_chains=2,\n",
    "        num_runs_per_config=1\n",
    "    )\n",
    "\n",
    "    # Create clinical validator\n",
    "    clinical_validator = ClinicalValidationExperiments(framework)\n",
    "\n",
    "    print(\"Running PD subtype classification...\")\n",
    "    try:\n",
    "        subtype_result = clinical_validator.run_pd_subtype_classification(config)\n",
    "        print(f\"PD subtype classification completed: {subtype_result.status}\")\n",
    "    except Exception as e:\n",
    "        print(f\"PD subtype classification failed: {e}\")\n",
    "\n",
    "    print(\"\\\\nRunning biomarker discovery...\")\n",
    "    try:\n",
    "        biomarker_result = clinical_validator.run_biomarker_discovery(config)\n",
    "        print(f\"Biomarker discovery completed: {biomarker_result.status}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Biomarker discovery failed: {e}\")\n",
    "\n",
    "    print(\"\\\\nClinical validation experiments completed!\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\" Cannot run experimental framework: {e}\")\n",
    "    print(\" Try the legacy analysis cells instead\")\n",
    "except Exception as e:\n",
    "    print(f\" Experiment execution failed: {e}\")\n",
    "    print(\" This may be due to missing clinical data or other dependencies\")\n",
    "\"\"\"\n",
    "\n",
    "    with open(\"clinical_validation_run.py\", \"w\") as f:\n",
    "        f.write(clinical_script)\n",
    "\n",
    "    !py310cuda clinical_validation_run.py\n",
    "    \n",
    "else:\n",
    "    print(\"  Experimental framework not available - using legacy analysis instead\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legacy_analysis",
   "metadata": {},
   "source": [
    "# Legacy Analysis (run_analysis.py)\n",
    "\n",
    "Direct model runs using the original interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legacy_smoke_test",
   "metadata": {},
   "outputs": [],
   "source": "# Quick smoke test - This should always work\nprint(\"=== Running Legacy Analysis Smoke Test ===\")\nprint(\"If experimental framework failed, this provides a reliable fallback\")\n\n!py310cuda core/run_analysis.py \\\n  --dataset synthetic \\\n  --K 5 \\\n  --num-samples 200 \\\n  --num-warmup 100 \\\n  --num-chains 2 \\\n  --num-runs 1 \\\n  --device gpu \\\n  --seed 42\n\nprint(\"\\n Legacy analysis smoke test completed!\")\nprint(\"If this worked, you can run all the other legacy analysis cells below.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qmap_pd_basic",
   "metadata": {},
   "outputs": [],
   "source": "# Basic qMAP-PD analysis\nprint(\"Running basic qMAP-PD analysis...\")\n!py310cuda core/run_analysis.py \\\n  --dataset qmap_pd \\\n  --K 10 \\\n  --num-samples 800 \\\n  --num-warmup 400 \\\n  --num-chains 2 \\\n  --num-runs 2 \\\n  --device gpu \\\n  --seed 42"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preprocessing_test",
   "metadata": {},
   "outputs": [],
   "source": "# Test preprocessing pipeline\nprint(\"Testing preprocessing pipeline...\")\n!py310cuda core/run_analysis.py \\\n  --dataset qmap_pd \\\n  --K 8 \\\n  --num-samples 500 \\\n  --num-warmup 250 \\\n  --num-chains 2 \\\n  --enable_preprocessing \\\n  --feature_selection variance \\\n  --n_top_features 200 \\\n  --device gpu \\\n  --seed 42"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cv_test",
   "metadata": {},
   "outputs": [],
   "source": "# Cross-validation test\nprint(\"Testing cross-validation...\")\n!py310cuda core/run_analysis.py \\\n  --dataset synthetic \\\n  --K 5 \\\n  --num-samples 400 \\\n  --num-warmup 200 \\\n  --num-chains 2 \\\n  --cv_only \\\n  --cv_folds 3 \\\n  --device gpu \\\n  --seed 42"
  },
  {
   "cell_type": "markdown",
   "id": "advanced_analysis",
   "metadata": {},
   "source": [
    "# Advanced Analysis Pipelines\n",
    "\n",
    "Computationally intensive analyses - run selectively based on available time/resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comprehensive_qmap_pd",
   "metadata": {},
   "outputs": [],
   "source": "# Comprehensive qMAP-PD analysis\nproceed = input(\"Run comprehensive qMAP-PD analysis? (15-20 minutes) [y/N]: \")\nif proceed.lower() in ['y', 'yes']:\n    !py310cuda core/run_analysis.py \\\n      --dataset qmap_pd \\\n      --K 20 \\\n      --num-samples 2000 \\\n      --num-warmup 1000 \\\n      --num-chains 3 \\\n      --num-runs 3 \\\n      --percW 33 \\\n      --enable_preprocessing \\\n      --feature_selection combined \\\n      --n_top_features 400 \\\n      --device gpu \\\n      --seed 42\n    print(\"\\nComprehensive analysis completed!\")\nelse:\n    print(\"Skipped comprehensive analysis.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "full_cv_analysis",
   "metadata": {},
   "outputs": [],
   "source": "# Full cross-validation analysis\nproceed = input(\"Run full cross-validation analysis? (20-25 minutes) [y/N]: \")\nif proceed.lower() in ['y', 'yes']:\n    !py310cuda core/run_analysis.py \\\n      --dataset qmap_pd \\\n      --K 15 \\\n      --num-samples 1200 \\\n      --num-warmup 600 \\\n      --num-chains 3 \\\n      --run_cv \\\n      --cv_folds 5 \\\n      --enable_preprocessing \\\n      --feature_selection combined \\\n      --n_top_features 300 \\\n      --device gpu \\\n      --seed 42\n    print(\"\\nFull CV analysis completed!\")\nelse:\n    print(\"Skipped full CV analysis.\")"
  },
  {
   "cell_type": "markdown",
   "id": "results_summary",
   "metadata": {},
   "source": [
    "# Results Summary and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "check_results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check generated results\n",
    "print(\"=== Generated Results ===\")\n",
    "!find ../experiment_results -name \"*.json\" -o -name \"*.pkl\" -o -name \"*.png\" | head -20\n",
    "\n",
    "print(\"\\n=== Legacy Results ===\")\n",
    "!find ../results -name \"*.json\" -o -name \"*.pkl\" -o -name \"*.png\" 2>/dev/null | head -10 || echo \"No legacy results found\"\n",
    "\n",
    "print(\"\\n=== Drive Storage ===\")\n",
    "!du -sh /content/drive/MyDrive/sgfa_experiments/ 2>/dev/null || echo \"Drive path not accessible\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generate_summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate experiment summary\n",
    "summary_script = \"\"\"\n",
    "import sys, json, os\n",
    "sys.path.append('.')\n",
    "\n",
    "from experiments.framework import ExperimentRunner\n",
    "from pathlib import Path\n",
    "\n",
    "# Look for experiment results\n",
    "results_dir = Path('../experiment_results')\n",
    "if results_dir.exists():\n",
    "    runner = ExperimentRunner()\n",
    "    \n",
    "    print(\"=== Experiment Summary ===\")\n",
    "    try:\n",
    "        # Generate comprehensive report\n",
    "        report = runner.generate_experiment_report()\n",
    "        \n",
    "        print(f\"Total experiments: {len(report.get('experiments', []))}\")\n",
    "        print(f\"Successful experiments: {report.get('summary', {}).get('successful', 0)}\")\n",
    "        print(f\"Failed experiments: {report.get('summary', {}).get('failed', 0)}\")\n",
    "        \n",
    "        # Save summary\n",
    "        with open('../experiment_results/session_summary.json', 'w') as f:\n",
    "            json.dump(report, f, indent=2, default=str)\n",
    "            \n",
    "        print(\"\\nSummary saved to: ../experiment_results/session_summary.json\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Could not generate summary: {e}\")\n",
    "else:\n",
    "    print(\"No experiment results directory found\")\n",
    "\"\"\"\n",
    "\n",
    "with open(\"generate_summary.py\", \"w\") as f:\n",
    "    f.write(summary_script)\n",
    "\n",
    "!py310cuda generate_summary.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "usage_notes",
   "metadata": {},
   "source": [
    "# Usage Notes\n",
    "\n",
    "## Workflow Recommendations:\n",
    "\n",
    "### For Quick Results (30-45 minutes):\n",
    "1. Run **Data Validation** experiments\n",
    "2. Run **Performance Benchmarks**\n",
    "3. Run basic **Legacy Analysis** smoke tests\n",
    "\n",
    "### For Comprehensive Results (60-90 minutes):\n",
    "1. Run all **Experimental Framework** modules\n",
    "2. Run **Advanced Analysis** pipelines\n",
    "3. Generate comprehensive visualizations\n",
    "\n",
    "### For Research Papers:\n",
    "1. **Data Validation** → **Method Comparison** → **Clinical Validation**\n",
    "2. Save optimal configs for transfer to remote GPU workstation\n",
    "3. Use **Performance Benchmarks** for scaling analysis\n",
    "\n",
    "## Key Improvements:\n",
    "- **Modular experimental design** with standardized configs\n",
    "- **GPU memory optimization** with adaptive batch sizing\n",
    "- **Comprehensive validation** framework for reproducibility\n",
    "- **Clinical validation** specific to Parkinson's disease research\n",
    "- **Performance profiling** with detailed memory and timing analysis\n",
    "\n",
    "## Results Location:\n",
    "- **Experimental Framework**: `../experiment_results/`\n",
    "- **Legacy Analysis**: `../results/`\n",
    "- **Persistent Storage**: Google Drive (`sgfa_experiments/`)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}