name: Release and Deployment

on:
  push:
    tags:
      - 'v*'  # Trigger on version tags like v1.0.0
  release:
    types: [published]

jobs:
  create-release-artifacts:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python 3.10
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install build twine
        
    - name: Run comprehensive test suite
      run: |
        pip install -r requirements-dev.txt
        pytest tests/ -v --cov=. --cov-report=xml
        
    - name: Generate performance benchmarks
      run: |
        python -c "
        import sys
        sys.path.insert(0, '.')
        from experiments.performance_benchmarks import PerformanceBenchmarkExperiments
        from experiments.framework import ExperimentConfig
        from data.synthetic import generate_synthetic_data
        
        print('Generating release performance benchmarks...')
        
        data = generate_synthetic_data(num_sources=3, K=5)
        X_list = [data['X1'], data['X2'], data['X3']]
        
        config = ExperimentConfig(
            output_dir='./release_benchmarks',
            save_plots=True,
            random_seed=42
        )
        
        benchmarker = PerformanceBenchmarkExperiments(config)
        
        # Run comprehensive benchmarks for release
        print('Running scalability benchmarks...')
        result = benchmarker.run_scalability_benchmarks(
            X_list, 
            {'K': 5, 'sparsity_level': 0.3}, 
            {'num_samples': 200}
        )
        
        if result.success:
            print('PASSED Release benchmarks completed successfully!')
        else:
            print('WARNING  Some benchmarks failed, but continuing with release...')
        "
        
    - name: Generate comprehensive experimental validation
      run: |
        python -c "
        import sys
        sys.path.insert(0, '.')
        from experiments import *
        from experiments.framework import ExperimentConfig
        from data.synthetic import generate_synthetic_data
        
        print('Running comprehensive experimental validation for release...')
        
        # Generate test data
        data = generate_synthetic_data(num_sources=3, K=5)
        X_list = [data['X1'], data['X2'], data['X3']]
        
        config = ExperimentConfig(
            output_dir='./release_experiments',
            save_plots=True,
            random_seed=42
        )
        
        experiments_run = []
        
        # Data validation
        try:
            validator = DataValidationExperiments(config)
            result = validator.run_data_quality_assessment(X_list)
            experiments_run.append(('Data Validation', result.success))
        except Exception as e:
            experiments_run.append(('Data Validation', False))
            print(f'Data validation failed: {e}')
        
        # Method comparison
        try:
            comparator = MethodComparisonExperiments(config)
            result = comparator.run_sgfa_variant_comparison(
                X_list, {'K': 5}, {'num_samples': 100}
            )
            experiments_run.append(('Method Comparison', result.success))
        except Exception as e:
            experiments_run.append(('Method Comparison', False))
            print(f'Method comparison failed: {e}')
        
        # Reproducibility
        try:
            repro_tester = ReproducibilityExperiments(config)
            result = repro_tester.run_seed_reproducibility_test(
                X_list, {'K': 5}, {'num_samples': 50}, seeds=[42, 123, 456]
            )
            experiments_run.append(('Reproducibility', result.success))
        except Exception as e:
            experiments_run.append(('Reproducibility', False))
            print(f'Reproducibility test failed: {e}')
        
        print('\\n=== RELEASE EXPERIMENTAL VALIDATION SUMMARY ===')
        for exp_name, success in experiments_run:
            status = 'PASSED PASSED' if success else 'FAILED FAILED'
            print(f'{exp_name}: {status}')
        
        # Write summary to file
        with open('./release_experiments/validation_summary.txt', 'w') as f:
            f.write('Release Experimental Validation Summary\\n')
            f.write('=====================================\\n\\n')
            for exp_name, success in experiments_run:
                status = 'PASSED' if success else 'FAILED'
                f.write(f'{exp_name}: {status}\\n')
        "
        
    - name: Build Python package
      run: |
        python -m build
        
    - name: Create release documentation
      run: |
        mkdir -p release_docs
        
        # Generate changelog entry
        cat > release_docs/CHANGELOG_ENTRY.md << EOF
        # Release Notes
        
        ## New Features
        - Comprehensive experimental validation framework
        - Performance optimization with memory management
        - Clinical validation capabilities for PD subtyping
        - Reproducibility testing and validation
        
        ## Performance Benchmarks
        See \`release_benchmarks/\` directory for detailed performance analysis.
        
        ## Experimental Validation
        See \`release_experiments/\` directory for comprehensive validation results.
        
        ## Installation
        \`\`\`bash
        pip install sgfa-qmap-pd
        \`\`\`
        
        ## Quick Start
        \`\`\`python
        from get_data import generate_synthetic_data
        from run_analysis import run_sgfa_analysis
        
        # Generate synthetic data
        data = generate_synthetic_data(num_sources=3, K=5)
        X_list = [data['X1'], data['X2'], data['X3']]
        
        # Run analysis
        results = run_sgfa_analysis(X_list, K=5, sparsity_level=0.3)
        \`\`\`
        EOF
        
        # Copy README for release
        cp README.md release_docs/
        
    - name: Upload release artifacts
      uses: actions/upload-artifact@v3
      with:
        name: release-artifacts
        path: |
          dist/
          release_benchmarks/
          release_experiments/
          release_docs/
          coverage.xml
          
    - name: Create GitHub release
      if: startsWith(github.ref, 'refs/tags/')
      uses: actions/create-release@v1
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      with:
        tag_name: ${{ github.ref_name }}
        release_name: SGFA qMAP-PD ${{ github.ref_name }}
        body_path: release_docs/CHANGELOG_ENTRY.md
        draft: false
        prerelease: false

  deploy-to-pypi:
    runs-on: ubuntu-latest
    needs: create-release-artifacts
    if: startsWith(github.ref, 'refs/tags/') && !contains(github.ref, 'alpha') && !contains(github.ref, 'beta')
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install build twine
        
    - name: Build package
      run: python -m build
      
    - name: Publish to PyPI
      env:
        TWINE_USERNAME: __token__
        TWINE_PASSWORD: ${{ secrets.PYPI_API_TOKEN }}
      run: |
        twine upload dist/*

  deploy-research-artifacts:
    runs-on: ubuntu-latest
    needs: create-release-artifacts
    if: startsWith(github.ref, 'refs/tags/')
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Download release artifacts
      uses: actions/download-artifact@v3
      with:
        name: release-artifacts
        path: ./artifacts/
        
    - name: Create research data package
      run: |
        mkdir -p research_package
        
        # Package experimental results
        tar -czf research_package/experimental_validation_${GITHUB_REF_NAME}.tar.gz -C artifacts release_experiments/
        tar -czf research_package/performance_benchmarks_${GITHUB_REF_NAME}.tar.gz -C artifacts release_benchmarks/
        
        # Create metadata file
        cat > research_package/metadata.json << EOF
        {
          "version": "${GITHUB_REF_NAME}",
          "release_date": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
          "repository": "${GITHUB_REPOSITORY}",
          "commit_sha": "${GITHUB_SHA}",
          "artifacts": [
            "experimental_validation_${GITHUB_REF_NAME}.tar.gz",
            "performance_benchmarks_${GITHUB_REF_NAME}.tar.gz"
          ]
        }
        EOF
        
    - name: Upload research artifacts to release
      if: startsWith(github.ref, 'refs/tags/')
      uses: softprops/action-gh-release@v1
      with:
        files: research_package/*
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}